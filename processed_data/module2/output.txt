✅ Model sentence-transformers/all-MiniLM-L6-v2 loaded successfully.
✅ Loaded 194 docs

Chunk 1:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: INFO 5731 Computational Methods for Information Systems Section: 020 SYLLABUS Spring 2025
Table of Contents COURSE INFORMATION 1 Instructor Contact Information 1 Teaching Assistant 1 Communicating with Your Instructor 1 Course Pre-requisites, Co-requisites, and/or Other Restrictions 2 Course Format 2 Course Description 2 Course Goals, Learning Objectives 2 Materials 2 Teaching Philosophy 3 TECHNICAL REQUIREMENTS/ASSISTANCE 3 Minimum Technical Skills Needed 4 Student Academic Support Services 4 ASSESSMENT & 


Chunk 2:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text:  Student Academic Support Services 4 ASSESSMENT & GRADING 4 Assessments 4 Grading 4 Grading Table 6 COURSE CALENDAR 6 Table 1. Lessons and Readings 7 Study Schedule and Due Dates 7 Table 2. Study Schedule and Due Dates 7 COURSE EVALUATION 8 Student Evaluation Administration Dates 8 COURSE POLICIES 9 Assignment Policy 9 Examination Policy 9 Instructor Responsibilities and Feedback 9 Late Work and Missed Work 9 Course Incomplete Grade 9 Withdrawal 9 Attendance Policy 10 Students’ Responsibility for Their Lear


Chunk 3:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text:  Policy 10 Students’ Responsibility for Their Learning 10
UNT POLICIES 10 Academic Integrity Policy 10 ADA Policy 10 Emergency Notification & Procedures 10 Retention of Student Records 10 Acceptable Student Behavior 11 Access to Information - Eagle Connect 11 Sexual Assault Prevention 11 Important Notice for F-1 Students taking Distance Education Courses 11 Federal Regulation 11 University of North Texas Compliance 12 Student Verification 12 Use of Student Work 12
INFO 5731— Computational Methods for Inform


Chunk 4:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: ork 12
INFO 5731— Computational Methods for Information Systems Spring 2025 COURSE INFORMATION • INFO 5731, Sections 020, 3 Credit Hours • Title: Computational Methods for Information Systems • Meeting Dates (Face-to-face): See Table 2 • Meeting Time: Wednesday 5:30PM - 8:20PM • Room: NTDP B185 Instructor Contact Information • Haihua Chen, Assistant Professor in Data Science, Anuradha and Vikas Sinha Department of Data Science, University of North Texas. • Office: DP E298A (By appointment) • Zoom Meeting ID


Chunk 5:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: ffice: DP E298A (By appointment) • Zoom Meeting ID: 247 728 2245 (By appointment) • Phone: (940) 268-8589 • Email address: haihua.chen@unt.edu Teaching Assistant • Fengjiao Tu, PhD student in Information Science, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Tuesday 1-5PM, E292L, other time by appointment • Zoom meeting ID: 884 2281 7391 (By appointment) • Email address: fengjiaotu@my.unt.edu • Huyen Thi Ngoc Nguyen, PhD candidate in Informati


Chunk 6:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text:  Huyen Thi Ngoc Nguyen, PhD candidate in Information Science, Teaching Fellow, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Wednesday 1-3PM, E292J, other time by appointment • Zoom meeting ID: 889 281 7606 (By appointment) • Email address: huyennguyen5@my.unt.edu • Fardeen Ali Mohammed, Master student in Information Science, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Wednesda


Chunk 7:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 69, 'num_chars': 512}
Text:  of North Texas • Office and office hour: Wednesday 1:30pm to 3:30pm, E292L, other time by appointment • Zoom meeting ID: TBA • Email address: fardeenalimohammed@my.unt.edu Communicating with Your Instructor This course will have a website in UNT Canvas (https://unt.instructure.com/login/canvas) for online discussion, assignment submissions, and sharing of reading materials. Students are welcome to make an appointment with the instructor and/or the teaching assistant (TA) to discuss course-related questions


Chunk 8:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: assistant (TA) to discuss course-related questions (in person or online). If you need to schedule an individual online meeting with the INFO 5731 1 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 instructor or the TA, please send her/him an email via the course website in Canvas Course Messages to make an appointment. Course Pre-requisites, Co-requisites, and/or Other Restrictions • Pre-requisite: Basic programming knowledge and experience (Python), or consent of instructor 


Chunk 9:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 72, 'num_chars': 512}
Text: and experience (Python), or consent of instructor Course Format INFO 5731, Sections 020 hold face-to-face lectures by the instructor. The course uses Canvas, UNT's new learning management system. All course materials will be available at the course site on Canvas that is accessible to all students. And students will submit all assignments through the tools available on Canvas. Course Description Introduces computational methods that facilitate information analysis, management, and presentation in informatio


Chunk 10:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 66, 'num_chars': 512}
Text: alysis, management, and presentation in information systems. Students learn effective computer programming skills and analytical tools to process real-world data. Problem-oriented and project-based, allows students to explore interesting research ideas or implement useful information management applications. Course Goals, Learning Objectives • Master key concepts and components of NLP and linguistics. • Manipulate large corpora, explore linguistic models, and test empirical claims. • Design and implement ap


Chunk 11:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: d test empirical claims. • Design and implement applications that process, manage, and analyze text data. • Clean and preprocess raw text data using basic natural language processing techniques. • Demonstrate the ability to extract and analyze information from text data using Python Program. • Build robust systems to perform linguistic tasks with technological applications. • Document and report on information processing and applications. Materials Textbook information (required): 1. Downey, Allen B. (2016)


Chunk 12:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 56, 'num_chars': 512}
Text: information (required): 1. Downey, Allen B. (2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. O’Reilly, ISBN-13: 978-1-491-93936-9. Free access link: https://greenteapress.com/thinkpython/thinkpython.html 2. Hapke, H., Howard, C., & Lane, H. (2021). Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (2nd Edition). Simon and Schuster. Link: https://www.manning.com/books/natural-language-processing-in-action-second-edition Free access link


Chunk 13:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 45, 'num_chars': 512}
Text: ocessing-in-action-second-edition Free access link: https://www.nltk.org/book/ Exercises in the book: https://github.com/STRZGR/Natural-Language-Processing-with-Python- Analyzing-Text-with-the-Natural-Language-Toolkit?tab=readme-ov-file 3. Tunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language processing with transformers (Revised Edition). " O'Reilly Media, Inc.". Link: https://transformersbook.com Free access link: https://books.google.ch/books?id=7hhyzgEACAAJ INFO 5731 2 Spring 2025
INFO 5731— 


Chunk 14:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 51, 'num_chars': 512}
Text: d=7hhyzgEACAAJ INFO 5731 2 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 Code example of the book: https://github.com/nlp-with-transformers/notebooks Supplementary materials and/or readings (recommended): 4. Python Documentation: https://www.python.org/doc/. 5. Python Forums: https://python-forum.io/. 6. Stackoverflow: https://stackoverflow.com/. 7. NLTK Documentation: https://www.nltk.org/. 8. Google Colab: http://colab.research.google.com/. 9. Success Story of Sylvain Gu


Chunk 15:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 51, 'num_chars': 512}
Text: search.google.com/. 9. Success Story of Sylvain Gugger: https://www.fast.ai/2019/01/02/one-year-of-deep-learning/. 10. Github link of the first textbook: https://github.com/AllenDowney/ThinkPython 11. Github link of the second textbook: https://github.com/totalgood/nlpia 12. Github link of the third textbook: https://github.com/nlp-with-transformers/notebooks 13. Jacob Eisenstein. (2019). Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press, ISBN-13: 


Chunk 16:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: Machine Learning series). The MIT Press, ISBN-13: 978-0262042840. Teaching Philosophy The instructor will take a problem-solving approach and work together with students to understand Natural Language Processing. We will learn how to solve practical data collecting, text processing, information extraction, and text mining problems. He will monitor the progress of students and is open to suggestions from students. Students are expected to study 12-15 hours per week, and to submit their assignments on time to


Chunk 17:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: r week, and to submit their assignments on time to achieve satisfactory class performance. Interaction between the student and the instructor/TA is guaranteed and strongly encouraged. Students who don’t have knowledge and experience in python are expected to spend extra hours on this course. TECHNICAL REQUIREMENTS/ASSISTANCE UIT Help Desk: https://www.python.org/doc/.0 The University of North Texas provides student technical support in the use of Canvas and supported resources. The student help desk may be 


Chunk 18:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 66, 'num_chars': 512}
Text: supported resources. The student help desk may be reached at: Email: helpdesk@unt.edu Phone: 940.565-2324 In-Person: Sage Hall, Room 330 Hours are: • Monday-Thursday 8am-midnight • Friday 8am-8pm • Saturday 9am-5p • Sunday 8am-midnight INFO 5731 3 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 • Canvas technical requirements: https://clear.unt.edu/supported- technologies/canvas/requirements • Other related hardware or software necessary for the course: such as headset/micro


Chunk 19:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: re necessary for the course: such as headset/microphone for synchronous chats, word processor, etc. Minimum Technical Skills Needed Using the Internet and the learning management system Canvas, using email with attachments, creating and submitting files in commonly used word processing program formats, downloading and installing software, using python programs. Student Academic Support Services • Code of Student Conduct: provides Code of Student Conduct along with other useful links • Office of Disability A


Chunk 20:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text: g with other useful links • Office of Disability Access: exists to prevent discrimination based on disability and to help students reach a higher level of independence • Counseling and Testing Services: provides counseling services to the UNT community, as well as testing services; such as admissions testing, computer-based testing, career testing, and other tests • UNT Libraries • UNT Learning Center: provides a variety of services, including tutoring, to enhance the student academic experience • UNT Writi


Chunk 21:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text: nhance the student academic experience • UNT Writing Center: offers free writing tutoring to all UNT students, undergraduate, and graduate, including online tutoring • Succeed at UNT: information regarding how to be a successful student at UNT ASSESSMENT & GRADING Assessments Class Attendance and Participation will not be directly graded. However, students who have 3 absences (including excused and unexcused) will receive an F directly. A student’s grade is composed of the following: • Assignments (50%) • T


Chunk 22:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text: composed of the following: • Assignments (50%) • Term Project (40%) • Discussions (10%) • Extra Credits (10%) Grading Class Attendance and Participation. Class Attendance and Participation will not be directly graded. However, students who have 3 absences (including excused and unexcused) will receive an F directly. Being late once will be counted as 0.5 absence. Not being present during roll call will be considered as an absence. Arriving within 10 minutes after roll call will be considered late. Beyond 10


Chunk 23:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 83, 'num_chars': 512}
Text: after roll call will be considered late. Beyond 10 minutes, it will be considered as an absence. Students who are late should inform the instructor after class to ensure they are marked as late instead of absent. This semester, you must meet your project instructor at least four times. These meetings will also be counted towards your class attendance and participation grade. INFO 5731 4 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 Assignments (50%). The class will have FI


Chunk 24:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: ing 2025 Assignments (50%). The class will have FIVE assignments. These assignments are designed to help students understand important concepts and gain hands-on experience in Python programming, data processing, and problem-solving. Assignments must be prepared and submitted using Overleaf in LaTeX format. Diagrams should be created using appropriate graphics software (e.g., PowerPoint, Excel, or similar tools). Code should be written on Google Colab, and students should submit a link to the Colab notebook


Chunk 25:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text: tudents should submit a link to the Colab notebook with their assignment. Term Project (40%). The term project is structured according to a project-based learning framework. Throughout the semester, students will submit four reports and deliver a final presentation. The first three reports each account for 5% of the total grade (15% in total). The final project submission consists of a final version of the report (5%) and a presentation (10%), making the term project worth 40% of the total course grade. • F


Chunk 26:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 82, 'num_chars': 512}
Text: m project worth 40% of the total course grade. • First Report (5%): The first report should introduce the project by providing: Background and significance of the chosen topic, a preliminary literature review, and an initial research design, specifying the types of data to be collected and the methodology to be used. • Second Report (5%): The second report should detail the progress of data collection, including: Data sources and collection methods and challenges encountered and how they were addressed. • T


Chunk 27:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: enges encountered and how they were addressed. • Third Report (5%): The third report should describe the selected models and evaluation metrics, including: the rationale for selecting these models and metrics, an explanation of the implementation process, and any preliminary results obtained during the project. • Final Submission (15%): The final report (5%) should be a comprehensive document summarizing the entire project, covering all aspects from the background and methodology to results, analysis, and c


Chunk 28:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: ground and methodology to results, analysis, and conclusions. The final presentation (10%) should concisely and effectively communicate the key components of the project, including the problem definition, methodology, results, key insights, and potential future work. Discussions (10%). Each week we will post a discussion question for the week in the discussion area. Each question is worth 1% of the grade. Please preview the class readings and prepare the discussion questions. Extra Credits (100 points). Ext


Chunk 29:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text: cussion questions. Extra Credits (100 points). Extra credits are divided into three parts: Paper reading notes (50 points), in- class presentation (10 points), course evaluation (10 points), and attending research presentations (30 points). • Paper reading notes (30 points): Students can submit up to 10 reading notes, each worth 10 points. Only the first 5 submissions will be counted toward the total 50 points. Reading notes should be one page long and submitted before each lecture. A list of related papers


Chunk 30:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text: tted before each lecture. A list of related papers will be provided for each lecture, and students should select papers from this list. • Peer review (20 points): Peer reviews are conducted as a team. Each team is required to provide at least N suggestions for another team's report, where N ≥ the number of team members. The team receiving feedback must respond to each suggestion individually, specifying whether they acceping or declining the suggestion and a clear and reasonable explanation must be provided


Chunk 31:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text:  clear and reasonable explanation must be provided. • In-Class Presentation (10 points): Each student is required to give at least one in-class presentation on a selected paper during the semester. The presentation schedule will be INFO 5731 5 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 determined during the first class. Completing both the reading notes and the in-class presentation can earn students a total of 60 points. • Course Evaluation (10 points): At the end of t


Chunk 32:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: . • Course Evaluation (10 points): At the end of the semester, students will receive a link to complete the course evaluation. Upon submitting a screenshot showing the completion of the evaluation, students will receive 10 extra points. • Attending Research Presentations (30 points): Throughout the semester, the instructor will announce relevant research presentations (e.g., online research talks). Students can attend up to 3 such presentations. Each attended presentation, with valid proof of attendance, wi


Chunk 33:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 94, 'num_chars': 512}
Text: d presentation, with valid proof of attendance, will earn 10 points, up to a maximum of 30 points. Total Points Possible for Semester/Grading Scale = 1100 1100-900 = A 899-800 = B 799-700 = C 699-600 = D 599 and below = F Grading Table Assignment Points Possible Percentage of Final Grade Assignment Assignment 1 – 100 points 10% Assignment 2 – 100 points 10% Assignment 3 – 100 points 10% Assignment 4 – 100 points 10% Assignment 5 – 100 points 10% Term First Report 50 points 5% Project Second Report 50 points


Chunk 34:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 88, 'num_chars': 512}
Text: eport 50 points 5% Project Second Report 50 points 5% Third Report 50 points 5% Final report 50 pointsb 5% Final presentation 100 points 10% Discussion Each discussion @ 10 points 100 points 10% Extra Readings and presentation @ 50 points 50 points 5% credits In-class presentation @ 10 points 100 points 10% Course evaluation @ 10 points 100 points 10% Attend research meetings @ 30 points 300 points 10% Total Points Possible 1100 points 110% COURSE CALENDAR The contents of the course are organized into 17 we


Chunk 35:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: he contents of the course are organized into 17 weeks. Please refer to Table 1 for lessons, topics, and readings materials. Table 2 lists the suggested study schedule, assignments, quiz, and term project due dates. INFO 5731 6 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 Table 1. Lessons and Readings Lessons Topics Readings Lesson 1 Introduction to Python and NLP, Google Colab, GitHub. Downey: Chapter 1 Course Orientation and Overview Hobson: Chapter 1 Core Concepts Relat


Chunk 36:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: and Overview Hobson: Chapter 1 Core Concepts Related to NLP Lesson 2 Python Basic (1): Integers, Floats, Booleans, Strings, Lists, List Downey: Chapter 2-3, 8, 10- Operations, Tuples, Dictionaries, Sets, List Comprehensions, Files, 14 Functions, I/O Lesson 3 Python Basics (2): Python Modules, Packages, Functions, Downey: Chapter 4-7, 9, 15- Conditionals, for Loops, Recursion, Selections, Exceptions, Classes 18 and Objects, Regular Expression Lesson 4 Accessing Text Copra and Lexical resources PPT Lesson 5 R


Chunk 37:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text: ng Text Copra and Lexical resources PPT Lesson 5 Raw Text Preprocessing and Cleaning: Removing Stop Words, Hobson: Chapter 2, 3 Stemming, Segmentation, and POS-Tagging Lesson 6 Analyzing Sentence Structure PPT Lesson 7 Extracting Information from Text Hobson: Chapter 6-10, PPT Lesson 8 Semantic Analysis of Sentences Hobson: Chapter 4 Lesson 9 Sentiment Analysis of Text PPT Lesson 10 Text Classification and Clustering PPT Lesson 11 Generative AI-Powered NLP applications (Optional) PPT Study Schedule and Due 


Chunk 38:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 94, 'num_chars': 512}
Text: pplications (Optional) PPT Study Schedule and Due Dates Lectures 1, 2, 4, 6, 8, 10, and 11 will be delivered by Dr. Haihua Chen, Lectures 3, 5, 7, and 9 will be delivered by Huyen Thi Ngoc Nguyen. (Assignments and the Project first submission will due on Sunday midnight of the specified week. Quizzes will be available online from 6:00 pm on Monday to 6:00 pm on Friday of the specified week. Term project final report will due on December 6 midnight). The time of the invited talk might be changed based on the


Chunk 39:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 104, 'num_chars': 512}
Text:  of the invited talk might be changed based on the speakers’ schedule. Table 2. Study Schedule and Due Dates Week Dates Meeting Study Focus Individual tasks Group tasks Date 1 Syllabus, Lesson 1 Discussion Student Grouping Jan 13 - Jan 19 Jan 15 Dr. Chen 2 Lesson 2 Assignment 1 Jan 20 - Jan 26 Jan 22 Dr. Chen Discussion 3 Lesson 3 Discussion Jan 27 - Feb 02 Jan 29 Huyen 4 Lesson 4 Discussion Feb 03 - Feb 09 Feb 05 Dr. Chen 5 Lesson 5 Assignment 2 Feb 10 - Feb 16 Feb 12 Huyen Discussion 6 Lesson 6-part 1 Dis


Chunk 40:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 100, 'num_chars': 512}
Text: b 16 Feb 12 Huyen Discussion 6 Lesson 6-part 1 Discussion Feb 17 - Feb 23 Feb 19 Dr. Chen INFO 5731 7 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 7 Lesson 6-part2 Assignment 3 Choosing research Feb 24 - Mar 02 Feb 26 Dr. Chen topic 8 Work on First Report (Term Project Proposal). Meet with each group Mar 03 - Mar 09 Mar 05 separately to discuss the term project. 9 Mar 10 - Mar 16 No class No class 10 Project Topics Presentation Mar 17 - Mar 23 Mar 19 Discussion Peer Revie


Chunk 41:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 105, 'num_chars': 512}
Text: ation Mar 17 - Mar 23 Mar 19 Discussion Peer Review 11 Lesson 7 Discussion Second Report (Meet Mar 24 - Mar 30 Mar 26 Huyen your Instructor) 12 Lesson 8 Assignment 4 Peer Review Mar 31 - Apr 06 Apr 02 Dr. Chen 13 Lesson 9 Discussion Apr 07 - Apr 13 Apr 09 Huyen Lesson 10 Assignment 5 Third Report (Meet Apr 14 - Apr 20 Apr 16 14 Dr. Chen your Instructor) 15 Lesson 11 (or Invited Talk Discussion Peer Review Apr 21 - Apr 27 Apr 23 from Industry) All the extra credit Dr. Chen submissions due Slides of the Proje


Chunk 42:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 85, 'num_chars': 512}
Text: redit Dr. Chen submissions due Slides of the Project Presentation Due Apr 29 Midnight Class Summary Term Project Final 16 Apr 28 - May 04 Apr 30 Term Project Report Due at Presentation May 02 Midnight (Before your Presentation Meet your Instructor) Peer Review 17 Instructor will work on May 05 - May 11 May 07 the grading COURSE EVALUATION Student Evaluation Administration Dates Student feedback is important and an essential part of participation in this course. The student evaluation of instruction is a req


Chunk 43:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text: se. The student evaluation of instruction is a requirement for all organized classes at UNT. The survey will be made available during weeks 13, 14 and 15 of the long semesters to provide students with an opportunity to evaluate how this course is taught. Students will receive an email from "UNT SPOT Course Evaluations via IASystem Notification" (no-reply@iasystem.org) with the survey link. Students should look for the email in their UNT email inbox. Simply click on the link and complete the survey. Once stu


Chunk 44:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: lick on the link and complete the survey. Once students complete the survey they will receive a confirmation email that the survey has been submitted. For additional information, please visit the SPOT website at http://spot.unt.edu/ or email spot@unt.edu. INFO 5731 8 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 COURSE POLICIES Assignment Policy Students should submit the assignments and term project reports via Dropbox at class site in canvas.unt.edu: PDF files with the c


Chunk 45:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: class site in canvas.unt.edu: PDF files with the code link on GitHub included in the file, also with the code uploaded on GitHub, details will be included in each assignment. Examination Policy There are no exams for this course. Instructor Responsibilities and Feedback • Helping students grow and learn • Providing clear instructions for projects and assessments • Answering questions about assignments • Identifying additional resources as necessary • Providing grading rubrics • Reviewing and updating course


Chunk 46:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 86, 'num_chars': 512}
Text: ng grading rubrics • Reviewing and updating course content • The instructor and TA will respond to students’ emails and questions posted to the discussion boards within two days except for the weekends • Assignments grades and feedback will be returned to the students within one week after the submission deadline. Late Work and Missed Work Students are expected to submit discussion assignments and projects on time. The due dates are Monday 11:59 pm of the week specified in Table 2. Study Schedule and Due Da


Chunk 47:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 88, 'num_chars': 512}
Text: ek specified in Table 2. Study Schedule and Due Dates. If an extenuating circumstance such as a medically diagnosed illness or a family emergency arises, which prevents you from submitting your assignments, you should contact the instructor and the TA as soon as possible before the due date. Late work without the permission of the instructor will receive a grade with a 10% penalty (or 10 points out of 100) per day after the due date. A student who is having trouble with assignments is strongly encouraged to


Chunk 48:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: trouble with assignments is strongly encouraged to contact the instructor and the TA as early as possible for personal advising. Course Incomplete Grade The UNT Graduate Catalog (http://catalog.unt.edu/index.php?catoid=16) describes and explains grading policies. A grade of Incomplete (I) will be given only for a justifiable reason and only if the student is passing the course. The student is responsible for meeting with the instructor to request an incomplete and discuss requirements for completing the cou


Chunk 49:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: te and discuss requirements for completing the course. If an incomplete is not removed within the time frame agreed to by the instructor and student, the instructor may assign a grade of F. Withdrawal The UNT Graduate Catalog (http://catalog.unt.edu/index.php?catoid=16) describes and explains withdrawal policies and deadlines. The UNT semester course schedule lists specific deadlines regarding withdrawal. A grade of Withdraw (W) or Withdraw-Failing (WF) will be given depending on a student's INFO 5731 9 Spr


Chunk 50:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text:  be given depending on a student's INFO 5731 9 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 attendance record and grade earned. Please note that a student who simply stops attending class and does not file a withdrawal form may receive an F. Attendance Policy Attending the class meeting is required, students who miss more than 3 class meetings will receive an F directly. Prior to the meeting, please preview the readings for the class and prepare your questions for discuss


Chunk 51:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: r the class and prepare your questions for discussion. You will miss in-class exercises, or quizzes if you do not attend the class. Students’ Responsibility for Their Learning The students are required to follow course schedule and finish the classwork, assignments, quizzes, and term projects. Students are expected to study 12-15 hours per week to achieve satisfactory class performance. Students do not have programming experience are required to find extra materials to study. UNT POLICIES Academic Integrity


Chunk 52:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text: aterials to study. UNT POLICIES Academic Integrity Policy Academic Integrity Standards and Consequences. According to UNT Policy 06.003, Student Academic Integrity, academic dishonesty occurs when students engage in behaviors including, but not limited to cheating, fabrication, facilitating academic dishonesty, forgery, plagiarism, and sabotage. A finding of academic dishonesty may result in a range of academic penalties or sanctions ranging from admonition to expulsion from the University. ADA Policy UNT m


Chunk 53:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: to expulsion from the University. ADA Policy UNT makes reasonable academic accommodation for students with disabilities. Students seeking accommodation must first register with the Office of Disability Accommodation (ODA) to verify their eligibility. If a disability is verified, the ODA will provide a student with an accommodation letter to be delivered to faculty to begin a private discussion regarding one’s specific course needs. Students may request accommodations at any time; however, ODA notices of acc


Chunk 54:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: modations at any time; however, ODA notices of accommodation should be provided as early as possible in the semester to avoid any delay in implementation. Note that students must obtain a new letter of accommodation for every semester and must meet with each faculty member prior to implementation in each class. For additional information see the ODA website at disability.unt.edu. Emergency Notification & Procedures UNT uses a system called Eagle Alert to quickly notify students with critical information in 


Chunk 55:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: ckly notify students with critical information in the event of an emergency (i.e., severe weather, campus closing, and health and public safety emergencies like chemical spills, fires, or violence). In the event of a university closure, please refer to Blackboard for contingency plans for covering course materials. Retention of Student Records Student records pertaining to this course are maintained in a secure location by the instructor of record. All records such as exams, answer sheets (with keys), and w


Chunk 56:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: ds such as exams, answer sheets (with keys), and written papers submitted during the duration of the course are kept for at least one calendar year after course completion. Course work completed via the Blackboard online system, including grading information and comments, is also stored in a safe electronic environment for one year. Students have the right to view their individual records; however, INFO 5731 10 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 information abou


Chunk 57:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: r Information Systems Spring 2025 information about student’s records will not be divulged to other individuals without proper written consent. Students are encouraged to review the Public Information Policy and the Family Educational Rights and Privacy Act (FERPA) laws and the University’s policy. See UNT Policy 10.10, Records Management and Retention for additional information. Acceptable Student Behavior Student behavior that interferes with an instructor’s ability to conduct a class or other students' o


Chunk 58:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: ’s ability to conduct a class or other students' opportunity to learn is unacceptable and disruptive and will not be tolerated in any instructional forum at UNT. Students engaging in unacceptable behavior will be directed to leave the classroom and the instructor may refer the student to the Dean of Students to consider whether the student's conduct violated the Code of Student Conduct. The University's expectations for student conduct apply to all instructional forums, including University and electronic c


Chunk 59:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: onal forums, including University and electronic classroom, labs, discussion groups, field trips, etc. The Code of Student Conduct can be found at deanofstudents.unt.edu/conduct. Access to Information - Eagle Connect Students’ access point for business and academic services at UNT is located at: my.unt.edu. All official communication from the University will be delivered to a student’s Eagle Connect account. For more information, please visit the website that explains Eagle Connect and how to forward e-mail


Chunk 60:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: t explains Eagle Connect and how to forward e-mail: eagleconnect.unt.edu/. Sexual Assault Prevention UNT is committed to providing a safe learning environment free of all forms of sexual misconduct, including sexual harassment sexual assault, domestic violence, dating violence, and stalking. Federal laws (Title IX and the Violence Against Women Act) and UNT policies prohibit discrimination on the basis of sex, and therefore prohibit sexual misconduct. If you or someone you know is experiencing sexual harass


Chunk 61:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text:  or someone you know is experiencing sexual harassment, relationship violence, stalking, and/or sexual assault, there are campus resources available to provide support and assistance. UNT’s Survivor Advocates can assist a student who has been impacted by violence by filing protective orders, completing crime victim’s compensation applications, contacting professors for absences related to an assault, working with housing to facilitate a room change where appropriate, and connecting students to other resourc


Chunk 62:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: ropriate, and connecting students to other resources available both on and off campus. The Survivor Advocates can be reached at SurvivorAdvocate@unt.edu or by calling the Dean of Students Office at 940-565- 2648. Additionally, alleged sexual misconduct can be non-confidentially reported to the Title IX Coordinator at oeo@unt.edu or at (940) 565 2759. Important Notice for F-1 Students taking Distance Education Courses Federal Regulation To read detailed Immigration and Customs Enforcement regulations for F-1


Chunk 63:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: ration and Customs Enforcement regulations for F-1 students taking online courses, please go to the Electronic Code of Federal Regulations website at http://www.ecfr.gov/. The specific portion concerning distance education courses is located at Title 8 CFR 214.2 Paragraph (f)(6)(i)(G). The paragraph reads: (G) For F-1 students enrolled in classes for credit or classroom hours, no more than the equivalent of one class or three credits per session, term, semester, trimester, or quarter may be counted toward t


Chunk 64:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: ter, trimester, or quarter may be counted toward the full course of study requirement if the class is taken on-line or through distance education and does not require the student's physical attendance for classes, examination or other purposes integral to INFO 5731 11 Spring 2025
INFO 5731— Computational Methods for Information Systems Spring 2025 completion of the class. An on-line or distance education course is a course that is offered principally through the use of television, audio, or computer transmi


Chunk 65:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text:  the use of television, audio, or computer transmission including open broadcast, closed circuit, cable, microwave, or satellite, audio conferencing, or computer conferencing. If the F-1 student's course of study is in a language study program, no on-line or distance education classes may be considered to count toward a student's full course of study requirement. University of North Texas Compliance To comply with immigration regulations, an F-1 visa holder within the United States may need to engage in an 


Chunk 66:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 82, 'num_chars': 512}
Text: within the United States may need to engage in an on-campus experiential component for this course. This component (which must be approved in advance by the instructor) can include activities such as taking an on-campus exam, participating in an on-campus lecture or lab activity, or other on-campus experience integral to the completion of this course. If such an on-campus activity is required, it is the student’s responsibility to do the following: (1) Submit a written request to the instructor for an on-ca


Chunk 67:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 86, 'num_chars': 512}
Text: t a written request to the instructor for an on-campus experiential component within one week of the start of the course. (2) Ensure that the activity on campus takes place and the instructor documents it in writing with a notice sent to the International Student and Scholar Services Office. ISSS has a form available that you may use for this purpose. Because the decision may have serious immigration consequences, if an F-1 student is unsure about his or her need to participate in an on-campus experiential 


Chunk 68:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 67, 'num_chars': 512}
Text:  need to participate in an on-campus experiential component for this course, s/he should contact the UNT International Student and Scholar Services Office (telephone 940-565-2195 or email internationaladvising@unt.edu) to get clarification before the one-week deadline. Student Verification UNT takes measures to protect the integrity of educational credentials awarded to students enrolled in distance education courses by verifying student identity, protecting student privacy, and notifying students of any sp


Chunk 69:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text:  student privacy, and notifying students of any special meeting times/locations or additional charges associated with student identity verification in distance education courses. See UNT Policy 07-002 Student Identity Verification, Privacy, and Notification and Distance Education Courses. Use of Student Work A student owns the copyright for all work (e.g., software, photographs, reports, presentations, and email postings) he or she creates within a class and the University is not entitled to use any student


Chunk 70:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 96, 'num_chars': 512}
Text:  the University is not entitled to use any student work without the student’s permission unless all of the following criteria are met: • The work is used only once. • The work is not used in its entirety. • The use of the work does not affect any potential profits from the work. • The student is not identified. • The work is identified as student work. If the use of the work does not meet all of the above criteria, then the University office or department using the work must obtain the student’s written per


Chunk 71:
Document ID: 6c015e8d-e923-4000-9853-e4750482626b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/INFO5731_Syllabus_Sping2025_updated_01142025.pdf', 'folder_name': 'Module 1', 'num_tokens': 23, 'num_chars': 143}
Text: ing the work must obtain the student’s written permission. Download the UNT System Permission, Waiver and Release Form INFO 5731 12 Spring 2025


Chunk 72:
Document ID: 04271ef9-2e55-4d76-b7d3-b82df019e037
Metadata: {'file_name': 'Introduction to Google Colab and Github (1) (1).docx', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Introduction to Google Colab and Github (1) (1).docx', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text: Assignment and In-class exercises submission using Google Colab and GitHub

We will use Google Colab + GitHub for in-class exercises and assignment submission. Please follow the following instructions to create a repository for this course and connect it with Google Colab.

Step 1: Create a new account on GitHub or sign in any existing account: https://github.com/



Step 2: Create a new repository for this course with a common convention for naming as follows: your firstname_INFO5731_Fall2024 (your first n


Chunk 73:
Document ID: 04271ef9-2e55-4d76-b7d3-b82df019e037
Metadata: {'file_name': 'Introduction to Google Colab and Github (1) (1).docx', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Introduction to Google Colab and Github (1) (1).docx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: ws: your firstname_INFO5731_Fall2024 (your first name_INFO5731_Fall2024)



Click “New” button on the left side of the window, and you will see the following screenshot, fill the information, and click the “create repository” button.







Now you have successfully created a repository for this course in your account, as shown in the following screenshot:





Step 3: Login Google Colab with your Google account by clicking this link: 

https://colab.research.google.com/notebooks/welcome.ipynb#



Step 4: C


Chunk 74:
Document ID: 04271ef9-2e55-4d76-b7d3-b82df019e037
Metadata: {'file_name': 'Introduction to Google Colab and Github (1) (1).docx', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Introduction to Google Colab and Github (1) (1).docx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: h.google.com/notebooks/welcome.ipynb#



Step 4: Create the first “.ipynb” file by click “File” to choose “New notebook in drive” to write your python code:



Rename the file, for example, “in_class_exercise_01”:



Then add Code module by clicking “+ Code” button or add Text module by clicking “+ Text” button.

Look at the following example:

Click “Run” under “Runtime” to run your code. Remember to save you code (by using Ctrl+S keyboard shortcut)





Step 5: Save your code in the repository you have ju


Chunk 75:
Document ID: 04271ef9-2e55-4d76-b7d3-b82df019e037
Metadata: {'file_name': 'Introduction to Google Colab and Github (1) (1).docx', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Introduction to Google Colab and Github (1) (1).docx', 'folder_name': 'Module 1', 'num_tokens': 89, 'num_chars': 512}
Text: ep 5: Save your code in the repository you have just created on GitHub.

Click “Save a copy in GitHub” under “File”, as shown below:



Select the repository you have just created for the course, and create a file path to your located file, as shown in the following screenshot:



Now your code has been saved successfully on GitHub:



Step 6: Copy the link for submitting as your assignment, as shown in the following screenshot:



The GitHub code is publicly accessible, which means that you may find your c


Chunk 76:
Document ID: 04271ef9-2e55-4d76-b7d3-b82df019e037
Metadata: {'file_name': 'Introduction to Google Colab and Github (1) (1).docx', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Introduction to Google Colab and Github (1) (1).docx', 'folder_name': 'Module 1', 'num_tokens': 30, 'num_chars': 179}
Text: y accessible, which means that you may find your classmates’ submissions. However, if we figure that you copy others’ code, you will certainly be given a zero, without any excuse!


Chunk 77:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text: Haihua Chen, Ph.D. Department of Information Science, UNT Aug 16, 2024
College of Information
Computational environment and resources
8/16/24
1
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
Content: Introduction to GitHub Intr


Chunk 78:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: es in the Lab
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
What is GitHub?
GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. Over 65 milli


Chunk 79:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text:  together on projects from anywhere. Over 65 million developers shape the future of software, together.
8/16/24
4
How to create a GitHub project?
Step 1: Create an account on GitHub: https://github.com/ Step 2: Create a new repository for this course with a common convention for naming as follows: for example, your first name_INFO5731_Spring2020
8/16/24
5
How to create a GitHub project?
8/16/24
6
Step 3: Login Google Colab with your Google account by clicking this link: https://colab.research.google.com/not


Chunk 80:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: g this link: https://colab.research.google.com/notebooks/welcome.ipynb# Step 4: Create the first .ipynb file by click “File” to choose “New Python 3 notebook” to write your python code: Rename the file, for example, “in_class_exercise_01”:
How to create a GitHub project?
Then add Code module by clicking “+ Code” button or add Text module by clicking “+ Text” button. Look at the following example: Click “Run” under “Runtime” to run your code. Remember to save you code (by using Ctrl+S keyboard shortcut)
8/16


Chunk 81:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 89, 'num_chars': 512}
Text:  you code (by using Ctrl+S keyboard shortcut)
8/16/24
7
How to create a GitHub project?
Step 5: Save your code in the repository you have just created on GitHub. Click “Save a copy in GitHub” under “File”, as shown below: Select the repository you have just created for the course, and create a file path to your located file, as shown in the following screenshot:
8/16/24
8
How to create a GitHub project?
Now your code has been saved successfully on GitHub:
8/16/24
9
Content: Introduction to GitHub Introducti


Chunk 82:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: 16/24
9
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
The history of Google Colab
8/16/24
11
Google is quite aggressive in AI research. Over many years, Google developed AI framework called TensorFlow and a development tool ca


Chunk 83:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 85, 'num_chars': 512}
Text: mework called TensorFlow and a development tool called Colaboratory. Today TensorFlow is open-sourced and since 2017, Google made Colaboratory free for public use. Colaboratory is now known as Google Colab or simply Colab. Another attractive feature that Google offers to the developers is the use of GPU. Colab supports GPU and it is totally free. The reasons for making it free for public could be to make its software a standard in the academics for teaching machine learning and data science. It may also hav


Chunk 84:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: machine learning and data science. It may also have a long-term perspective of building a customer base for Google Cloud APIs which are sold per-use basis. Irrespective of the reasons, the introduction of Colab has eased the learning and development of machine learning applications.
What is Google Colab?
8/16/24
12
Write and execute code in Python Document your code that supports mathematical equations Create/Upload/Share notebooks Import/Save notebooks from/to Google Drive Import/Publish notebooks from Git


Chunk 85:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 83, 'num_chars': 512}
Text: /to Google Drive Import/Publish notebooks from GitHub Import external datasets e.g. from Kaggle Integrate PyTorch, TensorFlow, Keras, OpenCV Free Cloud service with free GPU
Your First Colab Notebook
8/16/24
13
Step 1 − Open the following URL in your browser − https://colab.research.google.com Your browser would display the following screen (assuming that you are logged into your Google Drive) Step 2 − Click on the NEW PYTHON 3 NOTEBOOK link at the bottom of the screen. A new notebook would open up as shown


Chunk 86:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 87, 'num_chars': 512}
Text:  the screen. A new notebook would open up as shown in the screen below.
Your First Colab Notebook
8/16/24
14
Entering Code Executing Code Adding Code Cells Run All Changing Cell Order Deleting Cell
Documenting Your Code
8/16/24
15
Markdown Examples Mathematical Equations Code for Sample Equations
Saving Your Work
8/16/24
16
Saving to Google Drive Saving to GitHub
Sharing Notebook
8/16/24
17
You may enter the email IDs of people with whom you would like to share the current document. You can set the kind of 


Chunk 87:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 88, 'num_chars': 512}
Text: are the current document. You can set the kind of access by selecting from the three options shown in the above screen. Click on the Get shareable link option to get the URL of your notebook. You will find options for whom to share as follows − Specified group of people Colleagues in your organization Anyone with the link All public on the web
Invoking System Commands
8/16/24
18
Simple Commands Getting Remote Data Cloning Git Repository System Aliases: !ls /bin
Executing External Python Files
8/16/24
19
Mou


Chunk 88:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 89, 'num_chars': 512}
Text: bin
Executing External Python Files
8/16/24
19
Mounting Drive: Tools / Command palette Type a few letters like “m” in the search box to locate the mount command. Select Mount Drive command from the list. The following code would be inserted in your Code cell. If you run this code, you will be asked to enter the authentication code. The corresponding screen looks as shown below −
Executing External Python Files
8/16/24
20
Mounting Drive: Tools / Command palette Open the above URL in your browser. You will be


Chunk 89:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 98, 'num_chars': 512}
Text: te Open the above URL in your browser. You will be asked to login to your Google account. Now, you will see the following screen − If you grant the permissions, you will receive your code as follows − Cut-n-paste this code in the Code cell and hit ENTER. After a while, the drive will be mounted as seen in the screenshot below − Now, you are ready to use the contents of your drive in Colab.
Executing External Python Files
8/16/24
21
Listing Drive Contents You can list the contents of the drive using the ls c


Chunk 90:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 71, 'num_chars': 512}
Text:  can list the contents of the drive using the ls command as follows − [!ls "/content/drive/My Drive/Colab Notebooks"] This command will list the contents of your Colab Notebooks folder. The sample output of my drive contents are shown here − [Greeting.ipynb hello.py LogisticRegressionCensusData.ipynb LogisticRegressionDigitalOcean.ipynb MyFirstColabNotebook.ipynb SamplePlot.ipynb] Running Python Code Now, let us say that you want to run a Python file called hello.py stored in your Google Drive. Type the fol


Chunk 91:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: hello.py stored in your Google Drive. Type the following command in the Code cell − [!python3 "/content/drive/My Drive/Colab Notebooks/hello.py"] The contents of hello.py are given here for your reference − [print("Welcome to TutorialsPoint!")] You will now see the following output − [Welcome to TutorialsPoint!]
Graphical Outputs
8/16/24
22
Colab also supports rich outputs such as charts.
Code Editing Help
8/16/24
23
Function List Step 1 − Open a new notebook and type in the following code in the Code cell 


Chunk 92:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 101, 'num_chars': 512}
Text: k and type in the following code in the Code cell − Step 2 − Run the code by clicking on the Run icon in the left panel of the Code cell. Add another Code cell and type in the following code − Function Documentation Colab gives you the documentation on any function or class as a context-sensitive help.
Adding Forms
8/16/24
24
Adding Form Suppose, you want a user set time delay instead of a fixed delay of 5 seconds. For this, you can add a Form to the Code cell to accept the sleep time. Open a new notebook. 


Chunk 93:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 101, 'num_chars': 512}
Text: ll to accept the sleep time. Open a new notebook. Click on the Options (vertically-dotted) menu. A popup menu shows up as seen in the screenshot below − Now, select Add a form option. It will add the form to your Code cell with a Default title as seen in the screenshot here − To change the title of the form, click on the Settings button (pencil icon on the right). It will pop up a settings screen as shown here: Change the form title to “Form” and save the form. You may use some other name of your choice. No


Chunk 94:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 100, 'num_chars': 512}
Text: rm. You may use some other name of your choice. Notice that it adds the @title to your code cell.
Adding Forms
8/16/24
25
Adding Form Fields To add a form field, click the Options menu in the Code cell, click on the Form to reveal the submenus. The screen will look as shown below − Select Add a form field menu option. A dialog pops up as seen here − Leave the Form field type to input. Change the Variable name to sleeptime and set the Variable type to integer. Save the changes by clicking the Save button. Yo


Chunk 95:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: . Save the changes by clicking the Save button. Your screen will now look like the following with the sleeptime variable added into the code.
Adding Forms
8/16/24
26
Testing Form Inputting Text Dropdown List Date Input
Installing ML Libraries
8/16/24
27
!pip install or !apt-get install !pip install scikit-learn !pip install -q keras !pip3 install torch torchvision !pip install tensorflow !apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python !pip install -q xgboost==0.4a30 !apt-get -qq i


Chunk 96:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: hon !pip install -q xgboost==0.4a30 !apt-get -qq install -y graphviz && pip install -q pydot Python package index: https://pypi.org/
Using Free GPU
8/16/24
28
Enabling GPU To enable GPU in your notebook, select the following menu options − [Runtime / Change runtime type] Testing for GPU Listing Devices Checking RAM
Create your ML and DL models on Google Colab
8/16/24
29
Your First Machine Learning Model: https://colab.research.google.com/drive/1oQdzTY2wVjO6LqTCUlzR5BN5GL905fRq#scrollTo=1yE1ar-m5yEv Step-by-


Chunk 97:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 18, 'num_chars': 512}
Text: qTCUlzR5BN5GL905fRq#scrollTo=1yE1ar-m5yEv Step-by-step: https://machinelearningmastery.com/machine-learning-in-python-step-by-step/ Datasets: (1) https://archive.ics.uci.edu/ml/datasets.php (2) https://www.paperswithcode.com/datasets?fbclid=IwAR3XPvQwYKwPdPoW7iisKiOAFjlLR9dfG7hPvSTHE0tisclT-dZxDvTxHHg Code: https://github.com/maykulkarni/Machine-Learning-Notebooks
Your First Deep Learning Model: https://colab.research.google.com/drive/1oQdzTY2wVjO6LqTCUlzR5BN5GL905fRq#scrollTo=1yE1ar-m5yEv Step-by-step: htt


Chunk 98:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 27, 'num_chars': 512}
Text: N5GL905fRq#scrollTo=1yE1ar-m5yEv Step-by-step: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ Datasets: (1) https://archive.ics.uci.edu/ml/datasets.php (2) https://www.paperswithcode.com/datasets?fbclid=IwAR3XPvQwYKwPdPoW7iisKiOAFjlLR9dfG7hPvSTHE0tisclT-dZxDvTxHHg Code: https://neptune.ai/blog/how-to-use-google-colab-for-deep-learning-complete-tutorial
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tool


Chunk 99:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 52, 'num_chars': 512}
Text: o Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
How to use Anaconda and PyCharm?
Getting started with Anaconda: https://docs.anaconda.com/anaconda/user-guide/getting-started/ Getting started with PyCharm: https://www.jetbrains.com/help/pycharm/creating-and-running-your-first-python-project.html#edit-fi


Chunk 100:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 65, 'num_chars': 512}
Text: and-running-your-first-python-project.html#edit-file
8/16/24
31
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
Pandas is a package providing fast, flexible, and expressive data structures designed to make working with structure


Chunk 101:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive. Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. SciPy builds on NumPy and provides a large number of functions that operate on NumPy arrays and are useful for different types of scientific and engineering applications. Matplotlib is a compreh


Chunk 102:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 85, 'num_chars': 512}
Text:  engineering applications. Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.
8/16/24
33
Pandas + NumPy + Scipy + Matplotlib
8/16/24
34
What kind of data does Pandas handle? How do I read and write tabular data? How do I select a subset of a table? How to create plots in pandas? How to create new columns derived from existing columns? How to calculate summary statistics? How to reshape the layout of tables? How to combine data from multiple tables?


Chunk 103:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text:  tables? How to combine data from multiple tables? How to handle time series data? How to manipulate textual data?
Source: https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html
Pandas
点击添加标题
点击添加标题
8/16/24
35
Object creation
8/16/24
36
Viewing data
While standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods. There are many other functions for selection, I


Chunk 104:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 85, 'num_chars': 512}
Text: s. There are many other functions for selection, I will send you some materials
8/16/24
37
Selection
Pandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.
8/16/24
38
Merge
01 - Lesson: - Importing libraries - Creating data sets - Creating data frames - Reading from CSV – Exporting to CSV - Finding maximums - Plotting data 02 - Lesso


Chunk 105:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 88, 'num_chars': 512}
Text:  CSV - Finding maximums - Plotting data 02 - Lesson: - Reading from TXT - Exporting to TXT - Selecting top/bottom records - Descriptive statistics - Grouping/sorting data 03 - Lesson: - Creating functions - Reading from EXCEL - Exporting to EXCEL - Outliers - Lambda functions - Slice and dice data 04 - Lesson: - Adding/deleting columns - Index operations 05 - Lesson: - Stack/Unstack/Transpose functions 06 - Lesson: - GroupBy function 07 - Lesson: - Ways to calculate outliers 08 - Lesson: - Read from Microso


Chunk 106:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text: alculate outliers 08 - Lesson: - Read from Microsoft SQL databases 09 - Lesson: - Export to CSV/EXCEL/TXT 10 - Lesson: - Converting between different kinds of formats 11 - Lesson: - Combining data from various sources
8/16/24
39
Lessons for New pandas Users
Fundamental package for scientific computing with Python
Efficient, In-memory, Contiguous, Homogeneous
N-dimensional array object
An ndarray is a multidimensional container of items of the same type and size. The number of dimensions and items in an arra


Chunk 107:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text: ize. The number of dimensions and items in an array is defined by its shape, which is a tuple of N positive integers that specify the sizes of each dimension.
Linear algebra, Fourier transform, random number capabilities
Building block for other packages
SciPy, Pandas, Matplotlib, scikit-learn
suited to many applications
Image processing, Signal processing, Linear algebra, etc.
8/16/24
40
点击添加标题
点击添加标题
8/16/24
41
NumPy
点击添加标题
8/16/24
42
Attributes, arithmetic, vector
点击添加标题
8/16/24
43
Array broadcasting
8/1


Chunk 108:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 72, 'num_chars': 512}
Text: c, vector
点击添加标题
8/16/24
43
Array broadcasting
8/16/24
44
Matrix operations
8/16/24
45
Array operations along axes
Single Array Iteration (red: reshaping array)
Controlling Iteration Order (‘K’, ‘C’, ‘F’)
8/16/24
46
Iterating over arrays
Index Multi-index Be used to index into other arrays
8/16/24
47
Indexing
Matrix operations (import numpy.linalg)
Linear algebra (import numpy.linalg)
Fourier transform (import numpy.fft)
Random sampling (import numpy.random)
8/16/24
48
Other Useful Sub packages
SciPy is a l


Chunk 109:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text: 
8/16/24
48
Other Useful Sub packages
SciPy is a library of algorithms and mathematical tools built to work with NumPy arrays.
8/16/24
49
SciPy
点击添加标题
点击添加标题
scipy.cluster.vq for vector quantization and k-means clustering scipy.cluster.hierarchy for hierarchical and agglomerative clustering
8/16/24
50
Cluster
点击添加标题
点击添加标题
The most common IO in SciPy is to import and export Matlab files using loadmat/savemat.
scipy.io contains modules, classes and functions to read and write data to a variety of formats: Ma


Chunk 110:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: to read and write data to a variety of formats: Matlab WEKA Matrix Market Wav Netcdf
8/16/24
51
io(Input/output)
The scipy.linalg module provides standard linear algebra operations, some of the main functions are:
8/16/24
52
Linalg (Linear algebra)
8/16/24
53
Linalg (some examples)
8/16/24
54
Signal generation
Optimization is the problem of finding a numerical solution to a minimization or equality. The scipy.optimize module provides useful algorithms for: (1) Function minimization (scalar or multi-dimensio


Chunk 111:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: 1) Function minimization (scalar or multi-dimensional); (2) Curve fitting; (3) Root finding
8/16/24
55
Optimization
χ2-test: chi2_contingency() computes the chi-square statistic and p-value for the hypothesis test of independence of the observed frequencies in the contingency table observed. t-test: ttest ind() calculates the T-test for the means of TWO INDEPENDENT samples of scores Some other tests
8/16/24
56
Statistics
Since python ranges start with 0, the default x vector has the same length as y but sta


Chunk 112:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text:  default x vector has the same length as y but starts with 0. Hence the x data are [0,1,2,3]
8/16/24
57
Matplotlib
8/16/24
58
Matplotlib
8/16/24
59
Matplotlib
8/16/24
60
Matplotlib
8/16/24
61
Matplotlib
8/16/24
62
Matplotlib
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introd


Chunk 113:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text: ools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
8/16/24
64
NLTK is a leading platform for building Python programs to work with human language data. TextBlob is a Python library for processing textual data. Polyglot is a natural language pipeline which supports massive multilingual applications. spaCy is completely optimized and highly accurate library widely used in deep learning. AllenNLP includes reference implementations of high-quality models for both core


Chunk 114:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: plementations of high-quality models for both core NLP problems (e.g., semantic role labeling) and NLP applications (e.g., textual entailment). Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.
NLTK + TextBlob + polyglot + spaCy + AllenNLP + Gensim
8/16/24
65
1950- NLP started when Alan Turing published an article called "Machine and Intelligence." 1950- Attempts to automate translation between Russian and English 1960- The work of Chomsky and oth


Chunk 115:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: sian and English 1960- The work of Chomsky and others on formal language theory and generative syntax 1990- Probabilistic and data-driven models had become quite standard 2000- A Large amount of spoken and textual data become available
History & Components
8/16/24
66
How to Download & Install NLTK on Windows/Mac
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
67
Tokenize Words and Sentences with NLTK
Source: https://w


Chunk 116:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: ze Words and Sentences with NLTK
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
68
POS (Part-Of-Speech) Tagging & Chunking with NLTK POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context. It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also cal


Chunk 117:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text: ken (Parts of Speech) to each word. It is also called grammatical tagging. Chunking in NLP is a process to take small pieces of information and group them into large units. The primary use of Chunking is making groups of "noun phrases." It is used to add structure to the sentence by following POS tagging combined with regular expressions. The resulted group of words are called "chunks." It is also called shallow parsing.
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.ka


Chunk 118:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 48, 'num_chars': 512}
Text: utorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
69
Stemming and Lemmatization with Python NLTK
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
70
WordNet (a lexical database) with NLTK: Finding Synonyms for words in Python
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
71
Wo


Chunk 119:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 51, 'num_chars': 512}
Text: m/alvations/basic-nlp-with-nltk
NLTK
8/16/24
71
Word Embeddings A word embedding is a representation for text where words that have the same meaning have a similar representationlearned
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
72
Word Embeddings
Source: https://www.guru99.com/nltk-tutorial.html Play with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk
NLTK
8/16/24
73
Noun phrase extraction Par


Chunk 120:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 59, 'num_chars': 512}
Text: th-nltk
NLTK
8/16/24
73
Noun phrase extraction Part-of-speech tagging Sentiment analysis Classification (Naive Bayes, Decision Tree) Tokenization (splitting text into words and sentences) Word and phrase frequencies Parsing n-grams Word inflection (pluralization and singularization) and lemmatization Spelling correction Add new models or languages through extensions WordNet integration
Source: https://textblob.readthedocs.io/en/dev/
TextBlob
8/16/24
74
Tokenization (165 Languages) Language detection (196 La


Chunk 121:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 57, 'num_chars': 512}
Text: ization (165 Languages) Language detection (196 Languages) Named Entity Recognition (40 Languages) Part of Speech Tagging (16 Languages) Sentiment Analysis (136 Languages) Word Embeddings (137 Languages) Morphological analysis (135 Languages) Transliteration (69 Languages)
Source: https://polyglot.readthedocs.io/en/latest/index.html
polyglot
8/16/24
75
Non-destructive tokenization Named entity recognition Support for 53+ languages 17 statistical models for 11 languages pretrained word vectors State-of-the-a


Chunk 122:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 58, 'num_chars': 512}
Text: 1 languages pretrained word vectors State-of-the-art speed Easy deep learning integration Part-of-speech tagging Labelled dependency parsing Syntax-driven sentence segmentation Built in visualizers for syntax and NER Convenient string-to-hash mapping Export to numpy data arrays Efficient binary serialization Easy model packaging and deployment Robust, rigorously evaluated accuracy
Source: https://spacy.io/usage/spacy-101#whats-spacy
spaCy
8/16/24
76
Reading Comprehension Named Entity Recognition Sentiment A


Chunk 123:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 55, 'num_chars': 512}
Text: Comprehension Named Entity Recognition Sentiment Analysis Dependency Parsing Constituency Parsing Semantic Role Labeling Co-reference Resolution Semantic parsing Textual Entailment Language Modeling
Source: https://demo.allennlp.org/reading-comprehension
AllenNLP
8/16/24
77
Latent Semantic Analysis (LSA/LSI/SVD) Latent Dirichlet Allocation (LDA) Random Projections (RP) Hierarchical Dirichlet Process (HDP) word2vec deep learning Distributed computing: can run Latent Semantic Analysis and Latent Dirichlet All


Chunk 124:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 63, 'num_chars': 512}
Text:  Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers.
Source: https://radimrehurek.com/gensim/auto_examples/index.html
Gensim
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
8/16/24
79
scikit-learn


Chunk 125:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text: ional Resources in the Lab
8/16/24
79
scikit-learn is a free software machine learning library for the Python. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. auto-sklearn frees a machine learning user from algorithm selection and hyperparameter tuning. It leverages recent advantages in Bayesian 


Chunk 126:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 72, 'num_chars': 512}
Text: uning. It leverages recent advantages in Bayesian optimization, meta-learning and ensemble construction. transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. Keras is one of the most popular and open-source neural network libraries for Python. Keras was soon supported in Tensor


Chunk 127:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: ies for Python. Keras was soon supported in TensorFlow’s core library making it accessible on top of TensorFlow. TensorFlow is a fast, flexible, and scalable open-source machine learning (also deep learning) library for research and production. Together with TensorFlow, PyTorch is considered as one of the best Machine Learning and Deep Learning framework.
scikit-learn + auto-sklearn + transformers + Keras + TensorFlow + PyTorch
8/16/24
80
Preprocessing: Feature extraction and normalization. Dimensionality r


Chunk 128:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 54, 'num_chars': 512}
Text: ure extraction and normalization. Dimensionality reduction: Reducing the number of random variables to consider. Model selection: Comparing, validating and choosing parameters and models. Classification: Identifying which category an object belongs to. Regression: Predicting a continuous-valued attribute associated with an object. Clustering: Automatic grouping of similar objects into sets.
Learning Scikit-learn: https://www.youtube.com/watch?v=rvVkVsG49uU Scikit-learn website: https://scikit-learn.org/stab


Chunk 129:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 54, 'num_chars': 512}
Text: cikit-learn website: https://scikit-learn.org/stable/ Scikit-Learn ML on Kaggle example: https://www.kaggle.com/enerrio/scikit-learn-ml-from-start-to-finish
Scikit-learn
8/16/24
81
Classification Multi-label Classification Regression Continuous and categorical data Iterating over the models Using custom metrics Pandas Train and Test inputs Train a single configuration Resampling strategies Parallel usage (manual) Parallel usage (n_jobs) Random search Sequential usage Successive Halving Extending with a new 


Chunk 130:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 56, 'num_chars': 512}
Text: ial usage Successive Halving Extending with a new classifier Extending with a new regressor Extending with a new preprocessor Restrict hyperparameters for a component
Learning auto-sklearn with examples: https://automl.github.io/auto-sklearn/master/manual.html
auto-sklearn
8/16/24
82
transformers
Models: https://huggingface.co/models
8/16/24
83
Neural layers Activation and cost functions Objectives Batch normalization Dropout Pooling
Example: Text Classification With Python and Keras https://www.kaggle.com/


Chunk 131:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 47, 'num_chars': 512}
Text: tion With Python and Keras https://www.kaggle.com/sanikamal/text-classification-with-python-and-keras
Keras
8/16/24
84
Handling deep neural networks Natural Language Processing Partial Differential Equation Abstraction capabilities Image, Text, and Speech recognition Effortless collaboration of ideas and code Core Task: Build Deep Learning models
Example: Text Classification With Python and TensorFlow https://www.kaggle.com/iarunava/google-text-classification-notebook https://colab.research.google.com/githu


Chunk 132:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 67, 'num_chars': 512}
Text: n-notebook https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb
TensorFlow
8/16/24
85
Keras vs. tf.keras: What’s the difference in TensorFlow 2.0? Keras vs. TensorFlow – Which one is better and which one should I learn? Define your model using the easy to use interface of Keras. And then drop down into TensorFlow if you need (1) specific TensorFlow functionality or (2) need to implement a custom feature that Keras does not support but Tens


Chunk 133:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 67, 'num_chars': 512}
Text: ustom feature that Keras does not support but TensorFlow does.
Keras vs. tf.keras: What’s the difference in TensorFlow 2.0? Tensorflow Vs. Keras: Comparison by building a model for image classification
Keras VS. TensorFlow
8/16/24
86
Basic Pytorch Workflow
PyTorch
Tensors — torch.Tensor Optimizers — torch.optim module Neural Networks — nn module Autograd Core task: Developing and training deep learning models
Understand the PyTorch Modules by examples: https://pytorch.org/tutorials/beginner/pytorch_with_exa


Chunk 134:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 52, 'num_chars': 512}
Text: ://pytorch.org/tutorials/beginner/pytorch_with_examples.html#autograd
8/16/24
87
Easy to use API Python support Dynamic computation graphs TorchScript Distributed Training Tools and Libraries
PyTorch vs TensorFlow: Difference you need to know
PyTorch
Secure Model Building ML Production Anywhere Robust Experimentation for Research
TensorFlow
PyTorch VS. TensorFlow
8/16/24
88
Source: https://www.youtube.com/watch?time_continue=1&v=DmI58jz2i6w&feature=emb_logo
Keras VS. PyTorch VS. TensorFlow
Content: Introduc


Chunk 135:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 67, 'num_chars': 512}
Text: Keras VS. PyTorch VS. TensorFlow
Content: Introduction to GitHub Introduction to Google Colab Introduction to Anaconda and PyCharm Introduction to Python Tools for Scientific Computing Introduction to Python Tools for Natural Language Processing Introduction to Python Tools for Machine Learning and Deep Learning Introduction to Computational Resources in the Lab
Machine learning steps: Business understanding Data understanding Data preparation Data modeling Evaluation Deployment Factors affect the selection


Chunk 136:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 73, 'num_chars': 512}
Text: Evaluation Deployment Factors affect the selection and performance of ML models: size, quality, and nature of the domain data available computational time structure of the desired predictions and loss to be minimized
8/16/24
90
Machine learning
Deep learning steps: Defining network architectures involves setting fine-grained details such as activation functions and the types of layers as well as the overall architecture of the network. Defining training routines involves setting the learning rate schedules,


Chunk 137:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 71, 'num_chars': 512}
Text: ines involves setting the learning rate schedules, the learning rules [e.g. stochastic gradient descent (SGD), Adam], the loss functions (e.g. MSE), regularization techniques (e.g. early stopping) and hyper-parameter optimization (e.g. random search, bayesian guided search). Factors affect the selection and performance of DL models: new algorithmic advances availability of huge amount of data to train NNs increase of computing power
8/16/24
91
Deep learning
8/16/24
92
Why Computational Resources Matters?
Be


Chunk 138:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 86, 'num_chars': 512}
Text: 8/16/24
92
Why Computational Resources Matters?
Before 2012: It was uncommon to use GPUs for ML, making any of the results in the graph difficult to achieve. 2012 to 2014: Infrastructure to train on many GPUs was uncommon, so most results used 1-8 GPUs rated at 1-2 TFLOPS for a total of 0.001-0.1 pfs-days. 2014 to 2016: Large-scale results used 10-100 GPUs rated at 5-10 TFLOPS, resulting in 0.1-10 pfs-days. Diminishing returns on data parallelism meant that larger training runs had limited value. 2016 to 20


Chunk 139:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: larger training runs had limited value. 2016 to 2017: Approaches that allow greater algorithmic parallelism such as huge batch sizes, architecture search, and expert iteration, along with specialized hardware such as TPU’s and faster interconnects, have greatly increased these limits, at least for some applications.
8/16/24
93
Why Computational Resources Matters?
How to calculate the petaflop/s-day Example of Method 1: Counting operations in the model Example of Method 2: GPU Time Famous DL models: Attentio


Chunk 140:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text: e of Method 2: GPU Time Famous DL models: Attention is all you need: 0.089 pfs-days (6/2017) Adam Optimizer: less than 0.0007 pfs-days (12/2014) Learning to Align and Translate: 0.018 pfs-days (9/2014) GANs: less than 0.006 pfs-days (6/2014) Word2Vec: less than 0.00045 pfs-days (10/2013) Variational Auto Encoders: less than 0.0000055 pfs-days (12/2013)
8/16/24
94
Why Computational Resources Matters?
8/16/24
95
CPU vs GPU in ML and DL
Many state-of-the-art Deep learning networks wouldn’t be possible if not f


Chunk 141:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 89, 'num_chars': 512}
Text: ep learning networks wouldn’t be possible if not for GPU!
CPU: The Central Processing Unit is the heart of a computer CPU is a microprocessor designed for latency optimization CPU consists of few powerful cores to perform multiple processes at a time CPU has low bandwidth; it can fetch data at a faster rate but cannot process more data at a time GPU: Graphics processing unit is built to overcome the performances lag of CPU GPU is designed for bandwidth optimization GPU processor will not wait for data loadi


Chunk 142:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 92, 'num_chars': 512}
Text: ization GPU processor will not wait for data loading and can reuse the data from another thread or for another thread GPU comprises thousands of weaker cores for computations
8/16/24
96
CPU vs GPU in ML and DL
Before the boom of Deep learning, Google had an extremely powerful system to do their processing, which they had specially built for training huge nets. This system was monstrous and was of $5 billion total cost, with multiple clusters of CPUs.
8/16/24
97
CPU vs GPU in ML and DL: Examples
(1) High ban


Chunk 143:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: 
97
CPU vs GPU in ML and DL: Examples
(1) High bandwidth main memory (2) Hiding memory access latency under thread parallelism (3) Large and fast register and L1 memory which is easily programmable are the components which make GPUs so well suited for deep learning.
8/16/24
98
Why is GPU useful for machine learning and deep learning? Quora Answer
GPUs for Machine Learning and Deep Learning
1 NVIDIA DGX STATION: https://www.nvidia.com/en-us/data-center/dgx-station/ AI WORKSTATION FOR DATA SCIENCE TEAMS: http


Chunk 144:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 32, 'num_chars': 512}
Text: ation/ AI WORKSTATION FOR DATA SCIENCE TEAMS: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-station/dgx-station-print-Infographic-738375-nvidia-web.pdf 2 NVIDIA H100L 94GB * 2 (256GB): https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/h100/PB-11773-001_v01.pdf 1 NVIDIA TITAN V: https://www.nvidia.com/en-us/titan/titan-v/ 2 NVIDIA Quadro P5000: https://www.pny.com/nvidia-quadro-p5000 Texas Advanced Computing Center (TACC): https://research.unt.edu/research-services/research-


Chunk 145:
Document ID: 8e29030a-2f66-4cf9-a07b-131b18386c33
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Computational Environment and Resources-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 15, 'num_chars': 150}
Text: tps://research.unt.edu/research-services/research-computing
8/16/24
99
Computational Resources in our Lab
Account
8/16/24
100
How to Access Resources?


Chunk 146:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: Lesson 1: Core Concepts in INFO 5731
Haihua Chen Assistant Professor, Data Science
8/21/24
1
Lesson 1-6
Core concepts in Python Natural language processing Web scarping Corpus Data Quality Data wrangling/ preprocessing/ cleaning Tokenization Normalization Stemming Lemmatization Stop Words Parts-of-speech (POS) Tagging Statistical Language Modeling Bag of Words n-grams Regular Expressions Zipf's Law Feature Extraction/ engineering tf-idf Word Embedding
8/21/24
2
8/21/24
3
Natural language processing (NLP) co


Chunk 147:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: 4
2
8/21/24
3
Natural language processing (NLP) concerns itself with the interaction between natural human languages and computing devices.
https://hpccsystems.com/resources/understanding-natural-language/
8/21/24
4
Web scraping is the process of using bots to extract content and data from a website
https://kinsta.com/knowledgebase/what-is-web-scraping/
In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts or c


Chunk 148:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text: s may be formed of a single language of texts or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.).
8/21/24
5
Data quality is a measure of a data set's condition based on factors such as accuracy, completeness, consistency, reliability, validity, and others.
8/21/24
6
Data wrangling is the process of transforming and structuring data from one raw form into a desi


Chunk 149:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 88, 'num_chars': 512}
Text: and structuring data from one raw form into a desired format with the intent of improving data quality and making it more consumable and useful for analytics or machine learning.
8/21/24
7
Tokenization is, generally, an early step in the NLP process, a step that splits longer strings of text into smaller pieces or tokens.
8/21/24
8
Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctu


Chunk 150:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on.
8/21/24
9
Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.
8/21/24
10
Lemmatization is related to stemming, differing in that lemmatization is able to capture a word’s canonical forms based on its lemma. The process resolves words to their dictionary form (known as lemma), which requires 


Chunk 151:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text:  dictionary form (known as lemma), which requires detailed dictionaries so that the algorithm can look into and link words to their corresponding lemmas.
8/21/24
11
Stop words are those words that are filtered out before further processing of text since these words contribute little to overall meaning, given that they are generally the most common words in a language.
An example of stop word list: https://gist.github.com/sebleier/554280
8/21/24
12
POS tagging consists of assigning a category tag to the toke


Chunk 152:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 84, 'num_chars': 512}
Text: g consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.
8/21/24
13
Statistical Language Modeling is the process of building a statistical language model that is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences.
8/21/24
14
Bag of w


Chunk 153:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text: of various possible sequences.
8/21/24
14
Bag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order but is interested in the number of occurrences of words within the text.
8/21/24
15
n-grams is another representation model for simplifying text selection contents. As opposed to the representation of bag-of-words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selectio


Chunk 154:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 82, 'num_chars': 512}
Text: iguous sequences of N items from the text selection.
8/21/24
16
Regular expressions are a tried-and-true method of concisely describing patterns of text. A regular expression is represented as a special text string itself and is meant for developing search patterns on selections of text.
8/21/24
17
Zipf's Law is used to describe the relationship between word frequencies in document collections. If a document collection's words are ordered by frequency, and y is used to describe the number of times that the 


Chunk 155:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text:  is used to describe the number of times that the xth word appears, Zipf's observation is concisely captured as y = cx-1/2 (item frequency is inversely proportional to item rank).
8/21/24
18
Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set. Feature engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning. Feature


Chunk 156:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 76, 'num_chars': 512}
Text: s that can be used in supervised learning. Feature selection involves identifying the most relevant features for the target variable and discarding the irrelevant ones
8/21/24
19
https://www.kaggle.com/discussions/questions-and-answers/388133
tf-idf is a weighting system that assigns a weight to each word in a document based on its term frequency (tf) and the reciprocal document frequency (tf) (idf). The words with higher scores of weight are deemed to be more significant.
8/21/24
20
Word embedding in NLP i


Chunk 157:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 79, 'num_chars': 512}
Text: re significant.
8/21/24
20
Word embedding in NLP is an important term that is used for representing words for text analysis in the form of real-valued vectors. It is an advancement in NLP that has improved the ability of computers to understand text-based content in a better way. It is considered one of the most significant breakthroughs of deep learning for solving challenging natural language processing problems. Different word embedding techniques
8/21/24
21
Lesson 7-11
Information Extraction Name Entity


Chunk 158:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 59, 'num_chars': 512}
Text: 
21
Lesson 7-11
Information Extraction Name Entity Recognition Similarity Measures Syntactic Analysis Semantic Analysis Sentiment Analysis Text Clustering Topic Modeling Text Classification Machine Translation Information Retrieval Text Summarization Large language models (LLMs) Generative AI
8/21/24
22
Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. Ty


Chunk 159:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: s and other electronically represented sources. Typically, this involves processing human language texts by means of NLP. Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction.
8/21/24
23
Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organiz


Chunk 160:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 78, 'num_chars': 512}
Text: e-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.
8/21/24
24
Levenshtein - the number of characters that must be deleted, inserted, or substituted in order to make a pair of strings equal Jaccard - the measure of overlap between 2 sets; in the case of NLP, generally, documents are sets of words Smith Waterman - similar to Levenshtein, but with costs assigned to substitution, insertion, and deletion
8/21/24
25



Chunk 161:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 74, 'num_chars': 512}
Text:  substitution, insertion, and deletion
8/21/24
25
8/21/24
26
Also referred to as parsing, syntactic analysis is the process of analyzing the strings of symbols in natural language in conformance with grammatical rules.
Levels of syntactic analysis: POS tagging Constituency parsing Dependency parsing
8/21/24
27
Semantic analysis is interested in determining the meaning of text selections (either character or word sequences). After an input selection of text is read and parsed (analyzed syntactically), the te


Chunk 162:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 81, 'num_chars': 512}
Text: s read and parsed (analyzed syntactically), the text selection can then be interpreted for meaning. Simply put, syntactic analysis is concerned with what words a text selection was made up of, while semantic analysis wants to know what the collection of words actually means.
8/21/24
28
Sentiment analysis is the process of evaluating and determining the sentiment captured in a selection of text, with sentiment defined as feeling or emotion. This sentiment can be simply positive (happy), negative (sad or angr


Chunk 163:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 80, 'num_chars': 512}
Text:  be simply positive (happy), negative (sad or angry), or neutral, or can be some more precise measurement along a scale, with neutral in the middle, and positive and negative increasing in either direction.
8/21/24
29
Text clustering is the process of grouping similar documents together based on their content. By clustering text, we can identify patterns and trends that would otherwise be difficult to discern.
8/21/24
30
Topic modeling is a type of statistical modeling for discovering the abstract “topics” 


Chunk 164:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 83, 'num_chars': 512}
Text: al modeling for discovering the abstract “topics” that occur in a collection of documents. Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.
8/21/24
31
Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in Natural Language Processing (N


Chunk 165:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text: undamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.
8/21/24
32
Machine translation is the task of automatically converting source text in one language to text in another language. In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language.
8/21/24
33
Information retrieval


Chunk 166:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text: another language.
8/21/24
33
Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).
https://nlp.stanford.edu/IR-book/pdf/01bool.pdf
8/21/24
34
Text Summarization is an NLP task that involves condensing a lengthy text document into a shorter, more compact version while still retaining the most important information and meaning. The goal is to produce a summary t


Chunk 167:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 69, 'num_chars': 512}
Text: on and meaning. The goal is to produce a summary that accurately represents the content of the original text in a concise form.
8/21/24
35
Large language models (LLMs) are deep learning algorithms that can recognize, summarize, translate, predict, and generate text and other forms of content based on knowledge gained from massive datasets.
https://www.youtube.com/watch?v=UCg8W236vfw&t=12s https://www.youtube.com/watch?v=0pHi6h1DyvQ
8/21/24
36
Generative AI refers to systems that can create different types o


Chunk 168:
Document ID: b33c47c3-a793-43bc-9818-9dc19db75899
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Core Concepts-Fall 2024- Updated on Aug 21-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 53, 'num_chars': 330}
Text: efers to systems that can create different types of content, including text, images, and code by extrapolating from patterns learned from being trained on vast swathes of data.
ChatGPT, Claude AI, Google Bard AI, Bing AI Chat, Perplexity AI, OpenAI GPT Playground, Poe by Quora, YouChat, Chatsonic, and others
Thank you
8/21/24
37


Chunk 169:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 89, 'num_chars': 512}
Text: Lesson 1: Orientation and Overview
Introduction to computation with python
Haihua Chen Assistant Professor, Data Science
1
2
How to begin?
3
4
What and why is Python?
Natural Language Processing?
About the course
1
What and Why is Python?
It is an open-source programming language Created by Guido van Rossum in 1989 First version released in 1991 Now 3.12.5 / 7 August 2024
1.1. What is Python?
Very easy to use
1.2. Why is Python?
Let you work quickly
Integrate systems more effectively
One of the most popular


Chunk 170:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 49, 'num_chars': 512}
Text: e systems more effectively
One of the most popular programming languages in the planet
The Top Programming Languages 2024
Source: https://www.geeksforgeeks.org/top-programming-languages/
Scientific Calculation
Artificial Intelligence
Data Analysis
Web Crawler
Web Development
Python
1.3. Why is Python?--Area
Web Crawler
Scrapy
Source: https://www.datasciencecentral.com/what-is-web-scraping-and-how-does-it-work/
Data Analysis
NumPy,Pandas,Matplotlib
Scientific Calculation
NumPy,SciPy,BioPython
Artificial Inte


Chunk 171:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text:  Calculation
NumPy,SciPy,BioPython
Artificial Intelligence
Scikit-learn,NLTK, Keras, TensorFlow, PyTorch
Web Development
Django,Flask, Tornado
Visualization
Matplotlib
Text Mining
NLP, topic modeling
Who is using Python?
https://www.python.org/about/success/
Coding is never so easy like this
No. 1 vs No. 2
2
Natural Language Processing (NLP)
What is Natural Language Processing?
Brief history of NLP
Fundamental Tasks in NLP
Some applications of NLP
What will be covered?
2.1. What is Natural Language Processi


Chunk 172:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 58, 'num_chars': 512}
Text: be covered?
2.1. What is Natural Language Processing?
To get computers to perform useful tasks involving human languages
Answers on Quora: https://www.quora.com/Why-should-I-learn-NLP
Why should I learn NLP?
2 days NLP Level 1 Basic Workshop - Participants Interview: https://youtu.be/UA17IVfArbU
Is it still worth learning NLP in the age of API-accessibles LLM like GPT?: https://www.reddit.com/r/learnmachinelearning/comments/10c509n/is_it_still_worth_learning_nlp_in_the_age_of/
Languages involve many human a


Chunk 173:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 82, 'num_chars': 512}
Text: _nlp_in_the_age_of/
Languages involve many human activities
Why is NLP interesting
NLP is used to acquire insights from massive amount of textual data
NLP can interact with different languages and different linguistic structures easily
Voice can be used as a user interface in many applications
NLP is hard!
Why is NLP hard?
Highly ambiguous
Natural languages involve reasoning about the world
I made her duck may have different meanings
It is unlikely that an elephant wears a pajamas (sentence: I shot an eleph


Chunk 174:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 70, 'num_chars': 512}
Text: lephant wears a pajamas (sentence: I shot an elephant in pajamas).
1940s – 1950s Automaton, Probabilistic/Information-Theoretic models
1957-1970s Chomsky and others on formal language theory and generative syntax
2.2. Brief history of NLP
1970-1983 Explosion in research in speech and language processing
1983-1993 Empiricism and Finite State Models Redux
1994-1999 Probabilistic and data-driven models
2000-2018 Machine Learning and deep learning
2019-now Large Language models
Part of speech tagging Word segme


Chunk 175:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 67, 'num_chars': 512}
Text:  Language models
Part of speech tagging Word segmentation Syntactic analysis Semantic analysis and sentiment analysis Named entity recognition Topic segmentation and recognition Language translation
2.3. Fundamental Tasks in NLP
IR & QA
Information Extraction
Machine Translation
2.4. Some applications of NLP
Text Summarization and Generation
3
How to begin?
Good resources for reference
Python 3.12.5 https://www.python.org/downloads/release/python-3125/
10 Best Python Libraries for NLP in 2024 and their Use 


Chunk 176:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 42, 'num_chars': 512}
Text: st Python Libraries for NLP in 2024 and their Use Cases https://www.unite.ai/10-best-python-libraries-for-natural-language-processing/
Top 20 Python Libraries You Must Know In 2024 https://medium.com/@codingwinner/top-20-python-libraries-you-must-know-in-2024-e075c8e50ecf
Allennlp https://github.com/allenai/allennlp
Hugging Face: https://huggingface.co
All can be done in clouds (Recommended)
Google Colab https://colab.research.google.com/notebooks/welcome.ipynb
GitHub https://github.com
PyCharm Version: PyC


Chunk 177:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 66, 'num_chars': 512}
Text: ynb
GitHub https://github.com
PyCharm Version: PyCharm 2024.2 https://www.jetbrains.com/pycharm/whatsnew/
4
About the course
Master key concepts and components of NLP and linguistics. Manipulate large corpora, explore linguistic models, and test empirical claims. Design and implement applications that process, manage, and analyze text data. Clean and preprocess raw text data using basic natural language processing techniques. Demonstrate the ability of extracting and analyzing information from text data usi


Chunk 178:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 68, 'num_chars': 512}
Text: cting and analyzing information from text data using Python Program. Build robust systems to perform linguistic tasks with technological application. Document and report on information processing and applications.
Goals and Learning Objectives
Downey, Allen B. (2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. O’Reilly, ISBN-13: 978-1-491-93936-9. (Required)
Materials—Textbook
Hapke, H., Howard, C., & Lane, H. (2021). Natural Language Processing in Action: Understanding, analyzing, a


Chunk 179:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 77, 'num_chars': 512}
Text:  Processing in Action: Understanding, analyzing, and generating text with Python (2nd Edition). Simon and Schuster. (Required)
Tunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language processing with transformers (Revised Edition). " O'Reilly Media, Inc.". (Required)
In-class exercise (20%) — 5 in-class exercises Assignments (40%) — 4 assignments Quizzes (10%) — 5 in-class quizzes Term project (30%) — Group work Extra credits (6%) — Reading and presentation; Extra credits (3%, 1% / each time) — Atte


Chunk 180:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 83, 'num_chars': 512}
Text: ntation; Extra credits (3%, 1% / each time) — Attending research presentations in the department Extra credits (1%) — Course evaluation Attendance is mandatory to pass. Students who miss more than 3 times will be dropped with W or WF. Cheating will not be tolerated. Using ChatGPT is allowed but remember that ChatGPT gives the same output for similar prompts provided by different users. Similarity in work would count towards plagiarism. Late work without the permission of the instructor will receive a grade 


Chunk 181:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 75, 'num_chars': 512}
Text: permission of the instructor will receive a grade with a 10% penalty (or 10 points out of 100) per day after the due date.
Grading
Ph.D., Assistant Professor, Data Science 10-year experience in python and data analytics Machine learning, natural language processing, information retrieval, and text mining Reviewer of more than 30 peer-review journals and 20 international conferences A good friend who is willing to connect with our master students
About me
Publications Lab: https://idealabunt.github.io/home/i


Chunk 182:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 51, 'num_chars': 512}
Text: lications Lab: https://idealabunt.github.io/home/index.html Google Scholar: https://scholar.google.com/citations?user=URmnWAQAAAAJ&hl=en&oi=ao Research Gate: https://www.researchgate.net/profile/Haihua-Chen-2 GitHub: https://github.com
About me
Email: haihua.chen@unt.edu Cell Phone: 9402688589 Office: DP E298A Office hour: By appointment Zoom ID: 247 728 2245
About me
Fengjiao Tu, PhD student in Information Science (Data Science Concentration), Department of Information Science, University of North Texas Yu


Chunk 183:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 60, 'num_chars': 464}
Text:  Information Science, University of North Texas Yuhan Zhou, PhD student in Information Science (Data Science Concentration), Department of Information Science, University of North Texas Office hours: DP E292J, Monday - Friday 12:00 PM – 2:00 PM Email address: fengjiaotu@my.unt.edu, yuhanzhou@my.unt.edu
Teaching Assistant
Feedbacks from Previous Students
Feedbacks from Previous Students
Feedbacks from Previous Students
Feedbacks from Previous Students
Thank you


Chunk 184:
Document ID: 4000bbc1-9fe6-42b0-bab3-9fa8fddb7c5e
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 1/Lesson 1-Orientation and Overview-Fall 2024-Updated on Aug 16-2024.pptx', 'folder_name': 'Module 1', 'num_tokens': 1, 'num_chars': 2}
Text: ou


Chunk 185:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: CS224C: NLP for CSS Topic Modeling Diyi Yang Stanford CS 1
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 2
Topic Modeling Organize the documents into a set of coherent topics Find relationships between these topics Understand how different documents talk about the same topic Track the evolution of topics over time 3
Topic Modeling A method of (unsupervised) discovery of latent or hidden 


Chunk 186:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: d of (unsupervised) discovery of latent or hidden structure in a corpus ✦ Applied primarily to text corpora ✦ Provides a modeling toolbox ✦ Has prompted the exploration of a variety of new inference methods to accommodate large-scale datasets 4
5
Latent Dirichlet Allocation Generative Process Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." Journal of machine Learning research 3, no. Jan (2003): 993-1022. 6
Latent Dirichlet Allocation 7
The generative story begins with onl


Chunk 187:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 87, 'num_chars': 512}
Text:  Allocation 7
The generative story begins with only a Dirichlet prior over the topics Each topic is defined as a Multinomial distribution over the vocabulary, parameterized by φ k Example Credit to Matthew R. Gormley 8
A topic is visualized as its high probability words. A pedagogical label is used to identify the topic. Example Credit to Matthew R. Gormley 9
A topic is visualized as its high probability words. A pedagogical label is used to identify the topic. Example Credit to Matthew R. Gormley 10
Exampl


Chunk 188:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 86, 'num_chars': 512}
Text: ic. Example Credit to Matthew R. Gormley 10
Example Credit to Matthew R. Gormley 11
Example Credit to Matthew R. Gormley 12
Example Credit to Matthew R. Gormley 13
Example Credit to Matthew R. Gormley 14
Example Credit to Matthew R. Gormley 15
Example Credit to Matthew R. Gormley 16
Distribution over words (topics) Distribution over topics (docs) 17
Overview What is topic modeling? LDA topic modeling Evaluation methods 18
Interpreting Topics Models What is the meaning of each topic? How to set the number of


Chunk 189:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 94, 'num_chars': 512}
Text: he meaning of each topic? How to set the number of topics? How to evaluate the resulting topics? 19
Evaluating Topic Modeling Manual Inspection / Human judgement Top ranked words Intrinsic Evaluation Coherence score Intruder test Extrinsic Evaluation Downstream application 20
Coherence Score Whether the words in a topic is coherent in terms of semantic similarity p(w , w ) i j UCI coherence measure log ∑ p(w )p(w ) i j i<j 1 + D(w , w ) i j UMass coherence measure log ∑ D(w ) i i<j Mimno, David, Hanna Walla


Chunk 190:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 75, 'num_chars': 512}
Text: easure log ∑ D(w ) i i<j Mimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. "Optimizing semantic coherence in topic models." In Proceedings of the 2011 conference on empirical methods in natural language processing, pp. 262-272. 2011. Newman, David, Jey Han Lau, Karl Grieser, and Timothy Baldwin. "Automatic evaluation of topic coherence." In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics, p


Chunk 191:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: f the association for computational linguistics, pp. 100-108. 2010. 21
Word Intrusion Task Given a few randomly ordered words, find the word which is out of place or does not belong with the others, i.e., the intruder Dog, cat, horse, apple, pig, cow Car, teacher, platypus, agile, blue, Zaire Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. "Reading tea leaves: How humans interpret topic models." Advances in neural information processing systems 22 (2009). 22
Topic Intrusion Te


Chunk 192:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: rocessing systems 22 (2009). 22
Topic Intrusion Tests whether a topic model’s decomposition of documents into a mixture of topics agrees with human judgements of the document’s content Given a title and a snippet from a document, judge which topic out of the four given topics does not belong with the document 23
Two Intrusion Tasks to Evaluate Topics 24
Toolkits & Interactive topic model visualization • Gensim • https://github.com/bmabey/pyLDAvis • Jupiter Notebook demo Řehůřek, Radim, and Petr Sojka. "S


Chunk 193:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: otebook demo Řehůřek, Radim, and Petr Sojka. "Software framework for topic modelling with large corpora." (2010). 25
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model 26
What if the input text is “noisy”? Removing non-latin characters Filtering out stop words e.g., “the”, “is” and “and” Converting words to lower case? Filtering out words with a frequency less than k Performing stemming ... 27
What if the input text is short? Dirichlet Mul


Chunk 194:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 84, 'num_chars': 512}
Text:  27
What if the input text is short? Dirichlet Multinomial Mixture model for short text clustering (GSDMM) The Movie Group Process Yin, Jianhua, and Jianyong Wang. "A dirichlet multinomial mixture model-based approach for short text clustering." In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 233-242. 2014 28
What if the input text is short? Dirichlet Multinomial Mixture model for short text clustering (GSDMM) K p(d) = p(d | z = k)p(z = k) ∑ k=1 p(d


Chunk 195:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 99, 'num_chars': 512}
Text: ng (GSDMM) K p(d) = p(d | z = k)p(z = k) ∑ k=1 p(d | z = k) = Π p(w | z = k) w∈d 29
What if the input text is short? Performance of the models on the TweetSet. https://github.com/rwalk/gsdmm-rust 30
What if there are user priors? “To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set” “To improve document-topic distributions, we encourage the model to select topics based on the existence of input seed words in that do


Chunk 196:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 78, 'num_chars': 512}
Text: ed on the existence of input seed words in that document” Jagarlamudi, Jagadeesh, Hal Daumé III, and Raghavendra Udupa. "Incorporating lexical priors into topic models." In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pp. 204-213. 2012. 31
What if there are user priors? (seededLDA) SeededLDA allows one to specify seed words that can influence the discovered topics Ramesh, Arti, Dan Goldwasser, Bert Huang, Hal Daumé III, and Lise Getoor. "Unde


Chunk 197:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 82, 'num_chars': 512}
Text: Bert Huang, Hal Daumé III, and Lise Getoor. "Understanding MOOC discussion forums using seeded LD3A2." In Proceedings of the ninth workshop on innovative use of NLP for building educational applications, pp. 28-33. 2014.
What if there are user priors? (seededLDA) 33
What if there are some topics are related? “ Topic proportions θ can be correlated, and the prevalence of these topics can be influenced by some set of covariates X through a standard regression model with covariates Roberts, Margaret E., Brand


Chunk 198:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 81, 'num_chars': 512}
Text:  model with covariates Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, and Edoardo M. Airoldi. "The structural topic model and applied social science." In Advances in neural information processing systems workshop on topic models: computation, application, and evaluation, vol. 4, no. 1, pp. 1-20. 2013. 34
The Structural Topic Model • Topics can be correlated • Each document has its own prior distribution over topics, defined by covariate X rather than sharing a global mean • Word use within a topi


Chunk 199:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 84, 'num_chars': 512}
Text: han sharing a global mean • Word use within a topic can vary by covariate U Provide a way of “structuring” the prior distributions in the topic model 35
The STM for Open-ended Questions in Survey Experiments Party ID, Treatment, and the Predicted Proportion in Fear Topic (1 of 3)
How News Wires Describe China’s Rise, 1997-2006 Taiwanese Presidential Election Topic (1 of 80) with news-source specific content (2 of 5)
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLD


Chunk 200:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 71, 'num_chars': 512}
Text:  modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 38
BERTopic in 3 steps 1. Each document is converted to its embedding representation using a pretrained language model 2. The dimensionality of these embeddings is reduced to optimize clustering 3. Topic representations are extracted using a class-based variation of TF-IDF Grootendorst, Maarten. "BERTopic: Neural topic modeling with a class-based TF-IDF procedure." arXiv preprint arX


Chunk 201:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 102, 'num_chars': 512}
Text:  class-based TF-IDF procedure." arXiv preprint arXiv:2203.05794 (2022). 39
Topic Representation N Classic TF-IDF W = tf ⋅ log( ) t,d t,d df t Custom Class TF-IDF: models the importance of words in clusters N W = tf ⋅ log(1 + ) t,c t,c tf t The average number of words per class A divided by the freq of term t across all classes 40
Topic Representation and Dynamic Topic Model N Classic TF-IDF W = tf ⋅ log( ) t,d t,d df t Custom Class TF-IDF: models the importance of words in clusters N W = tf ⋅ log(1 + ) t,c 


Chunk 202:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 90, 'num_chars': 512}
Text: nce of words in clusters N W = tf ⋅ log(1 + ) t,c t,c tf t Create the local representation of each Local representation of each topic: topic by multiplying the term frequency of N documents at timestamp t with the pre- W = tf ⋅ log(1 + ) t,c,i t,c,i calculated global IDF values tf t 41
BERTopic in 3 steps Topic diversity: the percentage of unique words for all topics Topic coherence: normalized pointwise mutual information Grootendorst, Maarten. "BERTopic: Neural topic modeling with a class-based TF-IDF pro


Chunk 203:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 63, 'num_chars': 512}
Text: eural topic modeling with a class-based TF-IDF procedure." arXiv preprint arXiv:2203.05794 (2022). 42
BertTopic https://huggingface.co/blog/bertopic 43
TopicGPT: A Prompt-based Topic Modeling Framework Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 44
TopicGPT: A Prompt-based Topic Modeling Framework 1) Topic Generation: Given a corpus and some manually-curated example topics, TopicGPT identifies add


Chunk 204:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 74, 'num_chars': 512}
Text: ly-curated example topics, TopicGPT identifies additional topics in each corpus document. 2) Topic Assignment: Given the generated topics, TopicGPT assigns the most relevant topic to each document and provides a quote that supports this assignment. Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 45
More Metrics for Topic Alignment Given a set of ground-truth classes and a set of predicted assignment c


Chunk 205:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 75, 'num_chars': 512}
Text: -truth classes and a set of predicted assignment clusters Purity: harmonic mean of purity and inverse purity to match each ground-truth category with the cluster that has the highest combined precision and recall. Adjusted Rand Index: pairwise agreement between two sets of clusters Normalized Mutual Information: the amount of shared information between two sets of clusters. Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXi


Chunk 206:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 69, 'num_chars': 512}
Text: sed topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 46
Topical alignment between ground-truth labels and predicted assignments TopicGPT achieves the best performance across all settings and metrics compared to LDA, BERTopic, and SeededLDA Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 47
Example topic assignments from TopicGPT and LDA Pham, Chau Minh, Alexander Hoyle, Simeng Sun, a


Chunk 207:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 66, 'num_chars': 512}
Text: DA Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 48
Concept Induction via LLooM (https://stanfordhci.github.io/lloom) Lam, Michelle S., Janice Teoh, James Landay, Jeffrey Heer, and Michael S. Bernstein. "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM." arXiv preprint arXiv:2404.12259 (2024). 49
50
Overview What is topic modeling? LDA topic modeling Evaluation met


Chunk 208:
Document ID: 70ea7f8d-c7f1-45ba-83b3-eb5cf4141f6b
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/s5_topic_modeling.pdf', 'folder_name': 'Module 10', 'num_tokens': 21, 'num_chars': 154}
Text:  topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 51


Chunk 209:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 64, 'num_chars': 512}
Text: Topic Modelling with Scikit-learn Derek Greene School of Computer Science University College Dublin
Overview • Scikit-learn • Introduction to topic modelling • Working with text data • Topic modelling algorithms • Non-negative Matrix Factorisation (NMF) • Topic modelling with NMF in Scikit-learn • Parameter selection for NMF • Practical issues Code, data, and slides: https://github.com/derekgreene/topic-model-tutorial 2
Scikit-learn pip install scikit-learn conda install scikit-learn http://scikit-learn.org


Chunk 210:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 75, 'num_chars': 512}
Text: conda install scikit-learn http://scikit-learn.org/stable 3
Introduction to Topic Modelling Topic modelling aims to automatically discover the hidden thematic structure in a large corpus of text documents. Topics Documents LeBron James says President Trump 'trying to divide Topic 1 through sport' Basketball LeBron Basketball star LeBron James has praised the American football players who NBA have protested against Donald Trump, and accused the US president of "using ... sports to try and divide us". Trump s


Chunk 211:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 84, 'num_chars': 512}
Text: f "using ... sports to try and divide us". Trump said that NFL players who fail to stand during the national anthem should Topic 2 be sacked or suspended. NFL Football James praised the players' unity, and said: "The people run this country." American ... James, who plays for the Cleveland Cavaliers and has won three NBA championships, campaigned for Hillary Clinton, Trump's rival, during the 2016 presidential election campaign. Topic 3 Trump President Clinton A document is composed of terms related to one 


Chunk 212:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 87, 'num_chars': 512}
Text: on A document is composed of terms related to one or more topics. ... 4
Introduction to Topic Modelling • Topic modelling is an unsupervised text mining approach. • Input: A corpus of unstructured text documents (e.g. news articles, tweets, speeches etc). No prior annotation or training set is typically required. Input Output Topic 1 Data Topic Topic 2 Pre- Modelling processing Algorithm Topic k • Output: A set of k topics, each of which is represented by: 1. A descriptor, based on the top-ranked terms for 


Chunk 213:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 87, 'num_chars': 512}
Text: . A descriptor, based on the top-ranked terms for the topic. 2. Associations for documents relative to the topic. 5
Introduction to Topic Modelling Top Terms for Topic 1 Top Terms for Topic 2 Top Terms for Topic 3 Top Terms for Topic 4 6
Introduction to Topic Modelling In the output of topic modelling, a single document can potentially be associated with multiple topics... Politics or Health? Business or Sport?
Application: News Media We can use topic modelling to uncover the dominant stories and subjects i


Chunk 214:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 86, 'num_chars': 512}
Text: ing to uncover the dominant stories and subjects in a corpus of news articles. Rank Term Article Headline Weight 1 eu Archbishop accuses Farage of racism and 'accentuating fear' 0.20 2 brexit Cameron names referendum date as Gove declares for Brexit 0.20 Topic 1 3 uk Cameron: EU referendum is a 'once in a generation' decision 0.18 4 britain Remain camp will win EU referendum by a 'substantial margin' 0.18 5 referendum EU referendum: Cameron claims leaving EU could make cutting... 0.18 Rank Term Document Tit


Chunk 215:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 81, 'num_chars': 512}
Text:  could make cutting... 0.18 Rank Term Document Title Weight 1 trump Donald Trump: money raised by Hillary Clinton is 'blood money' 0.27 2 clinton Second US presidential debate – as it happened 0.27 Topic 2 3 republican Donald Trump hits delegate count needed for Republican nomination 0.26 4 donald Trump campaign reportedly vetting Christie, Gingrich as potential... 0.26 5 campaign Trump: 'Had I been president, Capt Khan would be alive today' 0.26 88
Application: Social Media Topic modelling applied to 4,170


Chunk 216:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 102, 'num_chars': 512}
Text: ion: Social Media Topic modelling applied to 4,170,382 tweets from 1,200 prominent Twitter accounts, posted over 12 months. Topics can be identified based on either individual tweets, or at the user profile level. Topic 1 Topic 2 Topic 3 Rank Term Rank Term Rank Term 1 space 1 #health 1 apple 2 #yearinspace 2 cancer 2 iphone 3 pluto 3 study 3 #ios 4 earth 4 risk 4 ipad 5 nasa 5 patients 5 mac 6 mars 6 care 6 app 7 mission 7 diabetes 7 watch 8 launch 8 #zika 8 apps 9 #journeytomars 9 drug 9 os 10 science 10 


Chunk 217:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 88, 'num_chars': 512}
Text: 8 apps 9 #journeytomars 9 drug 9 os 10 science 10 disease 10 tv 9
Application: Political Speeches Analysis of 400k European Parliament speeches from 1999-2014 to uncover agenda and priorities of MEPs (Greene & Cross, 2017). 1200 1000 800 600 400 200 0 2000 2002 2004 2006 2008 2010 2012 2014 10 sehceepS fo rebmuN Financial crisis D Euro crisis A C B Year
Other Applications Topic models have also been applied to discover the underlying patterns across a range of different non-textual datasets. LEGO colour the


Chunk 218:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 71, 'num_chars': 512}
Text: of different non-textual datasets. LEGO colour themes as topic models https://nateaff.com/2017/09/11/lego-topic-models 11
Working with Text Data
Working with Text Data Most text data arrives in an unstructured form without any pre- defined organisation or format, beyond natural language. The vocabulary, formatting, and quality of the text can vary significantly. 13
Text Preprocessing • Documents are textual, not numeric. The first step in analysing unstructured documents is tokenisation: split raw text into


Chunk 219:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: red documents is tokenisation: split raw text into individual tokens, each corresponding to a single term. • For English we typically split a text document based on whitespace. Punctuation symbols are often used to split too: text = "Apple reveals new iPhone model" text.split() ['Apple', 'reveals', 'new', 'iPhone', 'model'] • Splitting by whitespace will not work for some languages: e.g. Chinese, Japanese, Korean; German compound nouns. • For some types of text content, certain characters can have a special


Chunk 220:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 85, 'num_chars': 512}
Text: ext content, certain characters can have a special significance: 14
Bag-of-Words Representation • How can we go from tokens to numeric features? • Bag-of-Words Model: Each document is represented by a vector in a m-dimensional coordinate space, where m is number of unique terms across all documents (the corpus vocabulary). Example: Document 1: When we tokenise our corpus of 3 Forecasts cut as IMF issues documents, we have a vocabulary of warning 14 distinct terms Document 2: vocab = set() IMF and WBG meet t


Chunk 221:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: terms Document 2: vocab = set() IMF and WBG meet to for doc in corpus: discuss economy tokens = tokenize(doc) for tok in tokens: vocab.add(tok) Document 3: print(vocab) WBG issues 2016 growth {'2016', 'Forecasts', 'IMF', 'WBG', 'and', warning 'as', 'cut', 'discuss', 'economy', 'growth', 'issues', 'meet', 'to', 'warning'} 15
Bag-of-Words Representation • Each document can be represented as a term vector, with an entry indicating the number of time a term appears in the document: Document 1: Forecasts cut as 


Chunk 222:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 126, 'num_chars': 512}
Text: ars in the document: Document 1: Forecasts cut as IMF issues warning 0 1 1 0 0 1 1 0 0 0 1 0 0 1 16 6102 stsaceroF FMI GBW dna sa tuc ssucsid ymonoce htworg seussi teem ot gninraw 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 2 0 0 1 0 0 0 0 0 1 1 0 0 1 6102 stsaceroF FMI GBW dna sa tuc ssucsid ymonoce htworg seussi teem ot gninraw • By transforming all documents in this way, and stacking them in rows, we create a full document-term matrix: Document 2: IMF and WBG meet to discuss economy Document 


Chunk 223:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 72, 'num_chars': 512}
Text: t 2: IMF and WBG meet to discuss economy Document 3: 2016: WBG issues 2016 growth warning 3 Documents x 14 Terms
Bag-of-Words in Scikit-learn • Scikit-learn includes functionality to easily transform a collection of strings containing documents into a document-term matrix. Our input, documents, is a list of strings. Each string is a separate document. from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer() A = vectorizer.fit_transform(documents) Our output, A, is a sparse 


Chunk 224:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 76, 'num_chars': 512}
Text: t_transform(documents) Our output, A, is a sparse NumPy 2D array with rows corresponding to documents and columns corresponding to terms. • Once the matrix has been created, we can access the list of all terms and an associated dictionary (vocabulary_) which maps each unique term to a corresponding column in the matrix. terms = vectorizer.get_feature_names_out() vocab = vectorizer.vocabulary_ len(terms) vocab["world"] 3288 3246 How many terms in the vocabulary? Which column corresponds to a term? 17
Further


Chunk 225:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 84, 'num_chars': 512}
Text: ry? Which column corresponds to a term? 17
Further Text Preprocessing • The number of terms used to represent documents is often reduced by applying a number of simple preprocessing techniques before building a document-term matrix: - Minimum term length: Exclude terms of length < 2 - Case conversion: Converting all terms to lowercase. - Stop-word filtering: Remove terms that appear on a pre-defined filter list of terms that are highly frequent and do not convey useful information (e.g. and, the, while) - M


Chunk 226:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: nvey useful information (e.g. and, the, while) - Minimum frequency filtering: Remove all terms that appear in very few documents. - Maximum frequency filtering: Remove all terms that appear in a very large number of documents. - Stemming: Process by which endings are removed from terms in order to remove things like tense or plurals: e.g. compute, computing, computer = comput 18
Further Text Preprocessing • Further preprocessing steps can be applied directly using the CountVectorizer class by passing approp


Chunk 227:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 58, 'num_chars': 512}
Text:  using the CountVectorizer class by passing appropriate parameters - e.g.: from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer( stop_words=custom_list, min_df=20, max_df=1000, lowercase=False, ngram_range=2) A = vectorizer.fit_transform(documents) Parameter Explanation stop_words=custom_list Pass in a custom list containing terms to filter. min_df=20 Filter those terms that appear in < 20 documents. max_df=1000 Filter those terms that appear in > 1000 documents. lowercas


Chunk 228:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 86, 'num_chars': 512}
Text: se terms that appear in > 1000 documents. lowercase=False Do not convert text to lowercase. Default is True. ngram_range=2 Include phrases of length 2, instead of just single words. 19
Term Weighting • As well as including or excluding terms, we can improve the usefulness of the document-term matrix by giving higher weights to more "important" terms. • TF-IDF: Common approach for weighting the score for a term in a document. Consists of two parts: - Term Frequency (TF): Number of times a given term appears 


Chunk 229:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 100, 'num_chars': 512}
Text: quency (TF): Number of times a given term appears in a single document. - Inverse Document Frequency (IDF): Function of total number of distinct documents containing a term. Effect is to penalise common terms that appear in almost every document. w(t, D) = tf (t, d) (log( n ) + 1) n = total number df(t) ⇥ of documents • Example: the term "cat" appears in a given document 3 times and appears 50 times overall in a corpus of 1000 documents: 1000 w(cat, D) = 3 (log( ) + 1) = 11.987 50 ⇥ 20
Term Weighting in Sci


Chunk 230:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 72, 'num_chars': 512}
Text: log( ) + 1) = 11.987 50 ⇥ 20
Term Weighting in Scikit-learn • A similar vectorisation approach can be used in Scikit-learn to produce a TF-IDF normalised document-term matrix: from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() A = vectorizer.fit_transform(documents) The output, A, is a sparse NumPy array where the entries are all TF-IDF normalised. • Again we can perform additional preprocessing steps by passing the appropriate parameter values to TfidfVectorizer: fro


Chunk 231:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 55, 'num_chars': 512}
Text: propriate parameter values to TfidfVectorizer: from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer( stop_words=custom_list, min_df=20, max_df=1000, lowercase=False, ngram_range=2) A = vectorizer.fit_transform(documents) 21
Text Preprocessing Pipeline • Typical text preprocessing steps for a document corpus... Case Filter Short Filter Stop- Tokenisation Conversion Terms Words Corpus of Raw Documents Min/Max Term Stemming Filtering Term Vectorisation Weighting Document Ter


Chunk 232:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: iltering Term Vectorisation Weighting Document Term Matrix • Note: Stemming is not included in scikit-learn. See NLTK package. • Once we have our document-term matrix, we are ready to apply machine learning algorithms to explore the data. 22
Topic Modelling
Topic Modelling Algorithms Various different methods for topic modelling have been proposed. Two general approaches are popular: 1. Probabilistic approaches - View each document as a mixture of a small number of topics. - Words and documents get probabil


Chunk 233:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 80, 'num_chars': 512}
Text: mber of topics. - Words and documents get probability scores for each topic. - e.g. Latent Dirichlet Allocation (LDA) (Blei et al, 2003). 2. Matrix factorisation approaches - Apply methods from linear algebra to decompose a single matrix (e.g. document-term matrix) into a set of smaller matrices. - For text data, we can interpret these as a topic model. - e.g. Non-negative Matrix Factorisation (NMF) (Lee & Seung, 1999). 24
Non-negative Matrix Factorisation • Non-negative Matrix Factorisation (NMF): Family o


Chunk 234:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 87, 'num_chars': 512}
Text:  Non-negative Matrix Factorisation (NMF): Family of linear algebra algorithms for identifying the latent structure in data represented as a non-negative matrix (Lee & Seung, 1999). • NMF can be applied for topic modeling, where the input is a document-term matrix, typically TF-IDF normalised. • Input: Document-term matrix A; Number of topics k. • Output: Two k-dimensional factors W and H approximating A. m k m · n A n W k H NMF Factor H (topics x terms) Input Matrix Factor W (documents x terms) (documents x


Chunk 235:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 86, 'num_chars': 512}
Text: t Matrix Factor W (documents x terms) (documents x topics) 25
Example: NMF Topic Modelling Apply NMF topic modelling to a small document-term matrix A representing a corpus of 6 documents, to generate k=3 topics... document 1 document 2 document 3 document 4 document 5 document 6 26 hcraeser loohcs noitacude esaesid tneitap htlaeh tegdub ecnanif gniknab sdnob 6 Documents x 10 Terms
Example: NMF Topic Modelling Factor W Factor H Weights for 6 documents Weights for 10 terms relative to 3 topics relative to 3 


Chunk 236:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 109, 'num_chars': 512}
Text: s for 10 terms relative to 3 topics relative to 3 topics Topic 1 Topic 2 Topic 3 Topic 1 Topic 2 Topic 3 research 0.0 0.0 1.0 document 1 0.0 1.0 1.0 school 0.0 0.1 0.1 document 2 0.0 0.0 1.0 education 0.0 0.0 1.0 document 3 0.7 0.0 0.0 disease 0.0 0.6 0.0 patient 0.0 0.7 0.0 document 4 0.7 0.0 0.0 health 0.0 1.0 0.0 document 5 0.0 0.0 1.0 budget 0.3 0.1 0.2 finance 0.6 0.0 0.0 document 6 0.0 1.0 1.0 0.7 0.0 0.0 banking 6 Rows x 3 Columns 0.3 0.0 0.0 bonds 10 Rows x 3 Columns 27
Applying NMF in Scikit-learn 


Chunk 237:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 82, 'num_chars': 512}
Text:  Rows x 3 Columns 27
Applying NMF in Scikit-learn • Scikit-learn includes a fast implementation of NMF. • By default, the values in factors W and H are given random initial values. The key required input parameter is the number of topics (components) k: from sklearn import decomposition Apply NMF to document-term model = decomposition.NMF(n_components=k) matrix A, extract the resulting W = model.fit_transform( A ) factors W and H H = model.components_ • When using random initialisation, the results can be d


Chunk 238:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 81, 'num_chars': 512}
Text:  using random initialisation, the results can be different every time NMF is applied to the same data. More reliable results can be obtained if you initialise with SVD (Belford et al, 2018). from sklearn import decomposition model = decomposition.NMF(n_components=k, init="nndsvd") W = model.fit_transform( A ) H = model.components_ 28
Applying NMF in Scikit-learn • The H factor contains term weights relative to each of the k topics. Each row corresponds to a topic, and each column corresponds to a unique ter


Chunk 239:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 85, 'num_chars': 512}
Text: topic, and each column corresponds to a unique term in the corpus vocabulary. • Sorting the values in each row gives us a ranking of terms - the descriptor of each topic. import numpy as np For each topic, sort the row top_indices = np.argsort( H[topic_index,:] )[::-1] indices in reverse, then get top_terms = [] for term_index in top_indices[0:top]: the terms for the top indices. top_terms.append( terms[term_index] ) Repeat for all topics to get the full set of descriptors: Topic 01: eu, brexit, uk, britain


Chunk 240:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 69, 'num_chars': 512}
Text:  of descriptors: Topic 01: eu, brexit, uk, britain, referendum, leave, vote, european, cameron, labour Topic 02: trump, clinton, republican, donald, campaign, president, hillary, cruz, sanders, election Topic 03: film, films, movie, star, hollywood, director, actor, story, drama, women Topic 04: league, season, leicester, goal, premier, united, city, liverpool, game, ball Topic 05: bank, banks, banking, financial, rbs, customers, shares, deutsche, barclays, lloyds Topic 06: health, nhs, care, patients, ment


Chunk 241:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: lloyds Topic 06: health, nhs, care, patients, mental, doctors, hospital, people, services, junior Topic 07: album, music, band, song, pop, songs, rock, love, sound, bowie Topic 08: internet, facebook, online, people, twitter, media, users, google, company, amazon
Applying NMF in Scikit-learn • The W factor contains document membership weights across the k topics. Each row corresponds to a different document, and each column corresponds to a topic. • Sorting the values gives us a ranking of the most relevant


Chunk 242:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 78, 'num_chars': 512}
Text: the values gives us a ranking of the most relevant documents for each topic. For each topic, sort column top_indices = np.argsort( W[:,topic_index] )[::-1] indices in reverse, then get top_documents = [] the documents for the top for doc_index in top_indices[0:top]: top_documents.append( documents[doc_index] ) indices. The top documents for a topic might be summarised using titles or snippets: 01. Donald Trump: money raised by Hillary Clinton is 'blood money' 02. Second US presidential debate – as it happen


Chunk 243:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 83, 'num_chars': 512}
Text: ' 02. Second US presidential debate – as it happened 03. Trump campaign reportedly vetting Christie, Gingrich as potential running mates 04. Donald Trump hits delegate count needed for Republican nomination 05. Trump: 'Had I been president, Capt Khan would be alive today' 06. Clinton seizes on Trump tweets for day of campaigning in Florida 07. Melania Trump defends husband's 'boy talk' in CNN interview 08. Hillary Clinton: 'I'm sick of the Sanders campaign's lies' 09. Donald Trump at the White House: Obama 


Chunk 244:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 73, 'num_chars': 512}
Text:  lies' 09. Donald Trump at the White House: Obama reports 'excellent conversation' 10. Donald Trump: Hillary Clinton has 'no right to be running' 30
Applying NMF in Scikit-learn Topic 01: eu, brexit, uk, britain, referendum, leave, vote, european, cameron, labour Topic 02: trump, clinton, republican, donald, campaign, president, hillary, cruz, sanders, election Topic 03: film, films, movie, star, hollywood, director, actor, story, drama, women Topic 04: league, season, leicester, goal, premier, united, city


Chunk 245:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 76, 'num_chars': 512}
Text: ue, season, leicester, goal, premier, united, city, liverpool, game, ball Topic 05: bank, banks, banking, financial, rbs, customers, shares, deutsche, barclays, lloyds Topic 06: health, nhs, care, patients, mental, doctors, hospital, people, services, junior Topic 07: album, music, band, song, pop, songs, rock, love, sound, bowie Topic 08: internet, facebook, online, people, twitter, media, users, google, company, amazon 01. The lost albums loved by the stars – from ecstatic gospel to Italian prog 02. How t


Chunk 246:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 100, 'num_chars': 512}
Text: s – from ecstatic gospel to Italian prog 02. How to write a banger for Beyoncé 03. Albums of the year 2016 – our readers respond 04. Why Nirvana's In Bloom is busting out all over 05. Dead Kennedys – 10 of the best 06. Mogwai – 10 of the best 07. Marillion – 10 of the best 08. 'In the Faroe Islands, everyone is in a band' 09. Pop, rock, rap, whatever: who killed the music genre? 10. Iggy Pop – 10 of the best 31
Parameter Selection • The key parameter selection decision for topic modelling involves choosing


Chunk 247:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 83, 'num_chars': 512}
Text: ion decision for topic modelling involves choosing the number of topics k. • Common approach: Measure and compare the topic coherence of models generated for different values of k. • Topic coherence: The extent to which the top terms representing a topic (i.e. the topic descriptor) are semantically related, relative to some "background corpus". • A variety of different measures exist for measuring coherence e.g. NPMI, UMass, TC-W2V etc. (O'Callaghan et al, 2015). Rank Word Rank Word Rank Word 1 port 1 agric


Chunk 248:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 91, 'num_chars': 512}
Text: 015). Rank Word Rank Word Rank Word 1 port 1 agriculture 1 farmer 2 sea 2 farmer 2 naval 3 maritime 3 beef 3 dairy 4 naval 4 food 4 maritime 5 vessel 5 dairy 5 nuclear "High coherence topic" "High coherence topic" "Low coherence topic" 32
Parameter Selection • Typical approach for parameter selection: 1. Apply NMF for a "sensible" range k=[kmin,kmax]. 2. Calculate mean coherence of the topics produced for each k, relative to the overall corpus or a related background corpus. 3. Select the value of k giving 


Chunk 249:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 82, 'num_chars': 512}
Text: ackground corpus. 3. Select the value of k giving the highest mean coherence. Number of Topics 33 ecnerehoC naeM
Practical Issues • Preprocessing • Stop-word filtering often has a major impact. • TF-IDF often leads to more useful topics than raw frequencies. • Initialisation • Random initialisation of both NMF and LDA can lead to unstable results, particularly for larger datasets. • Scalability • NMF typically more scalable than LDA, but running times can increase considerably as number of topics k increase


Chunk 250:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 72, 'num_chars': 512}
Text: crease considerably as number of topics k increases. • Parameter Selection • In many cases, there can be several "good" values of k. • Choice of coherence measure can produce different results. • Interpretation • Topic models reflect the structure of the data available. Best used carefully as an exploratory tool to aid human interpretation. 34
Any Questions? derek.greene@ucd.ie https://github.com/derekgreene/topic-model-tutorial
References • Pedregosa, F., et al. Scikit-learn: Machine learning in Python. Jo


Chunk 251:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 83, 'num_chars': 512}
Text: t al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12. Oct (2011): 2825-2830. • Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. • Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. • Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by Non-negative Matrix Factorization. Nature, 401(6755), 788. • Belford, M., Mac Namee, B., & Greene, D. St


Chunk 252:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 79, 'num_chars': 512}
Text: 788. • Belford, M., Mac Namee, B., & Greene, D. Stability of Topic Modeling via Matrix Factorization. Expert Systems with Applications, 2018. • O’Callaghan, D., Greene, D., Carthy, J., & Cunningham, P. (2015). An analysis of the coherence of descriptors in topic modeling. Expert Systems with Applications, 42(13), 5645-5657. • Greene, D., & Cross, J. P. (2017). Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach. Political Analysis, 25(1), 77-94. • Rehurek, R., &


Chunk 253:
Document ID: 947f2afc-8a2f-4c93-8518-df4e4a7dcb00
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 10/topic-modelling-with-scikitlearn.pdf', 'folder_name': 'Module 10', 'num_tokens': 33, 'num_chars': 209}
Text: Political Analysis, 25(1), 77-94. • Rehurek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. 36


Chunk 254:
Document ID: 8196f225-0231-46b6-ab6b-a7b1cb0f8c79
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 48, 'num_chars': 512}
Text: 


# Load and prepare the dataset
import nltk
from nltk.corpus import movie_reviews
import random

nltk.download('movie_reviews')

documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)


# In[ ]:


# Define the feature extractor

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(do


Chunk 255:
Document ID: 8196f225-0231-46b6-ab6b-a7b1cb0f8c79
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 46, 'num_chars': 464}
Text: = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features


# In[ ]:


# Train Naive Bayes classifier
featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)


# 


Chunk 256:
Document ID: 8196f225-0231-46b6-ab6b-a7b1cb0f8c79
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 1, 'num_chars': 2}
Text: # 


Chunk 257:
Document ID: d54b687b-5473-4e78-9624-a67b7aa73282
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 7, 'num_chars': 81}
Text: 


# Test the classifier
print(nltk.classify.accuracy(classifier, test_set))


# 


Chunk 258:
Document ID: 24bffb0a-07ac-4af1-aeeb-f4768bb89162
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 21, 'num_chars': 165}
Text: 


# Show the most important features as interpreted by Naive Bayes
classifier.show_most_informative_features(5)


# # **2. Sentiment Analysis for Twitter Data**

# 


Chunk 259:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 50, 'num_chars': 512}
Text: 


import re 
import tweepy 
from tweepy import OAuthHandler 
from textblob import TextBlob 

class TwitterClient(object): 
	''' 
	Generic Twitter Class for sentiment analysis. 
	'''
	def __init__(self): 
		''' 
		Class constructor or initialization method. 
		'''
		# keys and tokens from the Twitter Dev Console 
		consumer_key = 'u7L1lnR7HN85dn1qnTFO1cegb'
		consumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'
		access_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO'
		access_


Chunk 260:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 42, 'num_chars': 512}
Text: 36620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO'
		access_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce'

		# attempt authentication 
		try: 
			# create OAuthHandler object 
			self.auth = OAuthHandler(consumer_key, consumer_secret) 
			# set access token and secret 
			self.auth.set_access_token(access_token, access_token_secret) 
			# create tweepy API object to fetch tweets 
			self.api = tweepy.API(self.auth) 
		except: 
			print("Error: Authentication Failed") 

	def clean_tweet(self, tweet): 


Chunk 261:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 57, 'num_chars': 512}
Text: ication Failed") 

	def clean_tweet(self, tweet): 
		''' 
		Utility function to clean tweet text by removing links, special characters 
		using simple regex statements. 
		'''
		return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split()) 

	def get_tweet_sentiment(self, tweet): 
		''' 
		Utility function to classify sentiment of passed tweet 
		using textblob's sentiment method 
		'''
		# create TextBlob object of passed tweet text 
		analysis = TextBlob(self.clean_tweet(t


Chunk 262:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 67, 'num_chars': 512}
Text: eet text 
		analysis = TextBlob(self.clean_tweet(tweet)) 
		# set sentiment 
		if analysis.sentiment.polarity > 0: 
			return 'positive'
		elif analysis.sentiment.polarity == 0: 
			return 'neutral'
		else: 
			return 'negative'

	def get_tweets(self, query, count = 10): 
		''' 
		Main function to fetch tweets and parse them. 
		'''
		# empty list to store parsed tweets 
		tweets = [] 

		try: 
			# call twitter api to fetch tweets 
			fetched_tweets = self.api.search(q = query, count = count) 

			# parsin


Chunk 263:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 69, 'num_chars': 512}
Text: api.search(q = query, count = count) 

			# parsing tweets one by one 
			for tweet in fetched_tweets: 
				# empty dictionary to store required params of a tweet 
				parsed_tweet = {} 

				# saving text of tweet 
				parsed_tweet['text'] = tweet.text 
				# saving sentiment of tweet 
				parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) 

				# appending parsed tweet to tweets list 
				if tweet.retweet_count > 0: 
					# if tweet has retweets, ensure that it is appended only once 
					if


Chunk 264:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 68, 'num_chars': 512}
Text: ets, ensure that it is appended only once 
					if parsed_tweet not in tweets: 
						tweets.append(parsed_tweet) 
				else: 
					tweets.append(parsed_tweet) 

			# return parsed tweets 
			return tweets 

		except tweepy.TweepError as e: 
			# print error (if any) 
			print("Error : " + str(e)) 

def main(): 
	# creating object of TwitterClient Class 
	api = TwitterClient() 
	# calling function to get tweets 
	tweets = api.get_tweets(query = 'Donald Trump', count = 200) 

	# picking positive tweets from t


Chunk 265:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 63, 'num_chars': 512}
Text: , count = 200) 

	# picking positive tweets from tweets 
	ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] 
	# percentage of positive tweets 
	print("Positive tweets percentage: {} %".format(100*len(ptweets)/len(tweets))) 
	# picking negative tweets from tweets 
	ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] 
	# percentage of negative tweets 
	print("Negative tweets percentage: {} %".format(100*len(ntweets)/len(tweets))) 
	# percentage of neutral tweets 



Chunk 266:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 51, 'num_chars': 475}
Text: )/len(tweets))) 
	# percentage of neutral tweets 
	print("Neutral tweets percentage: {} %".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets))) 

	# printing first 5 positive tweets 
	print("\n\nPositive tweets:") 
	for tweet in ptweets[:10]: 
		print(tweet['text']) 

	# printing first 5 negative tweets 
	print("\n\nNegative tweets:") 
	for tweet in ntweets[:10]: 
		print(tweet['text']) 

if __name__ == "__main__": 
	# calling main function 
	main() 


# 


Chunk 267:
Document ID: e0686a59-bc57-40d5-bd36-12778df5bac5
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 2, 'num_chars': 13}
Text: 	main() 


# 


Chunk 268:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 61, 'num_chars': 512}
Text: 


get_ipython().system('pip install vaderSentiment')


# # **3. Sentiment Analysis for Amazon Review**

# In[ ]:


# importing all the required Libraries
import glob
import json
import csv
import pandas as pd
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer



Chunk 269:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 51, 'num_chars': 512}
Text: rom textblob.sentiments import NaiveBayesAnalyzer
import string
import matplotlib.pyplot as plt
from nltk.stem import PorterStemmer
import warnings
warnings.filterwarnings("ignore")


# In[ ]:


# Data download link:
# https://drive.google.com/drive/folders/0B4Hj2axlpCcxWldiajctWmY0NG8
file=glob.glob('/Data/Tested_Data/ReviewSample.json')


# In[ ]:


# Reading a multiple json files from a single json file 'ReviewSample.json'.
review=[]
with open(file[0]) as data_file:
    data=data_file.read()
    for i in


Chunk 270:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 34, 'num_chars': 512}
Text:  data_file:
    data=data_file.read()
    for i in data.split('\n'):
        review.append(i)

# Making a list of Tuples containg all the data of json files.
reviewDataframe=[]
for x in review:
    try:
        jdata=json.loads(x)
        reviewDataframe.append((jdata['reviewerID'],jdata['asin'],jdata['reviewerName'],jdata['helpful'][0],jdata['helpful'][1],jdata['reviewText'],jdata['overall'],jdata['summary'],jdata['unixReviewTime'],jdata['reviewTime'])) 
    except:
        pass        

# Creating a dataf


Chunk 271:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 36, 'num_chars': 512}
Text:   except:
        pass        

# Creating a dataframe using the list of Tuples got in the previous step.    
dataset=pd.DataFrame(reviewDataframe,columns=['Reviewer_ID','Asin','Reviewer_Name','helpful_UpVote','Total_Votes','Review_Text','Rating','Summary','Unix_Review_Time','Review_Time'])


# In[ ]:


# Function to calculate sentiments using Naive Bayes Analyzer

def NaiveBaiyes_Sentimental(sentence):
    blob = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())
    NaiveBayes_SentimentScore=blob.sentiment


Chunk 272:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 52, 'num_chars': 512}
Text: er())
    NaiveBayes_SentimentScore=blob.sentiment.classification
    return NaiveBayes_SentimentScore


# In[ ]:


# Function to calculate sentiments using Vader Sentiment Analyzer

# VADER sentiment analysis tool for getting Compound score.
def sentimental(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    return score

# VADER sentiment analysis tool for getting pos, neg and neu.
def sentimental_Score(sentence):
    analyzer = S


Chunk 273:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 49, 'num_chars': 512}
Text: 
def sentimental_Score(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    if score >= 0.5:
        return 'pos'
    elif (score > -0.5) and (score < 0.5):
        return 'neu'
    elif score <= -0.5:
        return 'neg'


# In[ ]:


# sentiment calculation by our data as input
Selected_Rows=dataset.head(100000)
Selected_Rows['Sentiment_Score']=Selected_Rows['Review_Text'].apply(lambda x: sentimental_Score(x))
pos = Selected_Rows.l


Chunk 274:
Document ID: b0bb9bf5-812f-4f81-b292-57bafd839edc
Metadata: {'file_name': 'Lesson10_code.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Lesson10_code.ipynb', 'folder_name': 'Module 12', 'num_tokens': 15, 'num_chars': 186}
Text: bda x: sentimental_Score(x))
pos = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'pos']
print(pos)
neg = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'neg']
print(neg)




Chunk 275:
Document ID: 7c2dfb3c-982e-4031-a8df-12884e59b619
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 29, 'num_chars': 339}
Text: 


# Load and prepare the dataset
import nltk
from nltk.corpus import movie_reviews
import random

nltk.download('movie_reviews')

documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)


# 


Chunk 276:
Document ID: 327c7acc-61a2-4ced-ab8b-df397ae187e0
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 35, 'num_chars': 357}
Text: 


# Define the feature extractor

all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())
word_features = list(all_words)[:2000]

def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features


# 


Chunk 277:
Document ID: 2032c4a7-9987-46b3-abb6-6901ac8672c1
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 22, 'num_chars': 218}
Text: 


# Train Naive Bayes classifier
featuresets = [(document_features(d), c) for (d,c) in documents]
train_set, test_set = featuresets[100:], featuresets[:100]
classifier = nltk.NaiveBayesClassifier.train(train_set)


# 


Chunk 278:
Document ID: 33b9a40e-47ea-4f5d-9ec2-1a5a6facca6f
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 7, 'num_chars': 81}
Text: 


# Test the classifier
print(nltk.classify.accuracy(classifier, test_set))


# 


Chunk 279:
Document ID: 90237268-38b7-46f9-8d84-30a85ef75197
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 21, 'num_chars': 165}
Text: 


# Show the most important features as interpreted by Naive Bayes
classifier.show_most_informative_features(5)


# # **2. Sentiment Analysis for Twitter Data**

# 


Chunk 280:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 49, 'num_chars': 512}
Text: 


import re 
import tweepy 
from tweepy import OAuthHandler 
from textblob import TextBlob 

class TwitterClient(object): 
    ''' 
    Generic Twitter Class for sentiment analysis. 
    '''
    def __init__(self): 
        ''' 
        Class constructor or initialization method. 
        '''
        # keys and tokens from the Twitter Dev Console 
        consumer_key = 'u7L1lnR7HN85dn1qnTFO1cegb'
        consumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'
        access_token = '1144377


Chunk 281:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 35, 'num_chars': 512}
Text: UPnMCWV5M66SFc8wT'
        access_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO'
        access_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce'

        # attempt authentication 
        try: 
            # create OAuthHandler object 
            self.auth = OAuthHandler(consumer_key, consumer_secret) 
            # set access token and secret 
            self.auth.set_access_token(access_token, access_token_secret) 
            # create tweepy API object to fetch tweets 
     


Chunk 282:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 46, 'num_chars': 512}
Text:  # create tweepy API object to fetch tweets 
            self.api = tweepy.API(self.auth) 
        except: 
            print("Error: Authentication Failed") 

    def clean_tweet(self, tweet): 
        ''' 
        Utility function to clean tweet text by removing links, special characters 
        using simple regex statements. 
        '''
        return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split()) 

    def get_tweet_sentiment(self, tweet): 
        ''' 
       


Chunk 283:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 47, 'num_chars': 512}
Text: weet_sentiment(self, tweet): 
        ''' 
        Utility function to classify sentiment of passed tweet 
        using textblob's sentiment method 
        '''
        # create TextBlob object of passed tweet text 
        analysis = TextBlob(self.clean_tweet(tweet)) 
        # set sentiment 
        if analysis.sentiment.polarity > 0: 
            return 'positive'
        elif analysis.sentiment.polarity == 0: 
            return 'neutral'
        else: 
            return 'negative'

    def get_tweets


Chunk 284:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 64, 'num_chars': 512}
Text: 
            return 'negative'

    def get_tweets(self, query, count = 10): 
        ''' 
        Main function to fetch tweets and parse them. 
        '''
        # empty list to store parsed tweets 
        tweets = [] 

        try: 
            # call twitter api to fetch tweets 
            fetched_tweets = self.api.search(q = query, count = count) 

            # parsing tweets one by one 
            for tweet in fetched_tweets: 
                # empty dictionary to store required params of a twee


Chunk 285:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 51, 'num_chars': 512}
Text: mpty dictionary to store required params of a tweet 
                parsed_tweet = {} 

                # saving text of tweet 
                parsed_tweet['text'] = tweet.text 
                # saving sentiment of tweet 
                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) 

                # appending parsed tweet to tweets list 
                if tweet.retweet_count > 0: 
                    # if tweet has retweets, ensure that it is appended only once 
                   


Chunk 286:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 51, 'num_chars': 512}
Text: that it is appended only once 
                    if parsed_tweet not in tweets: 
                        tweets.append(parsed_tweet) 
                else: 
                    tweets.append(parsed_tweet) 

            # return parsed tweets 
            return tweets 

        except AttributeError as e: 
            # print error (if any) 
            print("Error : " + str(e)) 

def main(): 
    # creating object of TwitterClient Class 
    api = TwitterClient() 
    # calling function to get tweets 
 


Chunk 287:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 63, 'num_chars': 512}
Text: rClient() 
    # calling function to get tweets 
    tweets = api.get_tweets(query = 'Donald Trump', count = 200) 
    #print(tweets)
    # picking positive tweets from tweets 
    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] 
    # percentage of positive tweets 
    print("Positive tweets percentage: {} %".format(100*len(ptweets)/len(tweets))) 
    # picking negative tweets from tweets 
    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] 
    # percent


Chunk 288:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 49, 'num_chars': 512}
Text: f tweet['sentiment'] == 'negative'] 
    # percentage of negative tweets 
    print("Negative tweets percentage: {} %".format(100*len(ntweets)/len(tweets))) 
    # percentage of neutral tweets 
    print("Neutral tweets percentage: {} %".format(100*(len(tweets) - len(ntweets) - len(ptweets))/len(tweets))) 

    # printing first 5 positive tweets 
    print("\n\nPositive tweets:") 
    for tweet in ptweets[:10]: 
        print(tweet['text']) 

    # printing first 5 negative tweets 
    print("\n\nNegative t


Chunk 289:
Document ID: 69c07aed-9673-495b-98c7-34fc6426ba5c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 21, 'num_chars': 196}
Text: first 5 negative tweets 
    print("\n\nNegative tweets:") 
    for tweet in ntweets[:10]: 
        print(tweet['text']) 

if __name__ == "__main__": 
    # calling main function 
    main() 


# 


Chunk 290:
Document ID: 041d5b0a-0096-4976-97f0-5f71c43d6cd3
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 4, 'num_chars': 58}
Text: 


get_ipython().system('pip install vaderSentiment')


# 


Chunk 291:
Document ID: d8ceaaae-5cf5-401b-ac74-a3fd0ff3795a
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 12, 'num_chars': 102}
Text: 


get_ipython().system('pip install wordcloud')


# # **3. Sentiment Analysis for Amazon Review**

# 


Chunk 292:
Document ID: a31643d1-df41-4acf-b30a-44163f326a08
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 60, 'num_chars': 512}
Text: 


# importing all the required Libraries
import glob
import json
import csv
import pandas as pd
import numpy as np
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from textblob import TextBlob
from textblob.sentiments import NaiveBayesAnalyzer
import string
import matplotlib.pyplot as plt
from nltk.stem import PorterStemmer
import warnings
warnings.filter


Chunk 293:
Document ID: a31643d1-df41-4acf-b30a-44163f326a08
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 6, 'num_chars': 73}
Text: port PorterStemmer
import warnings
warnings.filterwarnings("ignore")


# 


Chunk 294:
Document ID: a26c392a-1aaf-43ae-afa2-65882bde9d1f
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 6, 'num_chars': 69}
Text: 


from google.colab import drive
drive.mount('/content/gdrive')


# 


Chunk 295:
Document ID: 4a4e93a7-f1ec-4d70-9e72-4c66edbfddda
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 11, 'num_chars': 182}
Text: 


# Data download link:
# https://drive.google.com/drive/folders/0B4Hj2axlpCcxWldiajctWmY0NG8
file=glob.glob('/content/gdrive/My Drive/INFO 5731 TA/Datasets/ReviewSample.json')


# 


Chunk 296:
Document ID: bd5d3d6f-d18d-4cbb-997e-163dffa6a5fc
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 2, 'num_chars': 12}
Text: 


file


# 


Chunk 297:
Document ID: 64aacb9d-fbd8-40d2-8a9a-7b42852475ad
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 44, 'num_chars': 512}
Text: 


# Reading a multiple json files from a single json file 'ReviewSample.json'.
review=[]
with open(file[0]) as data_file:
    data=data_file.read()
    for i in data.split('\n'):
        review.append(i)

# Making a list of Tuples containg all the data of json files.
reviewDataframe=[]
for x in review:
    try:
        jdata=json.loads(x)
        reviewDataframe.append((jdata['reviewerID'],jdata['asin'],jdata['reviewerName'],jdata['helpful'][0],jdata['helpful'][1],jdata['reviewText'],jdata['overall'],jdata


Chunk 298:
Document ID: 64aacb9d-fbd8-40d2-8a9a-7b42852475ad
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 19, 'num_chars': 407}
Text: ul'][1],jdata['reviewText'],jdata['overall'],jdata['summary'],jdata['unixReviewTime'],jdata['reviewTime'])) 
    except:
        pass        

# Creating a dataframe using the list of Tuples got in the previous step.    
dataset=pd.DataFrame(reviewDataframe,columns=['Reviewer_ID','Asin','Reviewer_Name','helpful_UpVote','Total_Votes','Review_Text','Rating','Summary','Unix_Review_Time','Review_Time'])


# 


Chunk 299:
Document ID: d2c45e3f-61fa-4e93-9f81-d67d52421348
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 19, 'num_chars': 267}
Text: 


# Function to calculate sentiments using Naive Bayes Analyzer

def NaiveBaiyes_Sentimental(sentence):
    blob = TextBlob(sentence, analyzer=NaiveBayesAnalyzer())
    NaiveBayes_SentimentScore=blob.sentiment.classification
    return NaiveBayes_SentimentScore


# 


Chunk 300:
Document ID: fdbce9e2-e945-4e4f-9a48-58df8d7f577c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 52, 'num_chars': 512}
Text: 


# Function to calculate sentiments using Vader Sentiment Analyzer

# VADER sentiment analysis tool for getting Compound score.
def sentimental(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    return score

# VADER sentiment analysis tool for getting pos, neg and neu.
def sentimental_Score(sentence):
    analyzer = SentimentIntensityAnalyzer()
    vs = analyzer.polarity_scores(sentence)
    score=vs['compound']
    if score >= 


Chunk 301:
Document ID: fdbce9e2-e945-4e4f-9a48-58df8d7f577c
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 25, 'num_chars': 189}
Text: entence)
    score=vs['compound']
    if score >= 0.5:
        return 'pos'
    elif (score > -0.5) and (score < 0.5):
        return 'neu'
    elif score <= -0.5:
        return 'neg'


# 


Chunk 302:
Document ID: c32a399a-b527-4d82-9e50-a4f7af88c96e
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 32, 'num_chars': 383}
Text: 


# sentiment calculation by our data as input
Selected_Rows=dataset.head(10)
Selected_Rows['Sentiment_Score']=Selected_Rows['Review_Text'].apply(lambda x: sentimental_Score(x))
pos = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'pos']
print(pos)
neg = Selected_Rows.loc[Selected_Rows['Sentiment_Score'] == 'neg']
print(neg)


# # **4. Aspect Based sentiment Analysis**

# 


Chunk 303:
Document ID: 5a875779-a119-44e9-b432-956e02719b88
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 4, 'num_chars': 33}
Text: 


pip install pyabsa==1.9.3


# 


Chunk 304:
Document ID: a1f68a41-bb40-46dc-9c66-30381dc38d50
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 15, 'num_chars': 139}
Text: 


# Find Available Checkpoints For Current Version

from pyabsa import available_checkpoints
checkpoint_map = available_checkpoints()


# 


Chunk 305:
Document ID: 79edfad3-64eb-4c26-8999-bd4082d0b01d
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 17, 'num_chars': 257}
Text: 


from pyabsa import ATEPCCheckpointManager

aspect_extractor = ATEPCCheckpointManager.get_aspect_extractor(checkpoint='english',
                                   auto_device=True  # False means load model on CPU
                                   )


# 


Chunk 306:
Document ID: ac9c06a8-954e-45c0-86b9-76018130c2e1
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 28, 'num_chars': 304}
Text: 


examples = ['Staff was very rude but food was delicious']
inference_source = examples
atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source,  #
                          pred_sentiment=True,  # Predict the sentiment of extracted aspect terms
                          )


# 


Chunk 307:
Document ID: 39eaf018-9c66-443c-9247-054ed8c28cea
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 2, 'num_chars': 20}
Text: 


atepc_result


# 


Chunk 308:
Document ID: e7ebd67e-8a35-4036-9724-9230084cb1d8
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 29, 'num_chars': 313}
Text: 


examples = ['Camera quality is very good but battery drains fast']
inference_source = examples
atepc_result = aspect_extractor.extract_aspect(inference_source=inference_source,  #
                          pred_sentiment=True,  # Predict the sentiment of extracted aspect terms
                          )


# 


Chunk 309:
Document ID: ae5e7816-7d8a-464f-a68d-de28b931292f
Metadata: {'file_name': 'Week 10 Code Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 12/Week 10 Code Demo.ipynb', 'folder_name': 'Module 12', 'num_tokens': 4, 'num_chars': 31}
Text: 


atepc_result


# In[ ]:







Chunk 310:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text: Lesson 11: Text Clustering and Classification
Introduction to machine learning for text
Haihua Chen, Ph.D. Assistant Professor
1
2
Text Classification
3
4
Machine Learning
Text Clustering
Python Examples
1
What is Machine Learning?
Machine learning consists of applying mathematical and statistical approaches to get machines to learn from data. It consists of four big families of techniques: Supervised learning Semi-supervised learning Unsupervised learning Reinforcement learning
1.1. Why is Machine learning


Chunk 311:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 79, 'num_chars': 512}
Text: einforcement learning
1.1. Why is Machine learning?
1.2. Supervised learning
1.3. Semi-supervised learning
1.4. Unsupervised learning
1.5. Reinforcement learning
1.6. How is text clustering different from text classification?
2
Text Clustering
Text clustering is the task of grouping a set of unlabeled texts in such a way that texts in the same cluster are more similar to each other than to those in other clusters. Text clustering algorithms process text and determine if natural clusters (groups) exist in th


Chunk 312:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 72, 'num_chars': 512}
Text: determine if natural clusters (groups) exist in the data:
2.1. What is text clustering?
Text clustering
Types of clustering
Document Retrieval: To improve recall, start by adding other documents from the same cluster. Taxonomy Generation: Automatically generate hierarchical taxonomies for browsing content. Fake News Identification: Detect if a news is genuine or fake. Language Translation: Translation of a sentence from one language to another. Spam Mail Filtering: Detect unsolicited and unwanted email/mess


Chunk 313:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 72, 'num_chars': 512}
Text: tering: Detect unsolicited and unwanted email/messages. Customer Support Issue Analysis: Identify commonly reported support issues. Others?
2.2. Text clustering applications?
Document level: It serves to regroup documents about the same topic. Document clustering has applications in news articles, emails, search engines, etc. Sentence level: It's used to cluster sentences derived from different documents. Tweet analysis is an example. Word level: Word clusters are groups of words based on a common theme. Th


Chunk 314:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 77, 'num_chars': 512}
Text: rs are groups of words based on a common theme. The easiest way to build a cluster is by collecting synonyms for a particular word. For example, WordNet is a lexical database for the English language that groups English words into sets of synonyms called synsets.
2.3. Levels of text clustering?
Text pre-processing: Text can be noisy, hiding information between stop words, inflexions and sparse representations. Pre-processing makes the dataset easier to work with (Tokenization, Transformation, Normalization,


Chunk 315:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 69, 'num_chars': 512}
Text: with (Tokenization, Transformation, Normalization, Filtering). Feature Extraction: One of the commonly used technique to extract the features from textual data is calculating the frequency of words/tokens in the document/corpus (Lexical features, Semantic features, Word embedding features). Clustering: We can then cluster different text documents based on the features we have generated.
2.4. Steps of text clustering?
Lexical similarity: Words are similar lexically if they have a similar character sequence. 


Chunk 316:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 78, 'num_chars': 512}
Text: ically if they have a similar character sequence. Lexical similarity can be measured using string-based algorithms that operate on string sequences and character composition. Semantic similarity: Words are similar semantically if they have the same meaning, are opposite of each other, used in the same way, used in the same context or one is a type of another. Semantic similarity can be measured using corpus-based or knowledge-based algorithms. Some of the metrics for computing similarity between two pieces 


Chunk 317:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text: trics for computing similarity between two pieces of text are Jaccard coefficient, cosine similarity and Euclidean distance.
2.5. Measuring similarity in text clustering?
Centroid-based clustering: we form clusters around several points that act as the centroids Density-based clustering: the clustering doesn’t happen around centroid or central points, but the cluster forms where the density looks higher Distribution-based clustering: if the distance between the point and the central distribution of points i


Chunk 318:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 76, 'num_chars': 512}
Text: the point and the central distribution of points increases, then the probability of the point being included in the distribution decreases. Hierarchical clustering: we deal with either merging of clusters or division of a big cluster
2.6. Text clustering algorithms?
K-means Clustering: The algorithm measures the Euclidean distances between the datapoints and all k centroids
2.6.1 Centroid-based clustering
DBSCAN uses a fixed distance for separating the dense clusters from the noise datapoints. It is the fas


Chunk 319:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 77, 'num_chars': 512}
Text:  clusters from the noise datapoints. It is the fastest among clustering algorithms. from sklearn.cluster import DBSCAN model = DBSCAN(eps=0.20, min_samples=5) eps is the max. distance between two data points. min_samples helps to set the minimum number of samples we want within a neighborhood collection of features. HDBSCAN uses a range of distances to separate itself from the noise. It requires the least amount of user-input. OPTICS measures the distance between neighboring features, and it draws a reachab


Chunk 320:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 80, 'num_chars': 512}
Text: tween neighboring features, and it draws a reachability plot to separate itself from the noise datapoints.
2.6.2 Density-based clustering
2.6.2 Density-based clustering-DBSCAN
Algorithmic steps for DBSCAN clustering The algorithm proceeds by arbitrarily picking up a point in the dataset (until all points have been visited). If there are at least ‘minPoint’ points within a radius of ‘ε’ to the point then we consider all these points to be part of the same cluster. The clusters are then expanded by recursivel


Chunk 321:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 76, 'num_chars': 512}
Text: ster. The clusters are then expanded by recursively repeating the neighborhood calculation for each neighboring point
2.6.2 Density-based clustering-DBSCAN
Gaussian distribution model: there will be a fixed number of distributors for the Gaussian model. These distributors are concentric figures with decreasing color intensity from inside to the outside. The central part tends to be denser, and it tends to decrease as we go outside as we have bigger distributors now. So, even though distributors might contai


Chunk 322:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 81, 'num_chars': 512}
Text: ors now. So, even though distributors might contain the same number of points, their densities may still differ due to the size of distributors. Overfitting can be a bit of a problem for this type of clustering. As long as we don’t set any strong criteria for points, we can avoid overfitting.
2.6.3 Distribution-based clustering
Agglomerative hierarchical clustering: merge smaller clusters into bigger clusters. But this also has some process. If the two clusters that we compare have similarities between them


Chunk 323:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 84, 'num_chars': 512}
Text: ers that we compare have similarities between them and if they are near to each other, then we merge them. Divisive hierarchical clustering: divide one big cluster into n-smaller clusters. The clusters here are divided if some datapoints are not similar to the larger cluster; we separate them and make an individual cluster for them. In hierarchical clustering, the most important job is to calculate the similarity between clusters. It’s the most important part as it helps us to understand whether to merge or


Chunk 324:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 73, 'num_chars': 512}
Text: t as it helps us to understand whether to merge or divide the cluster. We have several methods like: Single Linkage Algorithm (MIN) Complete Linkage Algorithm (MAX) Group Average Distance Between the Centroids Ward’s Method
2.6.4 Hierarchical clustering
2.6.4 Hierarchical clustering
External quality measure: External knowledge is required for measuring the external quality. For example, we can conduct surveys of users of the application that includes text clustering. Internal quality measure: The evaluation


Chunk 325:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 73, 'num_chars': 512}
Text: ustering. Internal quality measure: The evaluation of the clustering is compared only with the result itself, that is, the structure of found clusters and their relations to one another. Two main concepts are compactness and separation. Compactness measures how closely data points are grouped in a cluster. Separation measures how different the found clusters are from each other. More formally, compactness is intra-cluster variance whereas separation is inter-cluster distance.
2.7. Text clustering evaluation


Chunk 326:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text: -cluster distance.
2.7. Text clustering evaluation?
Document clustering is being studied for many decades. It's far from trivial or a solved problem. The challenges include the following: Selecting appropriate features of documents that should be used for clustering. Selecting an appropriate similarity measure between documents. Selecting an appropriate clustering method utilizing the above similarity measure. Implementing the clustering algorithm in an efficient way that makes it feasible in terms of memor


Chunk 327:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 74, 'num_chars': 512}
Text: cient way that makes it feasible in terms of memory and CPU resources. Finding ways of assessing the quality of the performed clustering.
2.8. Text clustering challenges?
3
Text Classification
Text classification is a simple, powerful analysis technique to sort the text repository under various tags, each representing specific meaning. Typical classification examples include categorizing customer feedback as positive or negative, or news as sports or politics.
3.1. What is text classification?
Text classifi


Chunk 328:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 68, 'num_chars': 512}
Text: s.
3.1. What is text classification?
Text classification example
Text classification process
Information Retrieval: With the rapid growth of online information, particularly in text format, text classification has become a significant technique for managing this type of data Information Filtering: Information filtering refers to the selection of relevant information or rejection of irrelevant information from a stream of incoming data. Sentiment Analysis: Sentiment classification methods classify a document


Chunk 329:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text: ntiment classification methods classify a document associated with an opinion to be positive or negative. Recommender Systems: Content-based recommender systems suggest items to users based on the description of an item and a profile of the user’s interests Knowledge Management: text classification has been used to find the relationship between railroad accidents’ causes and their correspondent descriptions in reports Document Summarization: Text classification used for document summarizing in which the sum


Chunk 330:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 73, 'num_chars': 512}
Text: ion used for document summarizing in which the summary of a document may employ words or phrases which do not appear in the original document Others: Health, Social Sciences, Business and Marketing, Law
3.2. Text classification applications?
Document level Paragraph level Sentence level Sub-sentence level
3.3. Levels of text classification?
Tokenization: Identifying words, symbols, emojis, hyperlinks, based on known delimiters and format rules. Word normalization: Reduce derived words into their root form (


Chunk 331:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text: ation: Reduce derived words into their root form (developmental becomes develop, encouragement becomes encourage). Text and feature encoding: ML models require numeric features and labels to provide a prediction. Feature representation: Every feature (category) is represented as a Word Count Vector (giving frequency count of each feature) or a TF-IDF vector (Term Frequency/Inverse Document Frequency) representing relative importance of a term in a document. Word/Document embedding: Every row in the dataset 


Chunk 332:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 75, 'num_chars': 512}
Text: Word/Document embedding: Every row in the dataset is an entire document, represented as a dense vector. The word position within the vector is learned from text and based on the surrounding words. Word embeddings can be trained using the input corpus but pre-trained embeddings (Glove, FastText, and Word2Vec) are available.
3.4. Steps of text classification?
Naive Bayes Classifier K-nearest Neighbor Support Vector Machine (SVM) Decision Tree Random Forest Deep Learning
3.5. Text classification algorithms?
3.


Chunk 333:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 68, 'num_chars': 512}
Text: p Learning
3.5. Text classification algorithms?
3.5.1 Naive Bayes Classifier
3.5.2 K-nearest Neighbor
3.5.3 Support Vector Machine
https://en.wikipedia.org/wiki/Support_vector_machine
3.5.4 Decision Tree
https://scikit-learn.org/1.5/modules/tree.html
3.5.5 Random Forest
Step 1: In Random forest n number of random records are taken from the data set having k number of records. Step 2: Individual decision trees are constructed for each sample. Step 3: Each decision tree will generate an output. Step 4: Final 


Chunk 334:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 73, 'num_chars': 512}
Text: ision tree will generate an output. Step 4: Final output is considered based on Majority Voting or Averaging for Classification and regression respectively.
3.5.6 Deep Learning
RNN
CNN
BERT
3.5.7 Semi-supervised learning
Pseudo-labeling: in pseudo labeling, a classifier is trained on both labeled data and the most confident predictions of the previous classifiers on unlabeled data. Co-training: Co-training is an extension of self-training to multiple classifiers that are iteratively retrained on each other’


Chunk 335:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 78, 'num_chars': 512}
Text: iers that are iteratively retrained on each other’s most confident predictions. Transfer Learning: Transfer learning (TL) has been widely used for improving the performance of machine learning across many tasks, and transfer learning by fine-tuning pre-trained neural networks outperforms the networks that are trained from scratch on the same data Active Learning: Active Learning aims to find the most efficient way to query for labels and learn a classifier with the minimal amount of human supervision. It in


Chunk 336:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 74, 'num_chars': 512}
Text: ith the minimal amount of human supervision. It interactively assigns certain specific data points to users for annotation by identifying the best data to annotate next. Expectation-maximization: EM is a class of iterative algorithms for maximum likelihood or maximum a posteriori estimation in problems with incomplete data. GAN: where a “generator” is trained to produce samples resembling some data distribution. The training process “adversarial” depends on a “discriminator”, which is instead trained to dis


Chunk 337:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 84, 'num_chars': 512}
Text: a “discriminator”, which is instead trained to distinguish samples of the generator from the real instances.
3.6. Factors behind the choice of an ML algorithm?
N-fold cross-validation: Split dataset into N folds. Runs test N times. At a time, use one-fold of data as test set, remaining N - 1 folds of data as training sets. Classification accuracy is average of results in N runs. Hold-out test: Divide dataset into training and test subsets. Varied splits will result in varied results and accuracy, especially


Chunk 338:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 71, 'num_chars': 512}
Text:  result in varied results and accuracy, especially for small datasets. Paired t-test can be used to measure significance in accuracy differences. Evaluation metrics: Accuracy, recall, precision, F-score
3.7. Text classification evaluation?
Data Zero-shot/Few-shot learning. The current model of deep learning is too dependent on numerous labeled data. The performance of these models is significantly affected in zero-shot or few-shot learning. The external knowledge. As we all know, the more beneficial informa


Chunk 339:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 77, 'num_chars': 512}
Text: ledge. As we all know, the more beneficial information is input into a DNN, its better performance. Therefore, we believe that adding external knowledge (knowledge base or knowledge graph) is an efficient way to promote the model’s performance. Nevertheless, how and what to add is still a challenge. The multi-label text classification task. Special domain with many terminologies. Models How to tradeoff between data and compute resources and prediction performance is worth studying Performance The semantic r


Chunk 340:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 34, 'num_chars': 512}
Text: mance is worth studying Performance The semantic robustness of the model. The interpretability of the model.
3.8. Text classification challenges?
4
Python Examples
K-means: https://github.com/MarcusChong123/Text-Clustering-with-Python DBSCAN: https://github.com/arnab64/textclusteringDBSCAN Gaussian Mixture Models: https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb#scrollTo=6Lj0uoesKOQw Hierarchical clustering: https://github.com/dip


Chunk 341:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 29, 'num_chars': 512}
Text: Qw Hierarchical clustering: https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch07%20-%20Text%20Similarity%20and%20Clustering/Ch07c%20-%20Document%20Clustering.ipynb
5.1. Text Clustering
5.2. Text classification – Supervised learning
Text Classification Algorithms: A Survey https://github.com/kk7nc/Text_Classification#conditional-random-field-crf
5.3. Text classification – Semi-supervised learning
Awesome Semi-Supervised Learning: https://github.com/yassouali/awesome-sem


Chunk 342:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 17, 'num_chars': 512}
Text: Learning: https://github.com/yassouali/awesome-semi-supervised-learning Pseudo-labeling: https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af Co-training: https://github.com/revorg7/co-training Expectation-maximization(EM) + Naive Bayes(NB): https://github.com/jerry-shijieli/Text_Classification_Using_EM_And_Semisupervied_Learning Pseudo-labeling + DNN: https://github.com/nanazhu/Pseudo-Label-for-Deep-Neural-Networks GAN-BERT: https://github.com/crux82/ganbert



Chunk 343:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 29, 'num_chars': 474}
Text: works GAN-BERT: https://github.com/crux82/ganbert
5.4. Text classification – Reinforcement learning
Reinforcement Learning Toturial: https://techvidvan.com/tutorials/reinforcement-learning/ Reinforcement Learning: An Introduction: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction Reinforcement Learning algorithms code: https://github.com/dennybritz/reinforcement-learning Reinforcement Learning course: https://www.davidsilver.uk/teaching/
Thank you


Chunk 344:
Document ID: eeed6754-88af-4301-9bd4-135b4c2d2561
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 13/Lesson eleven-text clustering and classification-11132024-1.pptx', 'folder_name': 'Module 13', 'num_tokens': 3, 'num_chars': 12}
Text: g/
Thank you


Chunk 345:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 89, 'num_chars': 512}
Text: Lesson 2: Python Basic (1)
Haihua Chen, Ph.D.
2025/3/6
1
1
2
3
4
Basic Syntax
Python Variables
Python Operators
Python Data Types
2025/3/6
2
5
Python Files
6
Loop
7
Conditionals
8
In-class Quiz 1
1
Basic Syntax
2025/3/6
3
a high-level programming language like Java, C#, C++ et al.
1. Execute Python Syntax
Interpreter: immediate mode and script mode
2025/3/6
4
Python Indentation
2025/3/6
5
Indentation refers to the spaces at the beginning of a code line.
Python uses indentation to indicate a block of code.
P


Chunk 346:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 92, 'num_chars': 512}
Text: on uses indentation to indicate a block of code.
Python Comments
2025/3/6
6
Comments can be used to explain Python code.
Comments can be used to make the code more readable.
Comments can be used to prevent execution when testing code.
2
Python Variables
2025/3/6
7
A variable is a name that refers to a value
Variables
2025/3/6
8
A variable can have a short name (like x and y) or a more descriptive name (age, carname, total_volume)
A variable name must start with a letter or the underscore character
A variabl


Chunk 347:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 85, 'num_chars': 512}
Text: ith a letter or the underscore character
A variable name cannot start with a number
A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )
Variable names are case-sensitive
These words can’t be used as variables
Variable names and keywords
2025/3/6
9
3
Python Data Types
2025/3/6
10
Commonly used python data types
2025/3/6
11
Python Numbers
2025/3/6
12
int
float
complex
Python Numbers - Type Conversion
2025/3/6
13
You can convert from one type to another with the int(), 


Chunk 348:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 80, 'num_chars': 512}
Text:  convert from one type to another with the int(), float(), and complex() methods
Python Numbers - Random Number
2025/3/6
14
Python has a built-in module called random that can be used to make random numbers
Python String
2025/3/6
15
Either single quotation marks, or double quotation marks
Multiline Strings
Strings are Arrays: len(), strip(), lower(), upper(), replace(), split(), format()
Python Casting
2025/3/6
16
int() - constructs an integer number from an integer literal, a float literal, or a string lit


Chunk 349:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 512}
Text:  integer literal, a float literal, or a string literal
float() - constructs a float number from an integer literal, a float literal or a string literal
str() - constructs a string from a wide variety of data types, including strings, integer literals and float literals
Python Booleans
2025/3/6
17
Booleans represent one of two values: True or False.
Python Booleans - Evaluate Values and Variables
2025/3/6
18
Almost any value is evaluated to True if it has some sort of content.
Any string is True, except empt


Chunk 350:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 81, 'num_chars': 512}
Text: e sort of content.
Any string is True, except empty strings.
Any number is True, except 0.
Any list, tuple, set, and dictionary are True, except empty ones.
There are not many values that evaluates to False, except empty values, such as (), [], {}, "", the number 0, and the value None.
Python List
2025/3/6
19
List is a collection which is ordered and changeable. Allows duplicate members.
W3schools_list: https://www.w3schools.com/python/python_lists.asp
Python Tuple
2025/3/6
20
Tuple is a collection which is


Chunk 351:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 55, 'num_chars': 512}
Text: n Tuple
2025/3/6
20
Tuple is a collection which is ordered and unchangeable. Allows duplicate members.
W3schools_tuple: https://www.w3schools.com/python/python_tuples.asp
Python Set
2025/3/6
21
Set is a collection which is unordered and unindexed. No duplicate members.
W3schools_set: https://www.w3schools.com/python/python_dictionaries.asp
Python Dictionary
2025/3/6
22
Dictionary is a collection which is unordered, changeable and indexed. No duplicate members.
W3schools_dictionary: https://www.w3schools.com


Chunk 352:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 65, 'num_chars': 512}
Text: s.
W3schools_dictionary: https://www.w3schools.com/python/python_lists.asp
4
Python Operators
2025/3/6
23
Python Operators - Arithmetic Operators
Arithmetic operators are used with numeric values to perform common mathematical operations
2025/3/6
24
Python Operators - Assignment Operators
Assignment operators are used to assign values to variables
2025/3/6
25
Python Operators - Comparison Operators
Comparison operators are used to compare two values
2025/3/6
26
Python Operators - Logical Operators
Logical o


Chunk 353:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 81, 'num_chars': 512}
Text: 
26
Python Operators - Logical Operators
Logical operators are used to combine conditional statements
2025/3/6
27
Python Operators - Identity Operators
Identity operators are used to compare the objects, not if they are equal, but if they are actually the same object, with the same memory location
2025/3/6
28
Python Operators - Membership Operators
Membership operators are used to test if a sequence is presented in an object:
2025/3/6
29
Python Operators - Bitwise Operators
Bitwise operators are used to com


Chunk 354:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 512}
Text: itwise Operators
Bitwise operators are used to compare (binary) numbers
2025/3/6
30
5
Python Files
2025/3/6
31
Reading and writing
To write a file, you have to open it with mode 'w' as a second parameter
2025/3/6
32
Filenames and paths
The os module provides functions for working with files and directories
2025/3/6
33
Input
Input is a built-in function in Python for getting input from the user Name=input(“please enter your name:”)
2025/3/6
34
6
Loop
2025/3/6
35
Problem Statement
It is really bad to repeat s


Chunk 355:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 85, 'num_chars': 512}
Text: 
35
Problem Statement
It is really bad to repeat similar codes again and again.
Python’s for loop could solve this kind of problems
for loop
We want to invite several friends to join a party.
for loop – invite friends
friend in line 1 is called loop variable
Line 2 and 3 are loop body
for--improved herd of turtles
for i in range(8)
for _ in range(8)
for color in [“yellow”,”red”,”purple”,”blue”] alex.color(color)
alex.shape(“turtle”)
for--Using for loop to print tables
The while statement--Using for loop to 


Chunk 356:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 512}
Text: int tables
The while statement--Using for loop to print tables
while condition: statement
The while statement—Another example
while condition: statement
Question: how to choose between for and while?
Use a for loop if you know, before you start looping, the maximum number of times that you’ll need to execute the body.
So any problem like “iterate this weather model for 1000 cycles”, or “search this list of words”, “find all prime numbers up to 10000” suggest that a for loop is best.
By contrast, if you are 


Chunk 357:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 95, 'num_chars': 512}
Text:  that a for loop is best.
By contrast, if you are required to repeat some computation until some condition is met, and you cannot calculate in advance when (of if) this will happen, as we did in this 3n + 1 problem, you’ll need a while loop.
We call the first case definite iteration — we know ahead of time some definite bounds for what is needed. The latter case is called indefinite iteration — we’re not sure how many iterations we’ll need — we cannot even establish an upper bound!
7
Conditionals
2025/3/6
4


Chunk 358:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 71, 'num_chars': 512}
Text: stablish an upper bound!
7
Conditionals
2025/3/6
44
if statement
If boolean expression: statement_1 else: statement_2
Omitting the else clause
If boolean expression: statement_1 statement_2
Chained conditionals
If x<y: statement_1 elif x>y: statement_2 Else: statement_3
Nested conditionals
If x<y: statement_1 else: if x>y: statement_2 else: statement_3
The break statement
The break statement is used to immediately leave the body of its loop.
continue statement
continue causes the program to immediately skip


Chunk 359:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 88, 'num_chars': 512}
Text: nt
continue causes the program to immediately skip the processing of the rest of the body of the loop
pass statement
pass statement is used when a statement is required syntactically but you do not want any command or code to execute. It is like null operation, as nothing will happen is it is executed
Question: What's the difference between "break" and "continue" in Python?
Break: Jumps out of the closest enclosing loop (past the entire loop statement)
Continue: Jumps to the top of the closest enclosing loo


Chunk 360:
Document ID: 1b1820c1-acc7-4ca2-8891-7dfaf6111107
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson two-Python Basic-Fall2024.pptx', 'folder_name': 'Module 2', 'num_tokens': 34, 'num_chars': 188}
Text: nue: Jumps to the top of the closest enclosing loop (to the loop’s header line)
8
In-class Quiz 1
2025/3/6
53
Can be accessed on Canvas!
In-class Exercise
2025/3/6
54
Thank you
2025/3/6
55


Chunk 361:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 82, 'num_chars': 512}
Text: 


# Python Indentations: Where in other programming languages the indentation in code is for readability only, in Python the indentation is very important. Python uses indentation to indicate a block of code.
if 5 > 2:
  print("Five is greater than two!")
if(7<2):
  print("Seven is not greater than 2? false")
else: 
  print("Ok, 7>2")


# (1) Python Variables: integer, floats, booleans, strings 

# In[ ]:


### Define variables
x = 5
y = "John"
z = 1000.0
m = True
print(x)
print(type(x))
print(y)
print(typ


Chunk 362:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 94, 'num_chars': 512}
Text:  = True
print(x)
print(type(x))
print(y)
print(type(y))
print(z)
print(type(z))
print(m)
print(type(m))


# In[ ]:


x = 4 # x is of type int
print(type(x))
x = "Sally" # x is now of type str
print(x)
print(type(x))


# In[ ]:


x = "John"
print(x)
# is the same as
x = 'John'
print(x)


# # **Variable Names**
# 
# A variable can have a short name (like x and y) or a more descriptive name (age, carname, total_volume). Rules for Python variables:
# 
# *   A variable name must start with a letter or the unders


Chunk 363:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 85, 'num_chars': 512}
Text: riable name must start with a letter or the underscore character
# *   A variable name cannot start with a number
# *   A variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )
# *   Variable names are case-sensitive (age, Age and AGE are three different variables)
# 
# 

# Assign Value to Multiple Variables
# 

# In[ ]:


x, y, z = "Orange", "Banana", "Cherry"
print(x)
print(y)
print(z)

print("--------------------------")
a = b = c = "Orange"
print(a)
print(b)
print(c)



Chunk 364:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 92, 'num_chars': 512}
Text: )
a = b = c = "Orange"
print(a)
print(b)
print(c)


# # Python Numbers
# There are three numeric types in Python:
# 
# *   int
# *   float
# *   complex
# 
# Variables of numeric types are created when you assign a value to them:
# 
# 
# 
# 

# In[ ]:


# Example 1:

x = 1    # int
y = 2.8  # float
z = 1j   # complex

print(type(x))
print(type(y))
print(type(z))

print("-------------------------------------")


# Example 2:

x = 1
y = 35656222554887711
z = -3255522

print(type(x))
print(type(y))
print(type(


Chunk 365:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 76, 'num_chars': 512}
Text: 3255522

print(type(x))
print(type(y))
print(type(z))

print("-------------------------------------")

# Example 3:


x = 1.10
y = 12E4
z = -35.59

print(type(x))
print(type(y))
print(type(z))

print("-------------------------------------")

# Example 4:

x = 3+5j
y = 5j
z = -5j

print(type(x))
print(type(y))
print(type(z))


# # Type Conversion
# You can convert from one type to another with the int(), float(), and complex() methods:

# In[ ]:


# Convert from one type to another:

x = 1 # int
y = 2.8 # fl


Chunk 366:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 96, 'num_chars': 512}
Text: rom one type to another:

x = 1 # int
y = 2.8 # float
z = 1j # complex

#convert from int to float:
a = float(x)

#convert from float to int:
b = int(y)

#convert from int to complex:
c = complex(x)

print(a)
print(b)
print(c)

print(type(a))
print(type(b))
print(type(c))


# # Random Number
# Python does not have a random() function to make a random number, but Python has a built-in module called random that can be used to make random numbers:

# In[ ]:


# Import the random module, and display a random nu


Chunk 367:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 88, 'num_chars': 512}
Text:  Import the random module, and display a random number between 1 and 9:

import random

print(random.randrange(1,10))


# # Python Strings
# String literals in python are surrounded by either single quotation marks, or double quotation marks.
# 'hello' is the same as "hello".

# In[ ]:


# Example 1
# Assign String to a Variable: Assigning a string to a variable is done with the variable name followed by an equal sign and the string:

a = "Hello"
print(a)

# Example 2
# Multiline Strings: assign a multiline


Chunk 368:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 83, 'num_chars': 512}
Text:  Example 2
# Multiline Strings: assign a multiline string to a variable by using three quotes (three double quotes or three single quotes)
b= """Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua."""
print(b)
c= '''Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.'''
print(c)


# Strings are Arrays

# In[ ]:


# Example 3
# Get the character at position 1 (remembe


Chunk 369:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 74, 'num_chars': 512}
Text: ample 3
# Get the character at position 1 (remember that the first character has the position 0):
a = "Hello, World!"
print(a[1])

print("-------------------------------------")

# Example 4
# Substring. Get the characters from position 2 to position 5 (not included):
b = "Hello, World!"
print(b[2:5])

print("-------------------------------------")


# Example 5
# The strip() method removes any whitespace from the beginning or the end:
c = " Hello, World! "
print(c.strip()) # returns "Hello, World!"

print(


Chunk 370:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 72, 'num_chars': 512}
Text: print(c.strip()) # returns "Hello, World!"

print("-------------------------------------")


# Example 6
# The len() method returns the length of a string:

d = " Hello, World! "
print(a.strip()) # returns "Hello, World!"
print("-------------------------------------")


# Example 6
# The lower() method returns the string in lower case:

a = "Hello, World!"
print(a.lower())



# # Python Functions
# In the context of programming, a function is a named sequence of statements that performs a computation. When 


Chunk 371:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 91, 'num_chars': 512}
Text: e of statements that performs a computation. When you define a function, you specify the name and the sequence of statements. Later, you can “call” the function by name.
# 
# A function is a block of code which only runs when it is called. You can pass data, known as parameters, into a function. A function can return data as a result.
# 

# In[ ]:


# Example 1: Creating a Function
# In Python a function is defined using the def keyword:

def my_function_1():
  print("Hello from a function")
  print("------


Chunk 372:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 64, 'num_chars': 512}
Text: :
  print("Hello from a function")
  print("-------------------------------------")


# Example 2: Calling a Function
# To call a function, use the function name followed by parenthesis:

def my_function_2():
  print("Hello from a function")
  print("*************************************")

my_function_2()


# Example 3: Parameters
# Parameters are specified after the function name, inside the parentheses. You can add as many parameters as you want, just separate them with a comma:

def my_function_3(fname)


Chunk 373:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 66, 'num_chars': 512}
Text: arate them with a comma:

def my_function_3(fname):
  print(fname + " Refsnes")

my_function_3("Emil")
my_function_3("Tobias")
my_function_3("Linus")


# Example 4: Default Parameter Value
# If we call the function without parameter, it uses the default value:

def my_function_4(country = "Norway"):
  print("I am from " + country)

my_function_4("Sweden")
my_function_4("India")
my_function_4()
my_function_4("Brazil")


# Example 5: Passing a List as a Parameter
# You can send any data types of parameter to 


Chunk 374:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 75, 'num_chars': 512}
Text: ter
# You can send any data types of parameter to a function (string, number, list, dictionary etc.), and it will be treated as the same data type inside the function.

def my_function_5(food):
  for x in food:
    print(x)

fruits = ["apple", "banana", "cherry"]

my_function_5(fruits)

# Example 6: Return Values
# To let a function return a value, use the return statement:

def my_function_6(x):
  return 5 * x

print(my_function_6(3))
print(my_function_6(5))
print(my_function_6(9))


# Example 7: Keyword A


Chunk 375:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 83, 'num_chars': 512}
Text: )
print(my_function_6(9))


# Example 7: Keyword Arguments
# You can also send arguments with the key = value syntax. This way the order of the arguments does not matter.

def my_function_7(child3, child2, child1):
  print("The youngest child is " + child3)

my_function_7(child1 = "Emil", child2 = "Tobias", child3 = "Linus")


# Example 8: Arbitrary Arguments
# If you do not know how many arguments that will be passed into your function, add a * before the parameter name in the function definition.

def my_


Chunk 376:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 62, 'num_chars': 512}
Text: arameter name in the function definition.

def my_function_8(*kids):
  print("The youngest child is " + kids[2])

my_function_8("Emil", "Tobias", "Linus")


# Example 9: Recursion

def tri_recursion(k):
  if(k>0):
    result = k+tri_recursion(k-1)
    print(result)
  else:
    result = 0
  return result

print("\n\nRecursion Example Results")
tri_recursion(6)



# # Three common ways to use python functions
# 
# 
# 1. Python Built-in Functions:
# 
# https://docs.python.org/3/library/functions.html
# 
# 
# 2


Chunk 377:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 76, 'num_chars': 512}
Text: docs.python.org/3/library/functions.html
# 
# 
# 2. Python libraries:
# 
# 20 Python libraries you can’t live without
# 
# https://pythontips.com/2013/07/30/20-python-libraries-you-cant-live-without/
# 
# 
# 3. Define the functions by yourself.

# # Python I/O
# There are several different ways of handling input/output.
# 

# In[ ]:


# Example 1: Keyboard Input and Output

# Store input numbers
num1 = input('Enter first number: ')
num2 = input('Enter second number: ')
# Add two numbers
sum = float(num1) + 


Chunk 378:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 91, 'num_chars': 512}
Text:  number: ')
# Add two numbers
sum = float(num1) + float(num2)
# Display the sum
print('The sum of {0} and {1} is {2}'.format(num1, num2, sum))



# In[ ]:


# Example 2: Opening and Closing Files

# Open a file
fo = open("california_housing_test.csv", "wb")
print ("Name of the file: ", fo.name)
print ("Closed or not : ", fo.closed)
print ("Opening mode : ", fo.mode)

# Close opend file
fo.close()


# In[ ]:


# Example 3: Writing into a file

# Open a file
fo = open("foo.txt", "w")
fo.write( "Python is a gr


Chunk 379:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 90, 'num_chars': 512}
Text: o = open("foo.txt", "w")
fo.write( "Python is a great language.\nYeah its great!!\n")

# Close opend file
fo.close()


# Python is a great language. and Yeah its great!! will be written in foo.txt


# # **Excise for assignent one (Exercise 8-5):**
# 
# A Caesar cypher is a weak form of encryption that involves “rotating” each letter by a fixed number of places. To rotate a letter means to shift it through the alphabet, wrapping around to the beginning if necessary, so ’A’ rotated by 3 is ’D’ and ’Z’ rotated


Chunk 380:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 99, 'num_chars': 512}
Text: essary, so ’A’ rotated by 3 is ’D’ and ’Z’ rotated by 1 is ’A’.
# 
# To rotate a word, rotate each letter by the same amount. For example, “cheer” rotated by 7 is “jolly” and “melon” rotated by -10 is “cubed”. In the movie 2001: A Space Odyssey, the ship computer is called HAL, which is IBM rotated by -1.
# 
# Write a function called rotate_word that takes a string and an integer as parameters, and returns a new string that contains the letters from the original string rotated by the given amount.
# 
# You 


Chunk 381:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 90, 'num_chars': 512}
Text: inal string rotated by the given amount.
# 
# You might want to use the built-in function ord, which converts a character to a numeric code, and chr, which converts numeric codes to characters. Letters of the alphabet are encoded in alphabetical order, so for example:
# 
# \>>>ord('c') - ord('a')
# 
# 2
# 
# Because 'c' is the two-eth letter of the alphabet. But beware: the numeric codes for upper case letters are different.
# 
# Potentially offensive jokes on the Internet are sometimes encoded in ROT13, wh


Chunk 382:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 68, 'num_chars': 512}
Text: on the Internet are sometimes encoded in ROT13, which is a Caesar cypher with rotation 13. If you are not easily offended, find and decode some of them.
# 
# **Solution:**
# http://thinkpython2.com/code/rotate.py.
# 

# In[ ]:


"""This module contains a code example related to

Think Python, 2nd Edition
by Allen Downey
http://thinkpython2.com

Copyright 2015 Allen Downey

License: http://creativecommons.org/licenses/by/4.0/
"""

from __future__ import print_function, division


def rotate_letter(letter, n)


Chunk 383:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 67, 'num_chars': 512}
Text: _function, division


def rotate_letter(letter, n):
    """Rotates a letter by n places.  Does not change other chars.

    letter: single-letter string
    n: int

    Returns: single-letter string
    """
    if letter.isupper():
        start = ord('A')
    elif letter.islower():
        start = ord('a')
    else:
        return letter

    c = ord(letter) - start
    i = (c + n) % 26 + start
    return chr(i)


def rotate_word(word, n):
    """Rotates a word by n places.

    word: string
    n: integer


Chunk 384:
Document ID: 16cb9d5e-6b8e-49e7-8464-b6a12c0e8180
Metadata: {'file_name': 'Lesson_two_example1.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example1.ipynb', 'folder_name': 'Module 2', 'num_tokens': 34, 'num_chars': 309}
Text: word by n places.

    word: string
    n: integer

    Returns: string
    """
    res = ''
    for letter in word:
        res += rotate_letter(letter, n)
    return res


if __name__ == '__main__':
    print(rotate_word('cheer', 7))
    print(rotate_word('melon', -10))
    print(rotate_word('sleep', 9))




Chunk 385:
Document ID: 6e749607-3a73-481f-a87d-d3a1d3c9b247
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 34, 'num_chars': 195}
Text: 


# Assign a list to an variable named my_list
my_list = [1,2,3]
print(my_list)


# We just created a list of integers, but lists can actually hold different object types. For example:
# 
# 

# 


Chunk 386:
Document ID: e1c2c2b0-00ae-4ad6-9622-ab8acd156b6d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 27, 'num_chars': 166}
Text: 


my_list = ['A string',23,100.232,'o']
print(my_list)


# Just like strings, the len() function will tell you how many items are in the sequence of the list.
# 

# 


Chunk 387:
Document ID: 04919201-2058-4c06-ae23-9645eb7880ed
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 30, 'num_chars': 164}
Text: 


len(my_list)


# # 1.2 Indexing and Slicing
# Indexing and slicing work just like in strings. Let's make a new list to remind ourselves of how this works:
# 

# 


Chunk 388:
Document ID: 436bc1dc-1c80-4c76-ac07-718d91db98e1
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 11, 'num_chars': 80}
Text: 


my_list = ['one','two','three',4,5]
# Grab element at index 0
my_list[0]


# 


Chunk 389:
Document ID: dfe8e823-0fac-4953-835d-91441fe10375
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 10, 'num_chars': 57}
Text: 


# Grab index 1 and everything past it
my_list[1:]


# 


Chunk 390:
Document ID: 668c1798-9dec-4ee6-adc1-e40032f88ac0
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 24, 'num_chars': 124}
Text: 


# Grab everything UP TO index 3
my_list[:3]


# We can also use + to concatenate lists, just like we did for strings.

# 


Chunk 391:
Document ID: 2328fb41-e805-43fc-a18f-41b0ebca5ffb
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 14, 'num_chars': 87}
Text: 


my_list + ['new item']


# Note: This doesn't actually change the original list!

# 


Chunk 392:
Document ID: b7f6aea3-47d1-4746-87bb-5569d0e0441f
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 15, 'num_chars': 84}
Text: 


my_list


# You would have to reassign the list to make the change permanent.

# 


Chunk 393:
Document ID: 12d922d8-1f14-44cd-9b77-96d72ebcd874
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 26, 'num_chars': 152}
Text: 


# Reassign
my_list = my_list + ['add new item permanently']
print(my_list)


# We can also use the * for a duplication method similar to strings:

# 


Chunk 394:
Document ID: 7dd76918-b3ab-4f3f-8d8d-ed1514e480fe
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 42}
Text: 


# Make the list double
my_list * 2


# 


Chunk 395:
Document ID: ace784eb-9f63-4d4a-a064-28c3c6bb99ef
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 92, 'num_chars': 512}
Text: 


# Again doubling not permanent
my_list


# # 1.3 Basic List Methods
# If you are familiar with another programming language, you might start to draw parallels between arrays in another language and lists in Python. Lists in Python however, tend to be more flexible than arrays in other languages for a two good reasons: they have no fixed size (meaning we don't have to specify how big a list will be), and they have no fixed type constraint (like we've seen above).
# 
# Let's go ahead and explore some more 


Chunk 396:
Document ID: ace784eb-9f63-4d4a-a064-28c3c6bb99ef
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 42, 'num_chars': 203}
Text: above).
# 
# Let's go ahead and explore some more special methods for lists:

# In[ ]:


# Create a new list
list1 = [1,2,3]


# Use the append method to permanently add an item to the end of a list:

# 


Chunk 397:
Document ID: ca98e607-5737-4a0f-b1c5-fc53ce135e5c
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 39, 'num_chars': 215}
Text: 


# Append
list1.append('append me!')
print(list1)


# Use pop to "pop off" an item from the list. By default pop takes off the last index, but you can also specify which index to pop off. Let's see an example:

# 


Chunk 398:
Document ID: e6c72961-603d-42af-8da2-f0b89109a754
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 10, 'num_chars': 62}
Text: 


# Pop off the 0 indexed item
list1.pop(0)
print(list1)


# 


Chunk 399:
Document ID: bc270300-2a26-41b7-9345-61e62ff675a5
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 40, 'num_chars': 249}
Text: 


# Assign the popped element, remember default popped index is -1
popped_item = list1.pop()
print(popped_item)
print(list1)


# It should also be noted that lists indexing will return an error if there is no element at that index. For example:

# 


Chunk 400:
Document ID: 5a8483d8-ead4-4c09-bf44-b89e0e34d460
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 18}
Text: 


list1[100]


# 


Chunk 401:
Document ID: 620148ef-3775-4001-8046-e945a74f5f27
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 15, 'num_chars': 128}
Text: 


new_list = ['a','e','x','b','c']

# Use reverse to reverse order (this is permanent!)
new_list.reverse()
print(new_list)


# 


Chunk 402:
Document ID: c4981fac-cb87-4c21-8027-dfe3222c1c9d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 65, 'num_chars': 368}
Text: 


# Use sort to sort the list (in this case alphabetical order, but for numbers it will go ascending)
new_list.sort()
print(new_list)


# # 1.4 Nesting Lists
# A great feature of of Python data structures is that they support nesting. This means we can have data structures within data structures. For example: A list inside a list.
# 
# Let's see how this works!

# 


Chunk 403:
Document ID: e95f37ba-c824-4abd-b3fb-16f2e20b2e81
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 54, 'num_chars': 319}
Text: 


# Let's make three lists
lst_1=[1,2,3]
lst_2=[4,5,6]
lst_3=[7,8,9]

# Make a list of lists to form a matrix
matrix = [lst_1,lst_2,lst_3]

print(matrix)


# We can again use indexing to grab elements, but now there are two levels for the index. The items in the matrix object, and then the items inside that list!

# 


Chunk 404:
Document ID: 343fa416-dce6-4f8f-a352-0bcefcc0bf13
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 52}
Text: 


# Grab first item in matrix object
matrix[0]


# 


Chunk 405:
Document ID: 908da858-39b6-4920-9a6a-9631a03b1a04
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 487}
Text: 


# Grab first item of the first item in the matrix object
matrix[0][0]


# # 1.5 List Comprehensions
# 
# Python has an advanced feature called list comprehensions. They allow for quick construction of lists. To fully understand list comprehensions we need to understand for loops. So don't worry if you don't completely understand this section, and feel free to just skip it since we will return to this topic later.
# 
# But in case you want to know now, here are a few examples!

# 


Chunk 406:
Document ID: 908da858-39b6-4920-9a6a-9631a03b1a04
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 6, 'num_chars': 25}
Text: e are a few examples!

# 


Chunk 407:
Document ID: c3d91834-19c2-49a4-9b54-a2f0a69b3496
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 93, 'num_chars': 512}
Text: 


# Build a list comprehension by deconstructing a for loop within a []
first_col = [row[0] for row in matrix]
print(first_col)


# For more advanced methods and features of lists in Python, check the **Python library**.

# # 2. Tuples
# In Python tuples are very similar to lists, however, unlike lists they are immutable meaning they can not be changed. You would use tuples to present things that shouldn't be changed, such as days of the week, or dates on a calendar.
# 
# In this section, we will get a bri


Chunk 408:
Document ID: c3d91834-19c2-49a4-9b54-a2f0a69b3496
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 96, 'num_chars': 512}
Text:  calendar.
# 
# In this section, we will get a brief overview of the following:
# 
# 
# 
# 1.   Constructing Tuples
# 2.   Basic Tuple Methods
# 3.   Immutability
# 4.   When to Use Tuples
# 
# 
# You'll have an intuition of how to use tuples based on what you've learned about lists. We can treat them very similarly with the major distinction being that tuples are immutable.

# # 2.1 Constructing Tuples
# 
# The construction of a tuples use () with elements separated by commas. For example:

# In[ ]:


# Cr


Chunk 409:
Document ID: c3d91834-19c2-49a4-9b54-a2f0a69b3496
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 16, 'num_chars': 79}
Text: separated by commas. For example:

# In[ ]:


# Create a tuple
t = (1,2,3)


# 


Chunk 410:
Document ID: 02b33b77-d06b-414b-bd10-4488b8bf046b
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 43}
Text: 


# Check len just like a list
len(t)


# 


Chunk 411:
Document ID: ee54309c-5b8e-4e93-9bf4-68dc6e760ede
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 11, 'num_chars': 58}
Text: 


# Can also mix object types
t = ('one',2)
print(t)


# 


Chunk 412:
Document ID: d9820022-40bd-4e05-88f6-8659aa95d7f2
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 11, 'num_chars': 53}
Text: 


# Use indexing just like we did in lists
t[0]


# 


Chunk 413:
Document ID: dccda1eb-053f-4c83-b167-c1377bebb36c
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 34, 'num_chars': 163}
Text: 


# Slicing just like a list
t[-1]


# # 2.2 Basic Tuple Methods
# 
# Tuples have built-in methods, but not as many as lists do. Let's look at two of them:
# 

# 


Chunk 414:
Document ID: f072ee80-5dcf-4877-ad07-485191daca0f
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 13, 'num_chars': 73}
Text: 


# Use .index to enter a value and return the index
t.index('one')


# 


Chunk 415:
Document ID: 1e241bb5-91fa-4ccb-aa5d-be6edede3101
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 34, 'num_chars': 188}
Text: 


# Use .count to count the number of times a value appears
t.count('one')


# # 2.3 Immutability
# 
# It can't be stressed enough that tuples are immutable. To drive that point home:

# 


Chunk 416:
Document ID: 0ec0d2b8-c65f-48cf-ad31-b5140daf63c9
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 3, 'num_chars': 22}
Text: 


t[0]= 'change'


# 


Chunk 417:
Document ID: 4a23c531-a8ab-452d-8b50-a54bd5bc3644
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 93, 'num_chars': 512}
Text: 


t.append('nope')


# # 2.4 When to use Tuples
# 
# You may be wondering, "Why bother using tuples when they have fewer available methods?" To be honest, tuples are not used as often as lists in programming, but are used when immutability is necessary. If in your program you are passing around an object and need to make sure it does not get changed, then a tuple becomes your solution. It provides a convenient source of data integrity.
# 
# You should now be able to create and use tuples in your programmin


Chunk 418:
Document ID: 4a23c531-a8ab-452d-8b50-a54bd5bc3644
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 89, 'num_chars': 512}
Text: e able to create and use tuples in your programming as well as have an understanding of their immutability.

# # 3. Dictionaries
# 
# **If you're familiar with other languages you can think of these Dictionaries as hash tables.**
# 
# This section will serve as a brief introduction to dictionaries and consist of:
# 
# 
# 
# 1.   Constructing a Dictionary
# 2.   Nesting Dictionaries
# 3.   Basic Dictionary Methods
# 4.   Advanced Dictionaries
# 
# So what are mappings? Mappings are a collection of objects th


Chunk 419:
Document ID: 4a23c531-a8ab-452d-8b50-a54bd5bc3644
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 94, 'num_chars': 512}
Text:  mappings? Mappings are a collection of objects that are stored by a key, unlike a sequence that stored objects by their relative position. This is an important distinction, since mappings won't retain order since they have objects defined by a key.
# 
# A Python dictionary consists of a key and then an associated value. That value can be almost any Python object.
# 
# 
# 
# 

# # 3.1 Constructing a Dictionary

# In[ ]:


# Make a dictionary with {} and : to signify a key and a value
my_dict = {'key1':'valu


Chunk 420:
Document ID: 4a23c531-a8ab-452d-8b50-a54bd5bc3644
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 10, 'num_chars': 75}
Text:  signify a key and a value
my_dict = {'key1':'value1','key2':'value2'}


# 


Chunk 421:
Document ID: 18915a94-709d-4cc3-9219-c269172ff3e1
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 33, 'num_chars': 244}
Text: 


# Call values by their key
my_dict['key2']


# Its important to note that dictionaries are very flexible in the data types they can hold. For example:

# In[ ]:


my_dict = {'key1':123,'key2':[12,23,33],'key3':['item0','item1','item2']}


# 


Chunk 422:
Document ID: 824c8087-cafe-475b-a39f-4c6d635ccfa5
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 62}
Text: 


# Let's call items from the dictionary
my_dict['key3']


# 


Chunk 423:
Document ID: c86fde1b-4d0a-4ef0-940e-d8becf0ca09f
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 10, 'num_chars': 60}
Text: 


# Can call an index on that value
my_dict['key3'][0]


# 


Chunk 424:
Document ID: 1921cee5-bec2-41ca-a884-600f0a9083a5
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 25, 'num_chars': 140}
Text: 


# Can then even call methods on that value
my_dict['key3'][0].upper()


# We can affect the values of a key as well. For instance:
# 

# 


Chunk 425:
Document ID: b01d24a1-1251-45cf-abe0-5e092d45d468
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 69, 'num_chars': 396}
Text: 


# Subtract 123 from the value
my_dict['key1'] = my_dict['key1'] - 123
print(my_dict['key1'])

# I have run this code for several times, every time it will subtract 123, that's why it is -984 now!


# A quick note, Python has a built-in method of doing a self subtraction or addition (or multiplication or division). We could have also used += or -= for the above statement. For example:
# 

# 


Chunk 426:
Document ID: 0e95205a-7035-418a-8eb8-750b6db2e8c4
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 72, 'num_chars': 386}
Text: 


# Set the object equal to itself minus 123 
my_dict['key1'] -= 123
my_dict['key1']


# We can also create keys by assignment. For instance if we started off with an empty dictionary, we could continually add to it:

# In[ ]:


# Create a new dictionary
d = {}


# In[ ]:


# Create a new key through assignment
d['animal'] = 'Dog'

# Can do this with any object
d['answer'] = 42


# 


Chunk 427:
Document ID: e5ef06e0-2b1c-4adb-8590-4cae2afb59ef
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 72, 'num_chars': 453}
Text: 


print(d)


# # 3.2 Nesting with Dictionaries
# 
# Hopefully you're starting to see how powerful Python is with its flexibility of nesting objects and calling methods on them. Let's see a dictionary nested inside a dictionary:

# In[ ]:


# Dictionary nested inside a dictionary nested inside a dictionary
d = {'key1':{'nestkey':{'subnestkey':'value'}}}


# That's a quite the inception of dictionaries! Let's see how we can grab that value:
# 
# 

# 


Chunk 428:
Document ID: c3ad6bc2-b267-4fd5-9d37-72ba3455194e
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 48, 'num_chars': 283}
Text: 


# Keep calling the keys
d['key1']['nestkey']['subnestkey']


# # 3.3 A few Dictionary Methods
# 
# There are a few methods we can call on a dictionary. Let's get a quick introduction to a few of them:

# In[ ]:


# Create a typical dictionary
d = {'key1':1,'key2':2,'key3':3}


# 


Chunk 429:
Document ID: 6d8772ea-3eda-4f4e-bb0b-d086d2de2469
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 11, 'num_chars': 55}
Text: 


# Method to return a list of all keys 
d.keys()


# 


Chunk 430:
Document ID: d7feeb44-9f37-4f91-a020-136a952ee60e
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 8, 'num_chars': 46}
Text: 


# Method to grab all values
d.values()


# 


Chunk 431:
Document ID: 9c1d248b-9494-473a-b313-23eda098c731
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 57, 'num_chars': 356}
Text: 


# Method to return tuples of all items  (we'll learn about tuples soon)
d.items()


# # 3.4 Advanced Dictionaries
# 

# **Dictionary Comprehensions:**
# 
# Just like List Comprehensions, Dictionary Data Types also support their own version of comprehension for quick creation. It is not as commonly used as List Comprehensions, but the syntax is:
# 

# 


Chunk 432:
Document ID: 2b2c10a4-12f2-4abc-85be-2eebff03063b
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 56, 'num_chars': 312}
Text: 


{x:x**2 for x in range(10)}


# One of the reasons it is not as common is the difficulty in structuring key names that are not based off the values.

# 
# **Iteration over keys, values, and items:** 
# 
# Dictionaries can be iterated over using the keys(), values() and items() methods. For example:
# 
# 

# 


Chunk 433:
Document ID: dadab1d3-b5a1-4d34-a484-4e145264b650
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 59}
Text: 


d = {'k1':1,'k2':2}
for k in d.keys():
    print(k)


# 


Chunk 434:
Document ID: d1bce625-6048-43ef-af88-c6c456c8328c
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 6, 'num_chars': 41}
Text: 


for v in d.values():
    print(v)


# 


Chunk 435:
Document ID: 51d9bad1-073b-4589-a0a8-fb3b8ca54d7c
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 45, 'num_chars': 277}
Text: 


for item in d.items():
    print(item)


# **Viewing keys, values and items:**
# 
# By themselves the keys(), values() and items() methods return a dictionary view object. This is not a separate list of items. Instead, the view is always tied to the original dictionary.

# 


Chunk 436:
Document ID: 2b4b8649-311c-4759-a00c-e2e00efc973d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 5, 'num_chars': 37}
Text: 


key_view = d.keys()

key_view


# 


Chunk 437:
Document ID: e8e5d810-1aa2-4f98-ad39-2f4f8796681d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 5, 'num_chars': 22}
Text: 


d['k3'] = 3

d


# 


Chunk 438:
Document ID: 5d76f12e-3857-40d7-96ab-04fff74fedb2
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 43, 'num_chars': 211}
Text: 


key_view


# # 4. Sets
# 
# Sets are an unordered collection of unique elements. We can construct them by using the set() function. Let's go ahead and make a set to see how it works

# In[ ]:


x = set()


# 


Chunk 439:
Document ID: 2a8e6bbc-1e18-4ac6-b373-e21cddae1e67
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 66, 'num_chars': 340}
Text: 


# We add to sets with the add() method
x.add(1)

print(x)


# **add:**
# 
# Note the curly brackets. This does not indicate a dictionary! Although you can draw analogies as a set being a dictionary with only keys.
# 
# We know that a set has only unique entries. So what happens when we try to add something that is already in a set?

# 


Chunk 440:
Document ID: 0195dc26-1230-42d9-928c-13a4cd4683c6
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 8, 'num_chars': 51}
Text: 


# Add a different element
x.add(2)
print(x)


# 


Chunk 441:
Document ID: bfba265f-26c3-4e87-9e30-c5ad104b618a
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 60, 'num_chars': 333}
Text: 


# Try to add the same element
x.add(1)
print(x)


# Notice how it won't place another 1 there. That's because a set is only concerned with unique elements! We can cast a list with multiple repeat elements to a set to get the unique elements. For example:

# In[ ]:


# Create a list with repeats
list1 = [1,1,2,2,3,4,5,6,1,1]


# 


Chunk 442:
Document ID: fe7a52ed-376b-4d77-9807-2168e29e0aec
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 18, 'num_chars': 101}
Text: 


# Cast as set to get unique values
set(list1)


# **clear:** removes all elements from the set

# 


Chunk 443:
Document ID: 4b3cc02d-3cd0-400c-b48d-0f7dc3f6578c
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 25, 'num_chars': 135}
Text: 


x.clear()
print(x)


# **copy:** returns a copy of the set. Note it is a copy, so changes to the original don't effect the copy.

# 


Chunk 444:
Document ID: 612ebc92-938c-456c-88c6-39ebcb6d42cf
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 31, 'num_chars': 197}
Text: 


s = {1,2,3}
sc = s.copy()
print(sc)

s.add(4)
print(s)


# **difference:** difference returns the difference of two or more sets. 
# 
# The syntax is: set1.difference(set2)
# 
# For example:

# 


Chunk 445:
Document ID: 2dcbacb7-a9c7-4317-81b8-e3ea63457c7e
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 23, 'num_chars': 183}
Text: 


s.difference(sc)


# 
# **difference_update:**
# 
# difference_update syntax is: set1.difference_update(set2)
# 
# the method returns set1 after removing elements found in set2

# 


Chunk 446:
Document ID: c35178f3-4e95-45c0-9f16-e64f520aaf8e
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 31, 'num_chars': 177}
Text: 


s1 = {1,2,3}
s2 = {1,4,5}
s1.difference_update(s2)
print(s1)


# **discard:** Removes an element from a set if it is a member. If the element is not a member, do nothing.

# 


Chunk 447:
Document ID: d2415983-9b35-4669-901c-b4f9993a1fe2
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 9}
Text: 


s


# 


Chunk 448:
Document ID: 9991981a-45ee-415f-8aec-cd6a82c7a938
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 28, 'num_chars': 184}
Text: 


s.discard(2)
print(s)


# **intersection and intersection_update:** Returns the intersection of two or more sets as a new set.(i.e. elements that are common to all of the sets.)

# 


Chunk 449:
Document ID: 51253070-5737-4971-a772-dc52a5f96f68
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 22, 'num_chars': 150}
Text: 


s1 = {1,2,3}
s2 = {1,2,4}
s1.intersection(s2)

print(s1)


# intersection_update will update a set with the intersection of itself and another.

# 


Chunk 450:
Document ID: 07ed65db-2174-4aaa-b986-973b054f9294
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 29, 'num_chars': 176}
Text: 


s1.intersection_update(s2)
print(s1)


# **isdisjoint:** This method will return True if two sets have a null intersection.

# In[ ]:


s1 = {1,2}
s2 = {1,2,4}
s3 = {5}


# 


Chunk 451:
Document ID: 828567fe-d5e9-4c9d-ad70-483eafb24643
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 25}
Text: 


s1.isdisjoint(s2)


# 


Chunk 452:
Document ID: 34325699-7496-4c3c-b43d-548205b5bcf3
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 13, 'num_chars': 101}
Text: 


s1.isdisjoint(s3)


# **issubset:** This method reports whether another set contains this set.

# 


Chunk 453:
Document ID: 503b56ca-d11f-4e8b-8e2d-2f8ee6b9cea1
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 10}
Text: 


s1


# 


Chunk 454:
Document ID: 9d37ec2d-c121-4d42-98d4-e919a37546e7
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 10}
Text: 


s2


# 


Chunk 455:
Document ID: f9ade1c6-53cc-4ab5-9785-d4819e3c835a
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 14, 'num_chars': 105}
Text: 


s1.issubset(s2)


# **issuperset:** This method will report whether this set contains another set.

# 


Chunk 456:
Document ID: 0427a407-b9d3-43a8-8392-5452aff1d4fb
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 25}
Text: 


s2.issuperset(s1)


# 


Chunk 457:
Document ID: 67e9ae3f-a7e6-4b72-955e-590b9038a652
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 27, 'num_chars': 189}
Text: 


s1.issuperset(s2)


# **symmetric_difference and symmetric_update:** Return the symmetric difference of two sets as a new set.(i.e. all elements that are in exactly one of the sets.)

# 


Chunk 458:
Document ID: f5972937-8da0-416f-a197-c01070adedfd
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 10}
Text: 


s1


# 


Chunk 459:
Document ID: 2eec34a7-436c-4420-9488-edb92c03501f
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 2, 'num_chars': 10}
Text: 


s2


# 


Chunk 460:
Document ID: 21e8ff28-6966-453f-8e15-81ccf17714e2
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 18, 'num_chars': 123}
Text: 


s1.symmetric_difference(s2)


# **union:** Returns the union of two sets (i.e. all elements that are in either set.)

# 


Chunk 461:
Document ID: e8427efa-ff03-4b4d-9f6a-32a2dbac638b
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 14, 'num_chars': 85}
Text: 


s1.union(s2)


# **update:** Update a set with the union of itself and others.

# 


Chunk 462:
Document ID: 693a5a26-c557-47ce-93eb-8ad0e6744c7d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 81, 'num_chars': 512}
Text: 


s1.update(s2)
print(s1)


# **This data structure is extremely useful and is underutilized by beginners, so try to keep it in mind!**

# # 5. Exercises 

# # Exercise 10.1: 
# Write a function called nested_sum that takes a nested list of integers and add up the elements from all of the nested lists.
# 

# In[ ]:


def nested_sum(nestedList):
        '''
        nestedList: list composed of nested lists containing int.
        Returns the sum of all the int in the nested list
        '''
        newList 


Chunk 463:
Document ID: 693a5a26-c557-47ce-93eb-8ad0e6744c7d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 37, 'num_chars': 512}
Text: nt in the nested list
        '''
        newList = []
        #Helper function to flatten the list
        def flatlist(nestedList):
                '''
                Returns a flat list
                '''
                for i in range(len(nestedList)):
                        if type(nestedList[i]) == int:
                                newList.append(nestedList[i])
                        else:
                                flatlist(nestedList[i])
                return newList

        flatlist(n


Chunk 464:
Document ID: 693a5a26-c557-47ce-93eb-8ad0e6744c7d
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 6, 'num_chars': 94}
Text:                 return newList

        flatlist(nestedList)
        print (sum(newList))


# 


Chunk 465:
Document ID: 2bd7c8a1-e683-4ef8-b036-a7b230c713ba
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 57, 'num_chars': 369}
Text: 


nestedList = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
nested_sum(nestedList)


# # Exercise 10.3:
# 
# Write a function that takes a list of numbers and returns the cumulative sum.
# 
# 
# 

# In[ ]:


def cumulative(list):
    cumulative_sum = 0
    new_list = []
    for i in l:
        cumulative_sum += i
        new_list.append(cumulative_sum)
    return new_list


# 


Chunk 466:
Document ID: 11c4c9f0-15d0-4fff-8623-358fe7fa6841
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 81, 'num_chars': 512}
Text: 


list = [1,2,3,4]

cumulative(list)


# # Exercise 11.1:
# 
# Write a function that reads the words in original_papers.txt and stores them as keys in a dictionary. It doesn’t matter what the values are. Then you can use the in operator as a fast way to check whether a string is in the dictionary.

# In[ ]:


fin = open('original_papers.txt')
englishdict = dict()


def create_diction():
    counter = 0
    dictionairy = dict()
    for line in fin:
        word = line.strip()
        dictionairy[word] = cou


Chunk 467:
Document ID: 11c4c9f0-15d0-4fff-8623-358fe7fa6841
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 12, 'num_chars': 103}
Text: ord = line.strip()
        dictionairy[word] = counter
        counter += 1
    return dictionairy


# 


Chunk 468:
Document ID: 07bbf630-d237-4c9a-b671-ecb2033c72e7
Metadata: {'file_name': 'Lesson_two_example2.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example2.ipynb', 'folder_name': 'Module 2', 'num_tokens': 1, 'num_chars': 21}
Text: 


create_diction()




Chunk 469:
Document ID: cf919bf5-d387-4dc3-8de6-764b9343bff6
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 90, 'num_chars': 512}
Text: 


import time
# time.time() return the time in seconds since the epoch as a floating point number

def chour(t):
	hour = t / 3600
	print("The number of hours has passed since epoch is %f" % hour)

def cminute(t):
	minute = t / 60
	print("The number of minutes has passed since epoch is %f" % minute)

def cseconds(t):
	seconds = t
	print("The number of seconds has passed since epoch is %f" % seconds)

def num_day():
	sec = time.time()
	num_day = sec / (60*60*24)
	print("The number of days has passed since ep


Chunk 470:
Document ID: cf919bf5-d387-4dc3-8de6-764b9343bff6
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 88, 'num_chars': 512}
Text: 24)
	print("The number of days has passed since epoch is %f" % num_day)

num_day()
chour(time.time())
cminute(time.time())
cseconds(time.time())


# **Exercise 5.2.** Fermat’s Last Theorem says that there are no positive integers a, b, and c such that
# 
# >$a^n+b^n=c^n$
# 
# 
# for any values of n greater than 2.
# 
# 1. Write a function named check_fermat that takes four parameters—a, b, c and n—and checks to see if Fermat’s theorem holds. If n is greater than 2 and
# 
# >$a^n+b^n=c^n$
# 
# the program sh


Chunk 471:
Document ID: cf919bf5-d387-4dc3-8de6-764b9343bff6
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 61, 'num_chars': 340}
Text: than 2 and
# 
# >$a^n+b^n=c^n$
# 
# the program should print, “Holy smokes, Fermat was wrong!” Otherwise the program should
# print, “No, that doesn’t work.”
# 
# 2. Write a function that prompts the user to input values for a, b, c and n, converts them to integers, and uses check_fermat to check whether they violate Fermat’s theorem.

# 


Chunk 472:
Document ID: b31ca324-0606-4ee4-bed0-033c2028728b
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 79, 'num_chars': 512}
Text: 


import sys
import os
import math


def check_fermat(a, b, c, n):
    '''This function does the actual checking of Fermat's last theorem.
    There should be no such integers such that
    a^n + b^n = c^n
    for n values greater than 2.
    '''
    left_side = pow(a, n) + pow(b, n)
    right_side = pow(c, n)
    if (n > 2) and (left_side == right_side):
        print ("Holy Smokes, Fermat was wrong!")
    else:
        print ("No, that doesn't work.")


def prompt_user():
    '''Prompt the user to input 


Chunk 473:
Document ID: b31ca324-0606-4ee4-bed0-033c2028728b
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 73, 'num_chars': 512}
Text: ef prompt_user():
    '''Prompt the user to input the required values
    for Fermat's last theorem
    Use a for loop to avoid repeated prompts
    '''
    parameters = []
    # create general prompt
    prompt = "Please enter "
    for value in ['a', 'b', 'c', 'n']:
        new_prompt = prompt + value + '\n'  # modify prompt for value in loop
        my_input = input(new_prompt)  # get input
        float_input = float(my_input)  # convert the entered value to float
        parameters.append(float_input)



Chunk 474:
Document ID: b31ca324-0606-4ee4-bed0-033c2028728b
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 79, 'num_chars': 512}
Text: e to float
        parameters.append(float_input)
    # call check_fermat function
    a,b,c,n = parameters[0],parameters[1],parameters[2],parameters[3]
    check_fermat(a, b, c, n)


def main():
    prompt_user()


if __name__ == '__main__':
    main()


# **Exercise 5.3.** If you are given three sticks, you may or may not be able to arrange them in a triangle.
# For example, if one of the sticks is 12 inches long and the other two are one inch long, you will not
# be able to get the short sticks to meet i


Chunk 475:
Document ID: b31ca324-0606-4ee4-bed0-033c2028728b
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 102, 'num_chars': 512}
Text: ll not
# be able to get the short sticks to meet in the middle. For any three lengths, there is a simple test to
# see if it is possible to form a triangle:
# 
# 
# If any of the three lengths is greater than the sum of the other two, then you cannot form a triangle. Otherwise, you can. (If the sum of two lengths equals the third, they form what is called a “degenerate” triangle.)
# 
# 
# 1. Write a function named is_triangle that takes three integers as arguments, and that prints either “Yes” or “No”, depe


Chunk 476:
Document ID: b31ca324-0606-4ee4-bed0-033c2028728b
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 60, 'num_chars': 335}
Text: uments, and that prints either “Yes” or “No”, depending on whether you can or cannot form a triangle from sticks with the given lengths.
# 
# 2. Write a function that prompts the user to input three stick lengths, converts them to integers,
# and uses is_triangle to check whether sticks with the given lengths can form a triangle.

# 


Chunk 477:
Document ID: d59d03f0-b2ed-41b4-b7ce-1e51120a3ac2
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 77, 'num_chars': 512}
Text: 


import sys
import os

def is_triangle(a,b,c):
    if (a + b > c) and (a + c > b) and (b + c > a):
        print ("Yes")
    else:
        print ("No")

def prompt_user():
    parameter_sides = []
    prompt = "Please enter side "
    for value in ['a', 'b', 'c']:
        new_prompt = prompt + value +': ' # modify prompt for value in loop
        my_input = input(new_prompt)   # get input
        float_input = float(my_input)      # convert the entered value to float
        parameter_sides.append(float_i


Chunk 478:
Document ID: d59d03f0-b2ed-41b4-b7ce-1e51120a3ac2
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 60, 'num_chars': 512}
Text: ue to float
        parameter_sides.append(float_input)
    print(parameter_sides)
    x,y,z = int(parameter_sides[0]),int(parameter_sides[1]),int(parameter_sides[2])
    is_triangle(x,y,z)

def results():
#     is_triangle(5,3,4)
    # is_triangle(5,3,12)
    prompt_user()

if __name__ == '__main__':
    results()


# **Exercise 5.4.** What is the output of the following program? Draw a stack diagram that shows the
# state of the program when it prints the result.
# 
# 
# 
# ```
# def recurse(n, s):
#   if


Chunk 479:
Document ID: d59d03f0-b2ed-41b4-b7ce-1e51120a3ac2
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 68, 'num_chars': 348}
Text: result.
# 
# 
# 
# ```
# def recurse(n, s):
#   if n == 0:
#      print(s)
#   else:
#     recurse(n-1, n+s)
# recurse(3, 0)
# ```
# 
# 
# 1. What would happen if you called this function like this: recurse(-1, 0)?
# 
# 2. Write a docstring that explains everything someone would need to know in order to use this
# function (and nothing else).

# 


Chunk 480:
Document ID: 01ca46b4-6b87-4b60-bd69-a6ec12dbf60d
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 14, 'num_chars': 97}
Text: 


def recurse(n, s):
  if n == 0:
     print(s)
  else:
    recurse(n-1, n+s)
recurse(3, 0)


# 


Chunk 481:
Document ID: cc1544eb-cda8-4b97-a753-207f16abed0e
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 512}
Text: 


recurse(-1, 0) #Runtime error: maxmimum recursion depth exceeded

'''
input n (n>0) since n need to approach 0
'''


# **Exercise 5.5.** Read the following function and see if you can figure out what it does (see the examples in Chapter 4). Then run it and see if you got it right.
# 
# 

# In[ ]:


def draw(t, length, n):
	if n == 0:
		return
	angle = 50 # initialize angle
	t.fd(length * n)
	t.lt(angle)
	draw(t, length, n-1)  #recursive call
	t.rt(2*angle)
	draw(t, length, n-1)
	t.lt(angle)
	t.bk(length*


Chunk 482:
Document ID: cc1544eb-cda8-4b97-a753-207f16abed0e
Metadata: {'file_name': 'Lesson_two_example3.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Lesson_two_example3.ipynb', 'folder_name': 'Module 2', 'num_tokens': 56, 'num_chars': 367}
Text: )
	draw(t, length, n-1)
	t.lt(angle)
	t.bk(length*n)

from swampy.TurtleWorld import *
import math

if __name__ == '__main__':
	world = TurtleWorld()    
	bob = Turtle()
	bob.delay = 0.001
	draw(bob, 3, 30)
	wait_for_user()

  # no display name and no $DISPLAY environment variable


# Google colab do not have GUI, so
# 
# **This code will be showed on my labtop.**



Chunk 483:
Document ID: 30e0d9ee-5398-46a3-bdfa-594434749e6b
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 12, 'num_chars': 103}
Text: 


thisdict = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}
print(thisdict["brand"])


# 


Chunk 484:
Document ID: c5eaa74b-7edd-4090-8f96-34c3d7c3a3b6
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 36, 'num_chars': 218}
Text: 


# Adding an item to the dictionary is done by using a new index key and assigning a value to it


thisdict = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}
thisdict["color"] = "red"
print(thisdict)


# 


Chunk 485:
Document ID: 639b75cb-d248-464a-9d3f-18b3e39c1481
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 25, 'num_chars': 181}
Text: 


# The pop() method removes the item with the specified key name

thisdict = {
  "brand": "Ford",
  "model": "Mustang",
  "year": 1964
}
thisdict.pop("model")
print(thisdict)


# 


Chunk 486:
Document ID: 89b89098-6dcd-4771-badb-e06fd145f770
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 19, 'num_chars': 191}
Text: 


# get keys, values, items

sample_dict = {'a': 100, 'b':200, 'c':300}

print(sample_dict.keys())
print(sample_dict.values())
print(sample_dict.items())


# # Dictionaries comprehension

# 


Chunk 487:
Document ID: 2eca875f-60b2-4294-84fa-ce6d999cf576
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 11, 'num_chars': 66}
Text: 


{x:x**2 for x in range(10)}


# # Nesting with Dictionaries

# 


Chunk 488:
Document ID: 67850e59-acdc-411b-9291-47d00cd47b03
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 9, 'num_chars': 112}
Text: 


d = {'key1':{'nestkey':{'subnestkey':'value'}}}

d['key1']['nestkey']['subnestkey']


# # Lambda function

# 


Chunk 489:
Document ID: c439093a-a9b0-49d5-96ec-86372c3003d5
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 22, 'num_chars': 114}
Text: 


# Lambda function with if but without else.
square = lambda x : x*x if (x > 0) else None

print(square(6))


# 


Chunk 490:
Document ID: a847a895-d2ff-4163-a734-3956d8b0deed
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 30, 'num_chars': 152}
Text: 


# Example of lambda function using if-else
max = lambda a, b : a if(a > b) else b

print(max(1, 2))
print(max(10, 2))


# # Factorial of a number

# 


Chunk 491:
Document ID: c4c91bda-69b7-48f7-b85a-2fa047b87514
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 73, 'num_chars': 458}
Text: 


# To find the factorial of a given number

def factorial(n):
    if n < 0:
        return 0
    elif n == 0 or n == 1:
        return 1
    else:
        fact = 1
        while(n > 1):
            fact *= n
            n -= 1
            #print(fact)
            #if you want to see the result after each step
        return fact

# Example of 5!
num = 5;
print("Factorial of", num,"is", factorial(num))

# This code is contributed by Dharmik Thakkar


# 


Chunk 492:
Document ID: f0bf4570-722f-4c80-aa97-51607a44bb3d
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 45, 'num_chars': 289}
Text: 


def factorial(n):
    if n < 0:
        return 0
    elif n == 0 or n == 1:
        return 1
    else:
        result = n * factorial(n - 1)
        #print(result)
        return result

# Example of 5!
num = 5;
print("Factorial of", num,"is", factorial(num))


# # Fibonacci number

# 


Chunk 493:
Document ID: 95eb2e8d-7e42-4b25-a13f-a96ef862b339
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 56, 'num_chars': 378}
Text: 


# Function for nth Fibonacci number
# Sequence: 0, 1, 1, 2, 3, 5, 8, 13

def Fibonacci(n):
    if n<= 0:
        print("Incorrect input")
    # First Fibonacci number is 0
    elif n == 1:
        return 0
    # Second Fibonacci number is 1
    elif n == 2:
        return 1
    else:
        return Fibonacci(n-1)+Fibonacci(n-2)


print(Fibonacci(10))


# # Prime number

# 


Chunk 494:
Document ID: 5d61e615-900b-420a-bc82-9d7a91347aeb
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 87, 'num_chars': 512}
Text: 


def is_prime(n):
    # A prime number is greater than 1 and divisible only by 1 and itself
    if n <= 1:
        return False
    # Check divisibility from 2 to the square root of n
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True

# Example:
number = 29
result = is_prime(number)
if result:
    print(f"{number} is a prime number.")
else:
    print(f"{number} is not a prime number.")


# # Practice Questions

# # Q1.) What is the output of the follow


Chunk 495:
Document ID: 5d61e615-900b-420a-bc82-9d7a91347aeb
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 94, 'num_chars': 512}
Text: estions

# # Q1.) What is the output of the following code??

# In[ ]:


print(9/2)


# In[ ]:


print(int(9/2))


# # Q2.) What will be the output of the following code snippet?

# In[ ]:


def func():
   global value
   value = "Local"

value = "Global"
func()
print(value)


# # Q3.) Short Circuiting
# 
# What is the output of the following programs?

# In[ ]:


l = [1, 2, 3]

exp = ((1 == 0) and (l.append(4)))
print(l)


# Tip: If the first condition is False, Python does not evaluate the second conditio


Chunk 496:
Document ID: 5d61e615-900b-420a-bc82-9d7a91347aeb
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 93, 'num_chars': 512}
Text: alse, Python does not evaluate the second condition, because the entire "and" expression will be False regardless of the second condition.

# In[ ]:


l = [1, 2, 3]

exp = ((1 != 0) or (l.append(4)))
print(l)


# Tip: If the first condition is True, Python does not evaluate the second condition, because the entire "or" expression will be True regardless of the second condition.

# # Q4.) What is the python statement that will result output 6?

# In[ ]:


A = [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]

'''



Chunk 497:
Document ID: 5d61e615-900b-420a-bc82-9d7a91347aeb
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 103, 'num_chars': 512}
Text:  [[1, 2, 3],
     [4, 5, 6],
     [7, 8, 9]]

'''
a) A[2][1]
b) A[1][2]
c) A[3][2]
d) A[2][3]

'''


# # Q5.)

# Which method is best to use when adding an item to the end of a list?
# 
# A. .insert()\
# B. .pop()\
# C. .append()\
# D. .remove()

# # Q6.) What will be the output of the following code snippet?

# In[ ]:


a = [1, 2, 3]
a = tuple(a)
a[0] = 2
print(a)


# # Q7.) How to get the unique elements from a list?
# sample = [1,1,1,1,2,2,3,3,3,3,4,5]

# In[ ]:


sample = [1,1,1,1,2,2,3,3,3,3,4,5]
set1 


Chunk 498:
Document ID: 5d61e615-900b-420a-bc82-9d7a91347aeb
Metadata: {'file_name': 'Week_2_Code_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 2/Week_2_Code_Demo.ipynb', 'folder_name': 'Module 2', 'num_tokens': 12, 'num_chars': 92}
Text:  In[ ]:


sample = [1,1,1,1,2,2,3,3,3,3,4,5]
set1 = set(sample)
unique = list(set1)
unique




Chunk 499:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 92, 'num_chars': 512}
Text: Lesson 3: Python Basic (2)
Haihua Chen, Ph.D.
2025/2/21
1
1
2
3
4
Python Dates
Python Arrays
Module and Package
Functions
2025/2/21
2
5
Python JSON
6
Python Try Except
7
Classes and Objects
8
Demo
9
In-class Exercise 1
1
Python Dates
2025/2/21
3
we can import a module named datetime to work with dates as date objects.
Python Dates
2025/2/21
4
Return the current local date and time.
Date Output
2025/2/21
5
The datetime module has many methods to return information about the date object.
More about Date
2025/


Chunk 500:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 85, 'num_chars': 512}
Text: ation about the date object.
More about Date
2025/2/21
6
https://docs.python.org/3/library/datetime.html#module-datetime
2
Python Arrays
2025/2/21
7
a = [1, 3.5, "Hello"]
Arrays
2025/2/21
8
b = [1.1, 3.5, 4.5]
We can treat lists as arrays.
we cannot constrain the type of elements stored in a list.
If you create arrays using the array module, all elements of the array must be of the same numeric type.
array of numeric values are supported in Python by the array module.
Arrays
2025/2/21
9
array of numeric val


Chunk 501:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 85, 'num_chars': 512}
Text: ay module.
Arrays
2025/2/21
9
array of numeric values are supported in Python by the Numpy library.
Arrays
2025/2/21
10
If you're going to perform arithmetic functions to your lists, you should really be using arrays instead
Differences between List and Arrays
2025/2/21
11
Arrays will store your data more compactly and efficiently, so if you're storing a large amount of data, you may consider using arrays as well
3
Functions
2025/2/21
12
A function is a named sequence of statements that belong together
Defi


Chunk 502:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 84, 'num_chars': 512}
Text: d sequence of statements that belong together
Definition of a function
Help us organize programs into chunks that match how we think about the problem
def <name>(<parameters>): <statements>
An example
A header line which begins with a keyword and ends with a colon.
A body consisting of one or more Python statements, each indented the same amount (4 spaces recommended ) from the header line.
Call the function with the right parameters
A further example
Functions can call other functions
Write a function whic


Chunk 503:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: ons can call other functions
Write a function which can draw a rectangle
Call the function with different arguments for width and height
We cannot repeat the same things 4 times, as the four sides are not equal
Functions can call other functions
Functions can call other functions.
draw_square(tess,50), animal and size are assigned the values of the tess object, and the int 50 respectively.
They are just like any other variable.
Draw_rectangle(animal,size,size), animal is assigned tess, the two sizes are ass


Chunk 504:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 85, 'num_chars': 512}
Text: e), animal is assigned tess, the two sizes are assigned the value of 50.
Functions that require arguments
The function abs require one argument
The function pow require two arguments
The function max could accept at least 2 arguments.
Functions can return values
Return the sum
Return the length
fruitful function
A function that returns a value is called a fruitful function
The opposite of a fruitful function is void function.
An example of fruitful function
Variables and parameters are local
a is local vari


Chunk 505:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: Variables and parameters are local
a is local variable
we can’t use outside the function of final_amount
p,r,n,t are all local variables
A further example
Calculate the distance between two points in two-dimensional space
Composition – A good way to reuse your code
You can call one function from within another, it is called composition
Calculate the area according to the coordinate axis of two points
Boolean functions
Functions can return Boolean values.
main functions
Use if __name__ == "__main__" to Contr


Chunk 506:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 75, 'num_chars': 512}
Text: n functions
Use if __name__ == "__main__" to Control the Execution of Your Code.
4
Module and Package
2025/2/21
27
Python Module
A module is a file containing Python definitions and statements.
2025/2/21
28
A module can define functions, classes and variables.
A module can also include runnable code.
Grouping related code into a module makes the code easier to understand and use.
https://www.w3schools.com/python/python_modules.asp
Python Package
Packages are namespaces which contain multiple packages and mo


Chunk 507:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 79, 'num_chars': 512}
Text:  namespaces which contain multiple packages and modules themselves.
2025/2/21
29
They are simply directories, but with a twist.
Each package in Python is a directory which MUST contain a special file called init.py. This file can be empty, and it indicates that the directory it contains is a Python package, so it can be imported the same way a module can be imported.
Grouping related code into a module makes the code easier to understand and use.
https://docs.python.org/3/tutorial/modules.html#package
Pytho


Chunk 508:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 71, 'num_chars': 512}
Text: s.python.org/3/tutorial/modules.html#package
Python Package Structure
2025/2/21
30
What is difference between a Python module and package?
A module is a single file (or files) that are imported under one import and used. E.g., import my_module
2025/2/21
31
A package is a collection of modules in directories that give a package hierarchy. E.g., from my_package.timing.danger.internets import function_of_love
Where to find Python packages?
2025/2/21
32
https://pypi.org/
How to install and use a python package?


Chunk 509:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 72, 'num_chars': 512}
Text: pypi.org/
How to install and use a python package?
2025/2/21
33
E.g., install textblob, a useful nlp package
https://pypi.org/
5
Python JSON
2025/2/21
34
Python JSON
2025/2/21
35
https://www.w3schools.com/js/js_json_intro.asp
JavaScript Object Notation (JSON) is an open-standard file format or data interchange format that uses human-readable text to transmit data objects consisting of attribute–value pairs.
Python XML
Extensible Markup Language (XML) is a markup language that defines a set of rules for enco


Chunk 510:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 72, 'num_chars': 512}
Text: rkup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.
2025/2/21
36
https://www.w3schools.com/xml/xml_whatis.asp
What is difference between a Python JSON and Dictionary?
JSON stands for JavaScript Object notation and is an open standard human readable data format.
2025/2/21
37
A python dictionary is a list that is indexed by key values.
https://www.w3schools.com/python/python_json.asp
It is apples vs. oranges comparison: JSON is a data 


Chunk 511:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text:  is apples vs. oranges comparison: JSON is a data format (a string), Python dictionary is a data structure (in-memory object).
If you need to exchange data between different (perhaps even non-Python) processes then you could use JSON format to serialize your Python dictionary.
6
Python Try Except
2025/2/21
38
The try block lets you test a block of code for errors.
Python Try Except
The except block lets you handle the error.
The finally block lets you execute code, regardless of the result of the try- and e


Chunk 512:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 89, 'num_chars': 512}
Text: e code, regardless of the result of the try- and except blocks.
Since the try block raises an error, the except block will be executed.
Why we need Try Except in our code
Without the try block, the program will crash and raise an error:
How to use Try Except
finally can be useful to close objects and clean up resources
The program can continue, without leaving the file object open
7
Classes and Objects
2025/2/21
42
Python Object Oriented Programming (OOP)
Python is a multi-paradigm programming language, it 


Chunk 513:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 80, 'num_chars': 512}
Text: thon is a multi-paradigm programming language, it supports different programming approach
One of the popular approach to solve a programming problem is by creating objects. This is known as Object-Oriented Programming (OOP).
The concept of OOP in Python focuses on creating reusable code.
An object has two characteristics: attributes and behavior
An example
What are classes and objects in Python?
Object is simply a collection of data (variables) and methods (functions) that act on those data. Class is a blue


Chunk 514:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 93, 'num_chars': 512}
Text: functions) that act on those data. Class is a blueprint for the object.
We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object.
As, many houses can be made from a description, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation.
An example
Principles of OOP
Defining a Class


Chunk 515:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text: ion.
An example
Principles of OOP
Defining a Class in Python
We define a class using the keyword class
Creating an Object in Python
This will create a new instance object named ob. We can access attributes of objects using the object name prefix.
Attributes may be data or method. Method of an object are corresponding functions of that class.
This means to say, since MyClass.func is a function object (attribute of class), ob.func will be a method object.
Methods and Constructors in a Class
Methods are functi


Chunk 516:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text: ods and Constructors in a Class
Methods are functions defined inside the body of a class. They are used to define the behaviors of an object.
Class functions that begin with double underscore (__) are called constructors, which get called whenever a new object of that class is instantiated
Inheritance
Inheritance is a way of creating a new class for using details of an existing class without modifying it. The newly formed class is a derived class (or child class). Similarly, the existing class is a base cla


Chunk 517:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 83, 'num_chars': 512}
Text: lass). Similarly, the existing class is a base class (or parent class).
Encapsulation
Using OOP in Python, we can restrict access to methods and variables. This prevents data from direct modification, which is called encapsulation. In Python, we denote private attribute using underscore as prefix i.e., single “ _ “ or double “ __“.
Polymorphism
Polymorphism is an ability (in OOP) to use common interface for multiple form (data types).
Suppose, we need to color a shape, there are multiple shape option (recta


Chunk 518:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 71, 'num_chars': 512}
Text: or a shape, there are multiple shape option (rectangle, square, circle). However, we could use same method to color any shape. This concept is called Polymorphism.
Advantages of Object Oriented Programming (OOP)
The programming gets easy and efficient.
The class is sharable, so codes can be reused.
The productivity of programmers increases.
Data is safe and secure with data abstraction.
Learn Python with ChatGPT
https://thedeveloperspace.com/learn-python-with-chatgpt/
Prepare Python for Interview
Thank you



Chunk 519:
Document ID: c6c28e99-0f6a-40e5-961a-1e9195bb44d4
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Lesson_three_Python_Basic.pptx', 'folder_name': 'Module 3', 'num_tokens': 9, 'num_chars': 62}
Text: h-chatgpt/
Prepare Python for Interview
Thank you
2025/2/21
57


Chunk 520:
Document ID: 73765dba-9719-4dfe-b31d-6483ea1bdd96
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 58, 'num_chars': 328}
Text: 


# Creatiing arrays

# importing "array" for array creations
import array as arr

# creating an array with integer type
# i represents the type of array i.e. integer array
a = arr.array('i', [1, 2, 3])

# printing original array
print("The new created array is : ", end=" ")
for i in range(0, 3):
    print(a[i], end=" ")


# 


Chunk 521:
Document ID: 851e6789-34e9-4fff-934b-7fb24ef072e6
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 86, 'num_chars': 504}
Text: 


# creating an array with double type
b = arr.array('d', [2.5, 3.2, 3.3])

# printing original array
print("\nThe new created array is : ", end=" ")
for i in range(0, 3):
    print(b[i], end=" ")


# In[ ]:


# inserting array using
# insert() function
a.insert(1, 4)

print("Array after insertion : ", end=" ")
for i in (a):
    print(i, end=" ")


# In[ ]:


# adding an element using append()
b.append(4.4)

print("Array after insertion : ", end=" ")
for i in (b):
    print(i, end=" ")
print()


# 


Chunk 522:
Document ID: 851e6789-34e9-4fff-934b-7fb24ef072e6
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 7, 'num_chars': 42}
Text: in (b):
    print(i, end=" ")
print()


# 


Chunk 523:
Document ID: c22a3615-91fc-4733-a01e-164810686393
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 57, 'num_chars': 347}
Text: 


# creating a list
mylist = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

a = arr.array('i', mylist)

# Print elements of a range using Slice operation
Sliced_array = a[3:8]
print("\nSlicing elements in a range 3-8: ")
print(Sliced_array)

Sliced_array = a[-2:]
print("\nElements sliced in reverse ")
print(Sliced_array)


# ## 1.2 Numpy arrays in python

# 


Chunk 524:
Document ID: e6db079b-c56d-426c-a9b7-3976df4d094b
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 80, 'num_chars': 512}
Text: 


# importing numpy module
import numpy as np


# In[ ]:


# creating list
list_1 = [1, 2, 3, 4]
list_2 = [5, 6, 7, 8]
list_3 = [9, 10, 11, 12]

# creating numpy array
sample_array = np.array([list_1,
                         list_2,
                         list_3])

print("Numpy multi dimensional array in python\n", sample_array)
# print shape of the array
print("Shape of the array :", sample_array.shape)


# In[ ]:


# python program to create
# Empty and Full Numpy arrays

import numpy as np


# Create


Chunk 525:
Document ID: e6db079b-c56d-426c-a9b7-3976df4d094b
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 86, 'num_chars': 512}
Text: d Full Numpy arrays

import numpy as np


# Create an empty array
empa = np.zeros([3, 4], dtype = float)
print("Array with zeros")
print(empa)

ones = np.ones([3, 4], dtype = int)
print("Array with ones")
print(ones)


# Create a full array
flla = np.full([3, 3], 55, dtype=float)
print("\n Full Array")
print(flla)


# In[ ]:


import numpy as np

np.arange(1, 20 , 2,dtype = np.int32)


# ## 1.3 Comparison of arrays

# In[ ]:


a = np.array([101, 99, 87])
b = np.array([897, 97, 111])

print("a > b")
print(np


Chunk 526:
Document ID: e6db079b-c56d-426c-a9b7-3976df4d094b
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 59, 'num_chars': 400}
Text:  np.array([897, 97, 111])

print("a > b")
print(np.greater(a, b))
print()
print("a >= b")
print(np.greater_equal(a, b))
print()
print("a < b")
print(np.less(a, b))
print()
print("a <= b")
print(np.less_equal(a, b))


# numpy.arange(): This is an inbuilt NumPy function that returns evenly spaced values within a given interval.
# 
# 

# ## 1.4 Check whether the element is present in array or not

# 


Chunk 527:
Document ID: 178f14bd-cd79-4059-b727-51b62bdb852b
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 40, 'num_chars': 263}
Text: 


n_array = np.array([[2, 3, 0],
                    [4, 1, 6]])

# Checking whether specific values are present in "n_array" or not
print(2 in n_array)
print(6 in n_array)
print(50 in n_array)


# ## 1.5 Flatten an array/Reducing the dimensions of the array

# 


Chunk 528:
Document ID: b98f8f02-9bad-4947-9bba-494437e33a08
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 75, 'num_chars': 420}
Text: 


# declare matrix with np
a1 = np.array([[6, 9], [8, 5], [18, 21]])

# using array.flatten() method
print(a1.flatten())


# # 2 JSON library

# It is a format for structuring data. It is mainly used for storing and
# transferring data between the browser and the server.

# ## 2.1 Convert from JSON to Python:

# json.loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary.

# 


Chunk 529:
Document ID: 41f484d1-00eb-492c-9519-26d6cee4d92d
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 22, 'num_chars': 154}
Text: 


import json

# JSON:
x =  '{ "name":"John", "age":30, "city":"New York"}'

# Converting to a Python dictionary:
y = json.loads(x)

print(y["age"])


# 


Chunk 530:
Document ID: 5a7c8c2f-cc6a-489a-aefa-58ab921f8fe8
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: 


# JSON string
employee = '{"id":"09", "name": "Sam", "department":"Student"}'
print("This is JSON", type(employee))

print("\nNow convert from JSON to Python")

# Convert string to Python dict
employee_dict = json.loads(employee)
print("\nConverted to Python", type(employee_dict))
print(employee_dict)


# ## 2.2 Convert from Python to JSON

# | Python  | JSON   |
# |---------|--------|
# | dict    | Object |
# | list    | Array  |
# | tuple   | Array  |
# | str     | String |
# | int     | Number |
# | f


Chunk 531:
Document ID: 5a7c8c2f-cc6a-489a-aefa-58ab921f8fe8
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 83, 'num_chars': 428}
Text:  | str     | String |
# | int     | Number |
# | float   | Number |
# | True    | true   |
# | False   | false  |
# | None    | null   |

# json.dumps() function will convert a subset of Python objects into a JSON string.

# Indentation: Each level in the nested structure of the JSON is indented by 4 spaces.
# 
# Readability: The JSON string becomes more human-readable, as the hierarchy and structure are clearly visible.

# 


Chunk 532:
Document ID: 925be010-4d91-4ccf-9eb8-28d187bebd43
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 34, 'num_chars': 273}
Text: 


import json

x = {
  "name": "John",
  "age": 30,
  "married": True,
  "divorced": False,
  "children": ("Ann","Billy"),
  "pets": None,
  "cars": [
    {"model": "BMW 230", "mpg": 27.5},
    {"model": "Ford Edge", "mpg": 24.1}
  ]
}

print(json.dumps(x, indent=3))


# 


Chunk 533:
Document ID: e3a1ac00-e7d9-4046-a84b-f67bd330872e
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 75, 'num_chars': 504}
Text: 


# JSON string
employee_dict = {'id': '09', 'name': 'Nitin', 'department': 'Finance'}
print("This is Python", type(employee_dict))

print("\nNow Convert from Python to JSON\n")

# Convert Python dict to JSON
json_object = json.dumps(employee_dict, indent=4)
print("Converted to JSON", type(json_object))
print(json_object)


# CSV to JSON
# JSON to CSV

# # 3 Try, except and finally

# ## 3.1 Basics

# try:
#     # code that may cause exception
# except:
#     # code to run when exception occurs

# 


Chunk 534:
Document ID: e3a1ac00-e7d9-4046-a84b-f67bd330872e
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 8, 'num_chars': 42}
Text:    # code to run when exception occurs

# 


Chunk 535:
Document ID: e39d5f5f-ab7d-40c0-ac50-cb79ab4c779c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 20, 'num_chars': 141}
Text: 


# The try block will generate an exception, because x is not defined:

try:
  print(myerror)
except:
  print("An exception occurred")


# 


Chunk 536:
Document ID: 51321b92-fa3c-4032-93c7-a89d76fe2502
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 93, 'num_chars': 512}
Text: 


x = "Information science"

try:
    print(x + 10)
except:
    print("Something went wrong")

finally:
    print("The 'try except' is finished")


# ## 3.2 The meaning of using try except

# ### 3.2.1 No exception handling.

# In[ ]:


# write your answer here
# Get user input for parameters a and b
a = float(input("Enter the value of 'a': "))
b = float(input("Enter the value of 'b': "))

# Calculate S using the formula
S = 3.14 * (1 + a / b)**3

# Display the result
print("The result of S is:", S)


#  #


Chunk 537:
Document ID: 51321b92-fa3c-4032-93c7-a89d76fe2502
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 16, 'num_chars': 92}
Text:  the result
print("The result of S is:", S)


#  ### 3.2.2 Exception Handling Using try.

# 


Chunk 538:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text: 


try:
    # Step 1: Get user input for parameters 'a' and 'b'
    a = float(input("Enter the value of 'a': "))
    b = float(input("Enter the value of 'b': "))

    # Check if 'b' is not zero to avoid division by zero
    if b == 0:
        raise ZeroDivisionError("Division by zero is not allowed.")

    # Step 2: Calculate S using the formula
    S = 3.14 * ((1 + a / b) ** 3)

    # Step 3: Display the result
    print(f"The result of S is: {S}")

except ValueError:
    print("Invalid input. Please enter


Chunk 539:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 69, 'num_chars': 512}
Text: ValueError:
    print("Invalid input. Please enter valid numeric values for 'a' and 'b.")
except ZeroDivisionError as e:
    print(e)
except Exception as e:
    print(f"An error occurred: {e}")


# ## 3.3 Catching Specific Exceptions in Python

# In[ ]:


try:

    even_numbers = [2,4,6,8]
    print(even_numbers[5])

except ZeroDivisionError:
    print("Denominator cannot be 0.")

except IndexError:
    print("Index Out of Bound.")

# Output: Index Out of Bound


# ## 3.4 Python try with else clause

# In[ 


Chunk 540:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 89, 'num_chars': 512}
Text: und


# ## 3.4 Python try with else clause

# In[ ]:


# program to print the reciprocal of even numbers

try:
    num = int(input("Enter a number: "))
    assert num % 2 == 0
except:
    print("Not an even number!")
else:
    reciprocal = 1/num
    print(reciprocal)


# # 4 Modules

# A module is a file containing Python code, including functions, classes, and variables, that you can import into other Python programs.

# ## 4.1 date

# Python has a module named datetime to work with dates and times.

# Get


Chunk 541:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 76, 'num_chars': 512}
Text: amed datetime to work with dates and times.

# Get Current Date and Time

# In[ ]:


import datetime

x = datetime.datetime.now()
print(x)


# Get Current Date

# In[ ]:


# get current date
current_date = datetime.date.today()

print(current_date)


# Date object to represent a date
# 

# In[ ]:


import datetime

d = datetime.date(2022, 12, 25)
print(d)


# In[ ]:


from datetime import datetime

given_date = datetime(2020, 2, 25)
print("Given date is", given_date)
print(given_date.strftime('%a, %d-%b, %Y


Chunk 542:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 78, 'num_chars': 512}
Text: ven_date)
print(given_date.strftime('%a, %d-%b, %Y'))


#  Print today's year, month and day

# In[ ]:


from datetime import date

# date object of today's date
today = date.today()

print("Current year:", today.year)
print("Current month:", today.month)
print("Current day:", today.day)


#  Time object to represent time

# In[ ]:


from datetime import time

# time(hour = 0, minute = 0, second = 0)
a = time()
print(a)

# time(hour, minute and second)
b = time(11, 34, 56)
print(b)

# time(hour, minute and 


Chunk 543:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: ime(11, 34, 56)
print(b)

# time(hour, minute and second)
c = time(hour = 11, minute = 34, second = 56)
print(c)

# time(hour, minute, second, microsecond)
d = time(11, 34, 56, 234566)
print(d)


#  Python datetime object
# 

# In[ ]:


from datetime import datetime

# datetime(year, month, day)
a = datetime(2022, 12, 28)
print(a)

# datetime(year, month, day, hour, minute, second, microsecond)
b = datetime(2022, 12, 28, 23, 55, 59, 342380)
print(b)


# Python datetime.timedelta Class

# In[ ]:


from datet


Chunk 544:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 109, 'num_chars': 512}
Text: on datetime.timedelta Class

# In[ ]:


from datetime import datetime, date

# using date()
t1 = date(year = 2023, month = 9, day = 6)
t2 = date(year = 2023, month = 9, day = 1)

t3 = t1 - t2

print("t3 =", t3)

# using datetime()
t4 = datetime(year = 2023, month = 9, day = 12, hour = 7, minute = 9, second = 33)
t5 = datetime(year = 2023, month = 8, day = 30, hour = 5, minute = 55, second = 13)
t6 = t4 - t5
print("t6 =", t6)

print("Type of t3 =", type(t3))
print("Type of t6 =", type(t6))


# In[ ]:


from 


Chunk 545:
Document ID: 8e2d37fb-0cfb-49dc-aff6-d9d9b5c0918f
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 48, 'num_chars': 315}
Text: print("Type of t6 =", type(t6))


# In[ ]:


from datetime import date, timedelta
dt = date.today() - timedelta(5)
print('Current Date :',date.today())
print('5 days before Current Date :',dt)


# ## 4.2 math

# The math module provides mathematical functions and constants, such as sin(), cos(), log(), and pi.

# 


Chunk 546:
Document ID: c5953449-6d09-4a8b-8c9d-760491ff91c2
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 36, 'num_chars': 219}
Text: 


import math

# sqrt computes the square root
square_root = math.sqrt(4)

print("Square Root of 4 is",square_root)

# pow() comptes the power
power = pow(2, 3)

print("2 to the power 3 is",power)


# ## 4.3 random

# 


Chunk 547:
Document ID: 01f473d1-fefb-4df3-b56e-da700b0e7809
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 40, 'num_chars': 257}
Text: 


import random

# Generate a random integer between 1 and 10
rand_int = random.randint(1, 10)
print("Random Integer:", rand_int)


# ## 4.4 re

# The re module provides support for regular expressions, which are useful for pattern matching in strings.

# 


Chunk 548:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 43, 'num_chars': 512}
Text: 


import re

# Search for a pattern in a string
pattern = r'\bword\b'
text = "A word in a sentence."
match = re.search(pattern, text)
if match:
    print("Pattern found:", match.group())
else:
    print("Pattern not found.")


# # 5 Classes and objects

# ![Screenshot 2023-09-06 203416.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgMAAAF+CAYAAAAfjxnVAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAKzvSURBVHhe7d0JuF1FmS7+goTMAwQCgRCGEOYxYZAwSBAZRAiigMjggNja0tdWbnub6/0/F7lPS+PT7dDdt


Chunk 549:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: GEOYxYZAwSBAZRAiigMjggNja0tdWbnub6/0/F7lPS+PT7dDdtvZVrkMjagdkCAqEMSCTTEGGMCVMYQphCAESMnH+9at9KmwOJyEhAznZ3wuVvc/atWrVqrVWve/31Ve11nrrrbfaUiAQCAQCgZbF2u2fgUAgEAgEWhQhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFocIQYCgUAgEGhxhBgIBAKBQKDFEWIgEAgEAoEWR4iBQCAQCARaHCEGAoFAIBBocYQYCAQCgUCgxRFiIBAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFoca7311ltt7d9XGNZaa632b4FAIBAIBN4v2tpWOEV3ivAMBAKBQCDQ4ggxEAgEAoFAiyPEQCAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAIND


Chunk 550:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OEIMBAKBQCDQ4ggxEAgEAoFAiyPEQCAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OEIMBAKBQCDQ4ljrrbfeamv/vsKw1lprtX8LBAKrO3IfkObMmZNeffXV9MYbb6S2tnd3Cb169Ur9+vUrqUePHu1bG/vOnTu37Pvaa68t2rdnz55pgw02SH369Fmq/mDBggXp5ZdfTq+//nr5e8CAAal///5pnXXWSWuvvWpsFucyf/78NG/evHJc57mqjh0ILA6dPY8rAyEGAoEWhU4G+c2aNSs99thj6Y477khTp04tBNi9e/f2XKmQ/cYbb5y22267tP3226dNN920kL189


Chunk 551:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 77khTp04tBNi9e/f2XKmQ/cYbb5y22267tP3226dNN920kL189n/mmWfSbbfdliZNmlTIfOHChWnYsGHpyCOPLPs0i4fOoB7qMGHChFKG/mOPPfZIe+21Vxo8eHA51sqGOhBE06dPT88//3waMmRISb17927PEQh8MFhVYqDbmWee+e327ysMIQYCgdUbb+UOZu6bb6ann366iABEfMUVV6S77rqrWMgInZdg5syZ6d57702PPvpoIX1kTwQMHDiwWM/ysujlufDCC9PEiRPTrbfeWraNGjUqbbHFFiXfkqCzc5xx48alCy64oJTFKzB8+PC03nrrvaeYWBEgambMmJHuvvvudMedd5bjEyI8IoFAKyB8YIFAiwH5LsjkxwK+6qqr0ve///30L//yL2nqY4+lD33oQ+mss85KP//5z9Mvf/nL9NOf/jR96Ut/VchR3u9+97vpxz/+cXr44YeLJY2ot9pqq3T88cenPffcswwNNHsVlhYEhqGBjTbaqJC


Chunk 552:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Xr44YeLJY2ot9pqq3T88cenPffcswwNNHsVlhYEhqGBjTbaqJCwcg0dEBsrGwvzMV6dNSs98sgj6dprr01/uvHGNOOFF9p/DQRaAyEGAoEWA6uf5X7dddel3/zmN8USX3fdddPI3XZL/+N//I+04447pm7dupW8LOOjj/5EOuKII9IOO+yQ3nzzzXTNNdcUQWBIYUWAEFh//fXTV77y1+m8884rdfra176Wdt5llxKjsLLx6syZ6Y7bby/H5ZkwZBEItBpimCAQaDFwyRsOuOyyy8oYvZiAbbfdtozxH3jggYWA6zPs05i94QFu9Oeeey69kK1m3oVBgwYVTwBXPi8BcSH2AJkSF8p84IEH0kUXXZTGjx9fhiNeeumltNlmmxXL30io/YiKX/ziF+nyyy9Pf/rTn9LkyZOL6Og/YEDq26fPouGI2bPnpEcffaQIhvPPP7/kv/rqq4u4IVqkZq+E+t58883pt7/9bfqv//qvkl969tlni/gQ3DhlypRSh


Chunk 553:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 4IVqkZq+E+t58883pt7/9bfqv//qvkl969tlni/gQ3DhlypRShvrJ6/y0h8977rmnDI0Y7hAgeemll6Zf//rXZUjFsInyX3zxxXK+6sB7Egh0VYQYCARaCFzvyPCGG24oJIj0kD2rn/XP5d9xjN/zbKaAvE8++WTZH/khXsRun45iANkTCfJNmzYtPfjggyUhT0TsN8F56oNob7nllhJrcOedd5b8ffv2TVtusUXJy3PwyiuvFJFw5ZVXlmBF8Qy8F8hcmepiH0l+dbzxxhsLcf/lL38p56EMMQHOQZ6hQ4cWDwnBItmuXGX4bcMNNyxBhLvsumupGzFgKEF7aaOnnnqqHEe8gfMhjAKBrooYJggEWgimzSHAJ554oli9LHBEyhuA/JBkZ0CQxvQReA0afOihhwpxI/SOqLMUtt5667TffvulbbbZJlv2s9Of//zn9Ic//CE9/vjjJQ/RgHBHjhxZPA1EhbKRsnJ5IOzHEufJQMiI+JBDDknHHHNMma2


Chunk 554:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Q/RgHBHjhxZPA1EhbKRsnJ5IOzHEufJQMiI+JBDDknHHHNMma2A7BE/sWBfwoXHQ37HU+ejjz46ffzjHy+k/Zd7/pL++Mc/FnHjvMyOMFuCZU/4iFngIfnMZz6TPvrRj5b4CkKHYNAG+++/f/EWEDriLoiRztogEOhKCDEQCLQQiAFkiagJAcROAHCxI7rFiQHWMLd6neaH/JRhyKGuDdAMxIrgBSSaIrjzzjsXK58AQdACEHkEWPcIWeAgQVJjFSqIAfnENfz+978vIgZ5K2/zzTcvJE44ECbKZKnL4xj33XdfqRvvxUc+8pF06KGHphEjRqS0VmOoRFsQQLwA6ubciAH1sQ8BY4qkc3V8AkZ5BIyZDrvvvnuZakk8LK7dAoGugriDA4EWAvJHbs2WLAJEwkhucUN8vAdSM+khata91BGEg2mFyBbRcqETG8bjWdIsaqLkvUCwiFFA8NWNbyz/N785P/3sZz8tMQbG7nko/F6HMgxJGBZQXyKDR4AHg


Chunk 555:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: A8NWNbyz/N785P/3sZz8tMQbG7nko/F6HMgxJGBZQXyKDR4AHgofg9NNPT5///OcL4asTIbSk6Y/KIBB8Gia45JJLSuyBNtttt91KiiGCQFdHiIFAoIWA8BEb13kl/jLVMIsDRO17Z+ASl8cnIEJlNHsLmqFsBEtAOCbC9VmPZTiAZf5eUCekLvCQ6LC/bXPmvJnr0D1tueWW6aCDDkpjxowpQwbqZKjA4kGOAUhcfbvn+owePbpMgzRkQKioY7PAeRcIpXwOhg3MslCm+IaLL75oUYyE89EOgUBXRoiBQKCFgLhF+hufR5yIkLcAMXOBL04MNAi4QeCI3r51TQBEvyTI7zjNpOv74rwQzSAAxAxIYL8aY3DwwQenT37yk+lzn/tcOuGEE9IBBxxQ6qSu8tu3ihhwNPUlIAw1NAuixcGvBA0xYKhBoKU2JAKsSUAQ3H///WUoIxDoyggxEAi0EFjCVg9EhkgVsSFMQoA1XS3/juCeR3gEAUJeb931ypi


Chunk 556:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: ggxEAi0EFjCVg9EhkgVsSFMQoA1XS3/juCeR3gEAUJeb931ypi6clj8SwKBUROw0hfnUeiIjvuunfclZHbaaadCzgL8JJ4BcQR10SPnaR/DDOpOxNQylhVrZ8HgPAmCT3ziE/lzTG67jUt8glkGkniCQKArI8RAINBiQMQC6bi9CQOWs3F3AXidDRXwHHDTG+cnCJDtRkM2KhH1gvgWh0riBAYydhyWOBGAtJdmXr5j8TxI9lWWJZQ7Chd1lOQXH2AIgEjhIRCjIH/1EKhTzV/PtaOHoNZdHisUInvBhGIOTjrp5HTUUUeVYQnlC140xTEQ6MoIMRAItBgEze26667F0mXds9S5va0saAEgBFflADJEpgLykJ7xe0Ruf9H0IvE7g/2qAOB1sJ9P8Qr2QayGKyqQuHp0JGXCBbFvsskmJRDQND9TCLnmCRTlGw4wO6D+rXxDAY5F3BAP119//aKhBvUgbOqMAqTP2+H4FYSGAEfxB455+RVXlFgBXgbn/


Chunk 557:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: P119//aKhBvUgbOqMAqTP2+H4FYSGAEfxB455+RVXlFgBXgbn/tWvfjUddthhxbviXKvQCAS6KmLRoUCgxYD4WNqi/H02hgieL0Q3a9ZrxQ3fI1vvczJ5InGLExkbZ/0iWO75L3zhC2VanXF3ZGr4QB6uc8Q4aL1BZTYBwjcTwIqHrGtj+lztphuy4MH+SNeCQMSI/oPVvcsuu5Spgyx85ZtVgPDV1zZeDQJBHa01YNoirwOhIY/61lkLL730YnkTonMnJm6//fZC4rwTylZvgsinuAIiichwTLMOLFWsbkSC4zoOgWR6JfFBGNknEOiqCDEQCLQYPJ+s4P79B2Ti26DdZT+gWMksbtPzKmGyhgXKIdcRI7ZOBx/80XT44YcXMicMoIoBbyxElILsNhu2WZr+wvRSnjn/pv45zsc+9rEyxo9Aa6wBUkbojlfd7VUMsLwRPtHRJx9PvleycGDVI2oE7Rg8AIQG4uZJ4FFQLvHw5JNPlP3UkWiwcqDfiRW


Chunk 558:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: leycGDVI2oE7Rg8AIQG4uZJ4FFQLvHw5JNPlP3UkWiwcqDfiRWCSNliC5w34WI/kMcwiLpqB/WVz/oFVkG0EBLhpC28pKmKm0CgKyLEQCDQgqiCAJlZTnjE1lsXUkSyEosaOSJcEB9w5JFHlCl5gve49ZvBG8AdL59gvuFbDV9kbRMS3Py2Hzl2bNoiE2zzDATkayiCaECwUMUAgme9ExnKWKcphqDuh+TN9UfIAv2cG+FBSAgkXLBgYQn4IwicE29DnYpIaCi/ChPChjeDENhhxx3LaoO9crvwmvCEKM9KhNYbsJ9j7rvvvmWlxUCgK2Ot/EC9vxDbJSDEQCDQdaAD4P6e3z7GjxA7AvnXxNXeEfZhnXv2/e7vOrUPbEOk62TSFZ3vd8eSh0eCRf6jH/2oLOajjLFZNJx00klFeBAC8FbeZ14+hv2q9Q617Bp3UOHYynaMZsiH/M1MUBdQnrwSNMrskfOtU+pTz61xXo6/sORRlmM3HzcQWJHo7HlcG


Chunk 559:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: MUBdQnrwSNMrskfOtU+pTz61xXo6/sORRlmM3HzcQWJHo7HlcGQgxEAgEGgGDudNZXMdTn+klPdsd9+34t33r/qx5LyUytMCjII7A2w29iAg+/elPl7UDjMOz3iuU2Vkdm8tuRmf5a76O+TvmbS6zefvi8gQCKwMd79+VhZhNEAgEyuI6SI2121laGtKreWrqrIwK1r1hBEsLc7uzvEEeVrsgPq7+jmsRdFZux7Kbsbh6dJa/Y97mPHWfJeUJBLoyQgwEAoFVDiRqXB7ZWwNAvIB3CohbMJbPI2B4gBs+EAisfIQYCAQCqxzG2TffYosScMgLICDPeL0YAbMNzEggDMLyDgRWDSJmIBAIrHII2DPLgDdA7EANXBTpzytgrYAQA4HAqosZCDEQCAQ+EBAERIC5+xWi8susg5yiHwkEQgwEAoFAINDyiNkEgUAgEAgEVglCDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAIND


Chunk 560:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OGIFwkAgsFLwVltbWjB/fnkBkVcUW3Z4zpw55dPfts/Pv9f3EsjvU+/h9cDeWGhZYq809p6Cvn37luS7tx12t2Rx41CBwBqLWI44EAh0CXjHwMKFCwvBI/vZs2eXT+m1115Ls2bNKp9eTCT5nSCQqiCwv3J0RvqPtXMiBJB+FQL9+/cvrzVeb731GmnQoDQof9rWI+frlgVEILCmIcRAIBBY7aBjWrgQ+S8oRC4hdST/yiuvpJdeeim98MIL6cUXX1z098yZM8vvxALSl5RTExFQy67QhxRRkAney4tq6tFjnTRgwMDy2uOhQ4emESNGlLccbrTRRkUs8CLIFwisKWh+LlYmQgwEAoElYhFp58SKn/3G7PTqqzML6UvTp08v6fnnny8i4OWXXy5C4NVXX01vvPFG2UefwMpH2P369


Chunk 561:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: L6UvTp08v6fnnny8i4OWXXy5C4NVXX01vvPFG2UefwMpH2P369cuEPqB89unTpxC4IQHEL18VCPYjIJTBu0BUzJr1ahEgPXr0TOuvv37abLPN0siRI9Nuu+1WRIFXIPMiVDERCHR1hBgIBAIfKHRCrHhufWSM5JE90n/22WfT008/XdJzzz2Xt8/IAmBmIW/u/Ur4yH/dddctbv1BgwYVAve3xL0vX69evYoYYNE3i4E67GCIoeFleDHNmPFiOTbxweugThtssEHacccdiyjYZZddireA58DwQngJAl0dIQYCgcAqRyVhAoBrn3WP/KdNm5aefPLJ9NRTT6Vnnnm6kLI8rHedlWeecGC9I+Kdd945bbfddmnYsGFpww03zEKgIQL69++3KCjQZxUB1StQUQWBMh2DN2DOm2+WY8544YVSj4ceeij95S9/KcJAECKhse2226ZRo0alnXbaqXgNbAtREOjKCDEQCARWOirpzp8vBqBhiXP9s/gRrkQI1CG


Chunk 562:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: XbaqXgNbAtREOjKCDEQCARWOirpzp8vBqBhiXP9s/gRrkQI1CGA6vpH0kiWtY/sJYR75513lrH8j3/842nPPfdcNJZvOKBnz15ZAHRvP/Kyo9ZVHXkE1PHBBx9Md911V7rvvvtKvQkMx99hhx3SrrvuWj6JAkKEACE6AoGuhBADgUBgpUDn8rYImF/I/eWXX8kWdsP9P3Xq1DRlypT02GOPFSHAHS8fMuXaZ22z/jfZZJO06aabFrIdMmRICST85S9/WX47+eST04c+9KEiAlZWf8AbQJzce++96c9//nMRBQ8//EgZsjD8wDNh6EDiMSASqpcgREGgqyDEQCAQWOHQsSwkAubNKy53Y/EIf/LkyYVMJ02aVIYDxAgQCyzt3pnQjf9vsvHGaeutty4ueGP0vhMBAvYEF0559NH0ne98p4iGz3zmM2mfffYpYmBVEK+hAnW/4YYb0o033pimZiHDg7BuFi+8AwcffHA64IADinhxLj179SrTFwOB1R0hB


Chunk 563:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 33pimZiHDg7BuFi+8AwcffHA64IADinhxLj179SrTFwOB1R0hBgKBwAoDsoYF8xvW9FNPPZnuvvvuIgDuv//+MhxAAIgX4AUgBET/i87ffffds5W/V9pll13TlltuWYYG/FZnAXje52cr/eGHHkpnn312CQg84YQT0ujRo1eZGOAl4JkwVPDAAw+kq666Kt18881F2BjS4NEQx3DIIYek/fbbr8w8IHLUvaT2cgKB1Q0hBgKBwAoBIfDG668XD8DDDz9cyNJYu2EAMwGMvxsqIAAquNlZ1EcffXQJyNt8882L+7+fufzrrNOeqwEdCAHx6COPFM8AV/yJJ55Yhgl69yYGVl1/INBQEKNzM3xwxx13FMFj2INIGbbZZmmXLArUTUzDFltsUYQCURMIrI4IMRAIBN43PNQs5VfaA+3EAYi+Nxzw6KOPpmeeeaZM2WNREwEdOxwEaaydu9+wgKmBZQpgFgKdrfTHM/BYPsa//du/lXxExMiRo7IY6LXK+wP


Chunk 564:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 9+wgKmBZQpgFgKdrfTHM/BYPsa//du/lXxExMiRo7IY6LXK+wPnw8Mh4JH4IQhuu+22Igi0iXOpsw6sT8DbsUEWOv2zACJkeDyq1yNiCwIfNEIMBAKBZYJOAxEaK3911qz0TBYBBADLmJXMM/DyK6+k2e0LAcm7OCBF0wL32GOPNHz48EKgtokhEIDX8Rnniudl+MMf/lCIdO+99y7z/eX/oPoDddIW4gkMhQgyJBCct3Mxw2CbrbdJW2+zddp8iy3SkI02KkMgdX2ExgyInkUUNCfnX4MQOybn6jMQWFEIMRAIBJYKOgsJwbP2eQFuv/32YhEbDkDS4gRYy/WlQO8FhFZXDESKCL6KgM6eb2Uqf8aMGWVfRLsyZxIsDRy7tos1EwyHGEYgBvymnn1690l9+zVegKS+Eq+I1LxoUsdtNb824gmRTJ20XHL1KNRzb/6s2+u2QOC9sDTP64pAiIFAoIuD9WsdAO5/i/CYc08QGAowLZBrHAHqVJalY0Fcz


Chunk 565:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: P64pAiIFAoIuD9WsdAO5/i/CYc08QGAowLZBrHAHqVJalY0FczeT1Xs+1sokN+eq+qwPUy/nzFDSffz2nWlcJkRM+PXv0zOSeP3sh+97Fk9BM/DwePm2X3hYEjcBKnzV/mZHR7omw7oKgTDEZ3fKxoqcMvBeW5ZldHoQYCAS6KBCcYDnj4kSAT5Y50gOfrGLWMI+BYQK/m1K4qjqYroaO4kDw49prdz4sUIUDsm8kwwj1e2OVRWKAAKjDDwMG9E+DB29Yhl4Mw1izQZ6uDMGaxCjx45wMKTn/1QGmmbrX1cu6GOrYGT8RzE888UQZUuL5EVNCwLnOSwvCmyfOPqaw1iWxlxchBgKBwGKB5K0R0BgLvz1NmfJocdMjHpH/yAfJ6Eh0dKYOXn/99cVrUMfNVyUqyda+Qb2kVV2PFYmOwqHj3wQEwbD99tuXBZA22GD9fL6N9z1YsbG+cRFRIaDFgYAzZdJQRwVSc41XFOG8XxiCuu6660psinvPWg5bbbV


Chunk 566:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: RFRIaDFgYAzZdJQRwVSc41XFOG8XxiCuu6660psinvPWg5bbbVVGUL5oFDveSLlZz/7WRnSGTNmTJkiS6h0xk9m01inwj7edWGNDNfENVxaCNS96aabSvnW4fAcLum6Li1WlRjoduaZZ367/fsKQ2eNHQgEVgyQCSFgiuD110/Mn/eXDoM18+EPf7hMmTMt0KJAps4hHuKhDh0glSV1MJ7fSmTVyu3W7W0iX1Z0y5Z1r969imXMVc5FXglsaWMYPmg492aCb7RJI4aCoHEeUvXEEGYIyRAO0iYGkKS4gzlzZmcSfb6QvDw8CAirtkU9VgWiveWWW8q6Cffcc0+57ixxeYiBD5J4BaZa08GnehM47jdi5YNC9ZhNmDAhXXzxxeVaeA6Qs3bu7D62j2vlN4LBYlqucWd5FwfXXRkEgOvic3XxkCwNIuw1EOhi4OZnwVht7/bb/1xISfT+Jz7xifSRj3ykiAAdMuJBukiG1cIjoLNanDVeSUhHjrh1aFyrl


Chunk 567:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: +Jz7xifSRj3ykiAAdMuJBukiG1cIjoLNanDVeSUhHjrh1aFyrlvHVOQ4atF4hHmTYTFbvBULAOLlFf/bff/+S9tprr7KM8ereWdY2cc59MpEg9o03HlLaRNJG2kqbydcZedTFnFicpjPuu+9+pV1Z+0gemZrxgfRZ2q+3v/uhwr51RgQr3D48Pe4D1/aDhPtJIoJWJy+P9jM0pl5LIzZ50VjzH/3oR8u9STQsyz0Ori/PCEFOfHyQIu39IDwDgUAXgo7t8ccfL+5IpMD6OOiggxa5Z60DoCNEFHWePdFw2WWXlfFMswo6dtqeV2TGauey5rreYYftS9puu+3L31tssWUhsDrFUD0cp5m0FgeeAGOorGMu82o1qctzzz5XOuzVEcgASRBW2na7bbctHpfaLrZpL78TBCz82i7NbcwFzVLlzameAALO+Vv7QLyHlRJdV96CuTkP8eHYa+U6DM4ko+0dQ9nWfSD89t133yL4lpW0ViQMDZil4dzUz/W17YP


Chunk 568:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: +U6DM4ko+0dQ9nWfSD89t133yL4lpW0ViQMDZil4dzUz/W17YP0DGgP96hr5RnRlu49cRqL8wwEcj+Qb9qIGQgEuggs7nPDxImF3HkHrKTHG8ASYWWzIg0DcCM/8sgjhWjqCnydWUpICTGz3A0z7LTTjrkzH7LIqkFqnmf5fEdsyn7ooQfT5MkPlhcbOV5HgdEMpEZEEAE+ASmygp9/7vk0b/68sm11grZEtFzLFiYiiHgFbHeuzSJIm2pbQzAP5jYRv/F8tuRZzBXas7YBCxJBEQPaklDQRgjMMRzLUs6u69Chm2Yx1bfks3CSuA/X6sgjj0y77LJLe+mdg9va9ZJ8B8clziSiBBA5D0MVFa6N/OqEPOVzjZXj3N0vAh9rP/+b3/wmXXvttaW9WNYWcXIuxZvSft41r3ZTvmPWe1GbOo5y7WOb+9jx5FWvWj955CV21Uk9m+89x1FvdemV62AlzIV538MPP3yRxQ/ESi0L1KtOv1Vn7eN4juFY7n/J8


Chunk 569:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: vdemV62AlzIV538MPP3yRxQ/ESi0L1KtOv1Vn7eN4juFY7n/J8Wpb1uvV3G7OS3spo4q+5UXz87oyEWIgEOhC0OEQAuPHjy8EZJ19wwI6HZ0ZK411aeYAa/PJJ54shD133tx3dSo6MZ0ZESC4atddd8kdXJ9CTiKj6zLF9tPxsfx4D/r27Zc7zTeL2PB+Ax4HrvDFdVpVTEg6e9CB6+ClVdXZLS14V5C2YY3Re+9dLHEEoD14W155+ZU0e87s0n61XZAPAnBNrPJ499135fZ5tOxXz0872Af5Vct53rz5pS0roWkfROvYRxxxRDroIOS6RbkmyyoGpk17Ot1//325HmaZvFjqgQCVbV/DHOpnmAKJVQIjaqY/Pz1tNGSjYl2z9F1jXibE6NXQR44dW5aldk7EwNVXX13OixAA11V5vCdeEqWd5G14Q6amm276U/lOEGi7uiKkc7fN/SvYlbeEmHBc5RFmyqyraRJJ8iNzxyQExC0IANxy+PB08kknlXM


Chunk 570:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: SvYlbeEmHBc5RFmyqyraRJJ8iNzxyQExC0IANxy+PB08kknlXMkmtdff1A+z5lFpPG0WGFTeeDa8iI4d9fbdudT72/HVg/PliEb14sA96pu77ggOEztrddHGbxh9lterKrnI4YJAoEuBJaLTlBHWdcQYPXrtG699dYSZCbdeeedZXux+ha+e1wZKbF+dPaGGFii6667XhmXNh3L0ILOUTm8C4LWpk6dkl54YUbee61CJFzkOjvWk06y2ULrCL/psHXcko57Sfk/KGgXnf5umfAMvyARZIKItYnk+12Z7AXNIQpT0pA1wkMQ2gWpanuE19yZ++7cXUdpwYJ3emq0CQFGdPjceusRacNMxgty2yEqx0LoyBNJLwl33XVn+tOf/pTvDW+ifCKnp0p9kSHSNbyBwH7+85+Xay4OQX3Fl7iX6v2DiAmBe+/9S9neEAS7lfdUaC/3jHvS/eg3ZInEiUVkrT2ISHA8gX3uKyJVPufkfKt3xHfHGTduXLryyitL+


Chunk 571:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 3ZInEiUVkrT2ISHA8gX3uKyJVPufkfKt3xHfHGTduXLryyitL+yBr7aXe2s89blEtdXQM96ihM2VWcTE4n9/Fl1yyiLxdIyLDrAFeKfc/Ie36en7+67/+q9zzrj/vjHa4JO9/yaWXlnMh9GwjRDx/rgcBI5ZEHQiiX/3qV+WZJDYISufTVRBiIBDoQkCkOlQdYX2Hv85Nh1zne+u0dJhLAqtHh8d9SgjowJDC+eefX4gfuSMN5SBxHTGrirfhvvvuLcQhqJCViVR0xspYVVbMyoB+C4kbX/7kJz9ZLGAzMM4555x0zTXXlHZHjm9mS75aotoICSI14gCRCUTTLsjtvnxdXLNlbRf5tSePzeabbVYWJ0J0yyIGWPzIWcDml770peJFUlfXkGeCoPvlL3+Zrs7nhuBYtzwGX/jCF9IGgzcoAgApv5SJ+NBDDinvm1AnhMqDsc02W5f2cr8gUW1FPH31q18tx9Q+hJP7UlvClVdOKCT/13/9lfTlL3+5CFF


Chunk 572:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: r8gUW1FPH31q18tx9Q+hJP7UlvClVdOKCT/13/9lfTlL3+5CFFlEQeOhzzrNFhCQNudcsoppf6Oz+q+9rrr0rSnnszW987ppGz5e38GAicYeASOP/74YvV7h8ZFv/99qYNyPvaxj5VjOobr6BoSKsTsr3/96yL0gFfANfztb39blte+P5/fc7ntDe/w1ojVqHUuL/DK2wkRZWgDngn7EwQhBkIMBAIrBTq1Ggug0/T3skIHPmSjIenAAw8s7x5g4VyarR9WpI7xvYiLlWVYAPEpS4eKmJAl0dBVBQHydi5jxhxYhBJrlzha2vNyLWZmy5olipwQIHHG2mZ9LosnxJAKC1dkOnLRpy6rGOCmVodts7hZN5eFnNUNaQ7Nv4lJQFqu//qDBhUCR3bEEAsYYW6cCY2H5GNZNK6TCRBhE4rrrjuwWMXajBhgaXPfH3rooWV/Qye8BsSIe9W99thjj+fv92QRuVEZOtjIDJXcTt26dS8iRT0QqHNTd9DuCF9gI


Chunk 573:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 99thjj+fv92QRuVEZOtjIDJXcTt26dS8iRT0QqHNTd9DuCF9gIgGrvFtuvjm98cbsknd4PiYL3H2rrbWNa4ik4fdZDPBKjB07thxzkyyAHs/iRDsQRK6zoQ3CyHVSjvprlx122DFf83xd87UjEIgM4sy1IYicM9FhOGLE1luXNiYGHF99CZeuJAY+uDDUQCDwgUDQFIIZsfWI0gHedffd6c677irfEdY6meB1pjo6EdhcqcZA7YPk5NEps4Z5KFia1jbQqdbAq64GZGvIw6yJ4cO3LJ09q5gQYA07ZwSnDRCDaZFIQ7s47759+hZL89WZM9NDDz5Y2gV5fySTIA8K0bQsUJ+a3i+QJMJk8XOr/8d//EcJ9EOEyu3V7h0QN+K6csFLCB7xIjIEt0Hetl4mVNtYxPatIrQKJOdHvGgj35Xhu+ReaQiR54uARNjK7p5J1efgwRtkAt2stL+hEfcPK7zWxXbHdlyf2t5xCYhZ+Z5dO2/X9o7jd8dshr/rjI9


Chunk 574:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: L+hEfcPK7zWxXbHdlyf2t5xCYhZ+Z5dO2/X9o7jd8dshr/rjI96Dghdea4rUeAaaQO/VRAtG244uJyX+hBfynB+zfEvjq8NtbW6dsX7H0IMBAIthGpxDt9qePnkwn3g/vvTC+3jqjqynrmzY+WJvvaJ9BAgi02nKI/O2HDE1ClTi6dCx4gY/e4YXQ1IojG1ctNCUGZLsHirp8Q5IwttoV3EBoi30C7aqP+Axvg50jM2bUzZ8I3fkASRtLLbBZFau4CFXN35NeiQdV49FM6HxECGlRjVXfLddql5m09o/ObbO70kzfl9d4wam649ba+ErX20E8jbEBIN4VD391nrUrcB4ma1ExS8JMbpBdMaLvMbEdvRY2Lfeg71vGt54HuzQKjwd91Pkqe5LhX+NiRR83ZVhBgIBLogdLbvBz1yh1mtGOTP8uUS1jkjRNakzpQlxTJDYjpeHTWiZ0H5nXdBHWa+OrMMV+iYiQGuUZ17xw5zdQfCcn7V5czrgVSdIxLQX


Chunk 575:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 5nXdBHWa+OrMMV+iYiQGuUZ17xw5zdQfCcn7V5czrgVSdIxLQXs5bHu3n/LULgWCb36oHgNXM5S1Yz/7F6sz7+G1losYtEAHOgXfioosuKsGBSIoL37CQ+qsjweDTfbC8cJ7IvpZFdLz22qxyDJa8lz0RW+4x9wvRQhjIL7n/kLn2dC0WB4SrvSXihwi4/PLLi3eKSDAs4vwCy44QA4FAF0MVAu9HDvTt06d0pDrmGh1tbFgnOzBvM25snBqZi7Dm6tbRspJZlixdY82NDr5XIRMdu/F124gI27uqGCB6uNG5oI1XI1HbTCHjGTHezsrm7jY+bAxdGyAhc+yRmXNHdHUaGoExaL33LwaQLLKUEKe/HRPhNiekbzjA8f3tmqina7LHHrun7bbbtog1hOyau67q6DwrkftsTs3bHL/jNveiNrLNORMhytN+SJ8XQMCd1St5U8QCEFmCX/0u9sR9qJ1AHm1Yj6P8elxQtmBHwsHQlBgH10ZMQL0viTf7VaH


Chunk 576:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: /0u9sR9qJ1AHm1Yj6P8elxQtmBHwsHQlBgH10ZMQL0viTf7VaHje3Oda2reVo8hb/2t+Xw75u1sW/P2mpTXVRABhIFAF4LOBYGbUuVTNPuydDiICakZ+9ZhW+seIXhmWbtc38Y9/aZsQWI66GpF6oR5CwgAhIJ0CAn7mpWAHJEMYuhKHSEhIDYCmbCseUwQGWLRHnvssWc5p1dfbSwQhPwJqkqkxIR8hl1YrLb5W8wFD4L2QnjKWBpoU/sTZ8qr7n4Ej0x5HgxFiGuoSR77NMh3UPESuEcImOnTXyieCrNOiBhlIi/fRdEj5rlzDSFYn+CpIiqIHnWuREccipp3jWfPbqzBrw2IHL+rl/IIxyoetaeXBAncc48QDo5taElex1Ce/c1kIDYJGLMTBLQ+99yzWbz0KMdxjeRz3s5FG8jr/HxXDqGgfPfoL37xizJTYkbO47iukeuqHbWNe9x2qNNpXT/3NEHCsyLGog4Xqb9z0g7qpizPhOE252LKpH3ka


Chunk 577:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Ne9x2qNNpXT/3NEHCsyLGog4Xqb9z0g7qpizPhOE252LKpH3kaY6Z8L0rIMRAINCF4NlCQJWEWHk6OZ9+Q8AsksWBO5sYQPBEgI5VZ6pTRPQsR/uz2HRyhxxySCE0HZyko3RckdTIUmeIMOUlBnTGXVEMsJ5FlhtzRggSi9W56cxFiDt/5y4PknbO8kjaQXLuyEgbIg/kps21F8JCKu8F1wKpu0a8NOrgGmlT5IKo1AkxEmk1IUxTGg3XyOdYrgFh4d7QLVtUSj7DHoaBHEs5trn2CNc+joXk5anBd1VAEAGGPupaE35HvtrGfvLYlxAgJrWr8xGLUoedlAXy+s09ydJXD+2J5P1W6yRIU53VzbVx7zme+js37e3e9Fx4FhyH0LAwkva3r7IIXOWrn3r7TXsSa8ojomxzvOodcu7O17l6RrSra10FiuvkvOV3zQlu94j28fuShj1WJ8QKhIFAF4KHVaczdcqUYuXxEOjIdJ626ySrq1aqJKVj1WEaV60


Chunk 578:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 8QKhIFAF4KHVaczdcqUYuXxEOjIdJ626ySrq1aqJKVj1WEaV60vY7G/hVZYNQhH54bkdKSEgk76U5/6VCEelpAxabCdiBAnoBPUuQus++Y3v1neEudlOrVz7SrgETGmjrxYeBMnTizkjTSQPle0tkUmiML0wxdfnFE8KyziddbpnvP2TDfmdpqRz12bI5VjjjmmeBFYnSxSwy7N0FciNAQmaWvtj3xMY7N/ER35moqcXxIQEdJDwspRf8eroo0okNwHzkt+qL/bB/Gpk3uIwABkZru/672lrsgU6VVyJ4S0hXvNPkjRsID6V05A2IQNz4B7xz3ifKuw8Ld7Wb3dz4CAiSpkTqSa+y+fcus5gPoRbO5PAtb6CM41K4hFJK3u8sjrnndeBID7nUhQd+em3jxfztc27YbY1cW5qkfNq72Vp97OzzHlrUJGfZYHq0pUhxgIBLoYdDZIqeH+nb5ICOjQJN9t87uOT8cl6QjNf65igKvUQikP545ZNDQrkCdA5


Chunk 579:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: +nb5ICOjQJN9t87uOT8cl6QjNf65igKvUQikP545ZNDQrkCdA545IdHrEA+IgGHR0BIPOjTuUENGhs6y4eE8//fQiLizQo16lI+4iqMv/WjTniiuuKIIAuensnbM17rUrkYPoCAJE4Tx11g1rvWf64x8vL+8lWJDJFcF99rOfLVM0uZu1i3atAkCqngdt3rA0B+f9Ninkp01Z+UisZ87T6r0qwnVNzjvvvDLMpV0JiApk7nf3ufbbLAsV0xe7OlaVGIhhgkCgi8HzVS1WlqCOEWkjNJ+s3NIZZovF7yxURMMKYsmx4iuZTX7wwfRiFhA6HCS31VbDS9kLF75VOlcWnHFnpM+6ckx5jasTGwifZeW4xqoJjCoSVlUntrzQnuaUOwdiCcE7b4JqrfyfGRi77LJrtgS9ZKfhZjZ2T3g5R5attlFOGbPO7eo7y5m44kau4oyFSXRVN7vfuNGVz01uWMAiP8bZ1cd1I0iiR22IAWKKqCJu3WPalACQeK54FAg


Chunk 580:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: NGVz01uWMAiP8bZ1cd1I0iiR22IAWKKqCJu3WPalACQeK54FAgrYpfYCiw9wjMQCKwBQEoS96lUhwbefNNbDF8rnaZxa0BSXJgCoyxHawgAsSEqxM7yZyETGUgP2REBOmAeAoFT1V1MOCAtq7Nxs//0pz8tljULuqt4BvRXrHCvBbZk7e23316Cz3zyhnRbu1vacKMNC4Gz1K3e95d77ikiS9sQDwLanLN21v7cyYYHvvOd7xThJbhNHp4aoosnQVtLvg8YMLCUxbugTQkA1yP60rfhftK+phK6NoifQKjQlu5bQoowXlOwqkR1iIFAYA2GjoQoQGosKR1qJRuBWJZr9RZEFj0SQ0DIaq/cqW6z9dYlj79ZW8ZQCQGWv3JZXsZEWbNf//rXSzS+ZY1FZrPgCJKuAOfHI2BqmrXtWfdeOGNtfx4Q56pdWPJIxqc8dTzYOPn99z+QxUBjaMQKjsOyYDAc861vfat4At7Ibf9a+zizsgwNEAxSkP/Swz2lv


Chunk 581:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: jaMQKjsOyYDAc861vfat4At7Ibf9a+zizsgwNEAxSkP/Swz2lvQkB4tR9XeE68oRJvGBrCkIMBAKBlQakhdxNISMIrOSmk9XxICYdKkuLJYukkFaNU7AfEAgsai+Q+cIXvlAsW0Lh8SeeSDNzHm/aW62Rz9X5ImZvBtxyi8Y8eJa+l+RoF8KGiAIEY/zeeWsH5KOteEl4VsRYaCuWvpcCeYGOOATj/WIyAoH3gxADgUBgpUInY6xVcBuLnjAQWU4oLAk8AgiUJ0CUvTf8iXyv0KGsqg5suaGeub/SZ9Vey9DKlCmPFo/JuHHjiovfkMiS2sX+a2chYC1/Qwna5Nhjjy1DCTF2HVgerKpnKQIIA4EWheeUm7tauyx+cQWGC+rvzfA3y5dFTAgcddRRJVmfv5nw7CVvl0qNqhcgde/qr+0iMI13oLN2qfubFdB/wIDSFjwChx12WPEyhBAIdBWEZyAQaGGwdgVhcXMbB7fCnGmDU6c+ll566cVFswIME3B


Chunk 582:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: BAIdBWEZyAQaGGwdgVhcXMbB7fCnGmDU6c+ll566cVFswIME3B/m6HA8rXAkAA50fDc52vaM78wt4tZAcamxVMIsrSIDU8K0cRT4Jx79+6V1l+/sSjObiNHpv323bcEVBpmWd755YEAxDBBIBBY6fDwt2XiE5hlypyFhMwYEDgnOt42goE3QAxBnQ5nUZcaQMdbsCaCJ+DNTPpeS2yqobYRGGmaIJHE6hcnoR14AQgCbUM0EU/hFQisCIQYCAQCqww6nOolEAhXFyuqMxCQG+LjBeA6JwIEz63JhFeEknbJokCbEEbaxKd2cu5EkvbQLo2pgb3KkEH0gYEVhRADgUDgAwOruK5VAIivW7fumegaS9q2InTK2mP+fEs7N+IHtEX3LIqsdBf9XmBlIMRAIBAIBAItjlUlBmJQKxAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFocMbVwGWCKR00d4ZzXWnvts


Chunk 583:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: AoMURYiAQCAQCgRZHiIFAIBAIBFocMbVwGWCKR00d4ZzXWnvtssZ5x9/9ZZU3KG0jr1S2BAKBQCDQOVbV1MIQA0sJK5A99vjjaeqUKWVt8uZ3tVuJzVKkXt5iKVLLlXrZSc8ePdIrM2c2lnd96qmS18ptQzfdNI0YMaK88zwQCAQCgcUhxMBqAiLAa10fe+yxNGHChPIiF3/3yERvKVKwVOuoUaPSHnvsUdYp98pTa7hL1jK//PLLy/vRLfHqTWhjxowprzc96KCDyv6BQCAQCHSGEAOrAVyEu+++u4gA73x/+OGHMtkPTvvtt38h9B122L7ks075/fffn2688cbyxrfXX389ffGLX0yHHHJIWbP8vvvuS+eff34pw/rlRMAJJ5xQXnMaCAQCgcDisKrEQAQQLgbWIL/rrrvSJZdcUiz7qY89loYN2yyNHTs2feITR6V99hld3twmbbvttmnfffctBH/cccct2h94D9Zdd90iCuItZoFAIBBYHdHtzDP


Chunk 584:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: bvttmnfffctBH/cccct2h94D9Zdd90iCuItZoFAIBBYHdHtzDPP/Hb79xWGruwZoMEWZiI3FHDBBReka7I1P2Xq1NQnk/oBBxxQrHnvKzccYHjAG8rEDIgFsA35e/3r9ttvX+II+vbtW8riHXjwwQdLXgJi5513LnEDFV4Xywvhvel//vOf05133pkeffTRMrTgTWj1lahUonepT58+PV111VXp5ptvLjEJjvHyyy8veuWsF6jIK991112Xbr311vI+du9nF9cwZcqUUo76SIFAIBBoXYQY6IhMoNz8DzzwQPrdb39XSHxeJk3ETgiMHj26jPs3E6jzJQoIAbEESBjhb7zxxuXvl156qRBxZ2KA+HjqySeLECAAHn/88ZLknzx5chEJyq3vSFf2jBkzypDEH/7whyI8DFMQAcSDuvfs2avsI97hnnvuSRdddFF5F7vXr9ZXsD744ORclx7lHfWETCAQCARaF+G37gBky8pmSSPXmTNfKZa5YMBhw4alg


Chunk 585:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: clx7lHfWETCAQCARaF+G37gBky8pmSSPXmTNfKZa5YMBhw4algQMHFuJfHAwJNOIJdkjrr7/+Eq1uxyI0bsrW/WWXXVYCDzfaaKNyHHjooYdKnMENN9yQnn766UL6ZjEISrz44osKofM8DBkypFj5BMQjjzySXnml4SEgFJyHeAZ1JkLUX5m2mfXQPCsiEAgEAq2JEAOdwNTBW265Jb2cSdV73REukkak7zXuzyLfaqutingQJ7Ak4aBshHxjJvtrrrmmkLz4A8MRhiIICRZ9de/PmTMnzZ49Oz311FPpuuuuTy+99HLxWOy9997F00B88ETwVMj3ZLvHgYdAffbZZ5/04Q9/uAQxGkZYQ+I8A4FAILCcCDHQAax1Fjhi9mn8HaHzDiDnFTUE4jjKZtG/8fob6aUXXyqEb0iBSEDWNUZg3rx5RQiw4u0jOHFe/v7888+X+IJJkyaVMvfff/8yvdHwhDorx76vzXot3XXnXenqq68u3gKBjvvv/+G06ab


Chunk 586:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: vfff/8yvdHwhDorx76vzXot3XXnXenqq68u3gKBjvvv/+G06abDytBDIBAIBFobIQYWAyRcUYm7edvygqhA+Mbstxy+Zdp2u20zOW9aCB/JCwYkFBwTqVd3PlHCSzFy5MhC5Lfffnsj0PGaa4o3gHdAMlwhtkFcgn3EPvzmN79J5513XhkG2WKLLYpo4MkIBAKBQGsjxEAn4AEwM6C63BExVztPwYoUBMSAoQTDAscff3zab7/9SnCgtQrED5hJ4HiOX70UyBvBf+Hzny9DCjwGhhGshSCgUJCiIEJDGwift2CXXXZJ3dfpXmIgiIZf//rXJV+deRAIBAKB1kbMJugEyNRYvah+1jaXOytbUKDgPiJhcaheBG0gse5Z+oudTZDz9+/fv4zjExwseMGCzz33XPEO8AoQJvLbjyeBgNg+1+Ulyx6vvXY5Bvf/vffeW7wFAgpZ/TwE6603KG/rUY5LOCjbDAPnxcOwySablGMHAoFAoHURYqAjmuqOwAUTs


Chunk 587:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 3KG/rUY5LOCjbDAPnxcOwySablGMHAoFAoHURYqAjmuqOwAUTssoJgO22264EB3K7L+4ckTei9TuiNma/ODEwPJc1N/9+1513pl/96lfp/PN/XfY7+OCDC+nXtQM23HDDtNtuuxXxIIhx/oIFaXYWDh856KC0dd5mH4GI6ur4xIJ9CBjiYNdddy37W2zygQfuLzEHZh0QNtZD2CDn69LXLBAIBALLhRgm6ACUaLzdioKi9FnOrPdK6Bbree3118v6AB1R8/32t78t+Vjii4O8phU+mklZ/iuuuCJvXSt95CMfKS884ubvDIYMnnj88fQf//Ef6dlnnilkbunjM844o9QXuP/VgwfgssvGF0/Hxvk8vnjqF9P/+38/L0MHvA0luDDnnTd3XtkvEAgEAq2JEAOdwLAAF/unP/3pMp7P5W783kp+gvVuu/XW9ML06e25G94Av3sZ0U9/+tOyjQdgSdMQiQHETjQYjuABAFY9EYGkeSQ6wrHMOrjwwgtLMn2


Chunk 588:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: tOyjQdgSdMQiQHETjQYjuABAFY9EYGkeSQ6wrHMOrjwwgtLMn2QB4AoaAwLrFcSTwZvgSmShMMj7WsYCFTcfPPNy1BCWXAoi45u3bu1lx4IBAKBVkQMEywGiBx51iA/4/qC+8zxR9xWBkTEFu+RrBbI9c4Fb5XCGluAuAUEetuhcX0igNWPiHkdlOkdCMpyzP79+xXCdxz5iQzlIHoEbnzfPoIFbSvxDXlfxyYqDEHstdeH0tChm6RZs14rgYjz5s0tKw+WlQ3vvTdNuvvuMuTAQ0BEOL8YJAgEAoHWRYiBJYAQIAiMuSNNRE8EWAtAgCERIGjPp8A87v1jjjmmEC3Slp/lby0AkfyW/UW8Yg7MCqjT++yL1P3OK0Ao1DZUB3ntQwyIBfBdfrMEEPyt2fp3HB4Cqx/uscfuZaijxjpYe0BgonUGHnrwwRKLcOKJJxXRojzHCAQCgUDrIl5hvJQQFMhS9+IfJN8M54t0Wfo8AnXVQfl4EcwMICAq/M7TI


Chunk 589:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: rIl5hvJQQFMhS9+IfJN8M54t0Wfo8AnXVQfl4EcwMICAq/M7TIC9i5wVg7fMIKIvFb0aCY9ahAkKBGPAb8DgQHMoXOAiEBnFBCCiHYCAueCIcw7BBxU477VyGE9ZZZ/ErJAYCgUDggwVv8qpAiIGlBHI2xi91vDjOVxInINXzlw/B26dZQPidNV7jCgTy+V3++hs4ptRcfhUayrSv/RwDapkd928+BiiLCJHP90AgEAisnggxEAgEAoFAi2NViYGYTRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OEIMBAKBQCDQ4ggxEAgEAoFAiyPEQCAQCAQCLY5Yjng1Rbko7ctQWo6yLknZvDSldw5Aae+cotUDgUBgzUK8m2ANRCX1+gIhyUuGmlP9faHf27f5u26vZWhjLy3ysiGioKb6t09Jnre3S3FtAoFAoKtAf78qEGJgFQKxz5s3P82


Chunk 590:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: y3ysiGioKb6t09Jnre3S3FtAoFAoKtAf78qEGJgFQKxz5s3P82ZMzu98cYb5RXDs2bNKunVV18tf8+dO3fRq4slf3tDYbMQqDdHz549C9l7U6HvXqPsdcUDBw4sySuN6/f+/Qekvv36ljxrx/UJBAKBLoEQA10ULpxXBSP0SvaIvn7OnDmzJH9XQeBTfgn5S145XF+ZXF89XG6KpqvVrXu1/qXG65O9mrhPnz6pX79+JREHRMF6661X0rrrrlvEwaBBg9L6669f/pa/u9cph0gIBAKB1QohBlZzaLS2bK0jahY8Qpdee+21QvQvvvhieumll9LLL79cPl955ZWSqhCoxF/JntegfvICNHsCoLRphyu1VpPLv7Z5HRqoHgOfBEJHUbDhhhumjTfeOG200UZpcP4+KG9reBD6F3HA0xAenkAgEPhgEWJgNYSLgrBZ7UUAZEJ/rZ34Z8yYkaZPn55eeOGFkvyN+ImDKhSaBQCy106VvH3WZHsd5/dbjQewv


Chunk 591:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: YkaZPn55eeOGFkvyN+ImDKhSaBQCy106VvH3WZHsd5/dbjQewvaZaH+VUIVHL9XfdJslnnyoMkD3S5xXYYIMNsiAYkjbZZOM0dOjQkoYMGVI8BwSE/IYW1CUQCAQCqxYhBlYDuAhSJVYigFXP0kf4zz33XHr22WdLahYCvAFEgPz2VQYyr2Qs9e7duySEi5ylup1Fj4Clat0j42ZRAIi/ioAqUGoiPKqnolmMNAsG4AHgMSAKCIFhw4alzTbbrKRNN920CIPBgweXIYdaj1yBmLkQCAQCqwAhBlYDIMw5c+YUQuXiZ+0/88wzaerUqenRRx9NU6ZMSU8++WQRB8h4cRcNgSN2pIpwuearu37QoPXzZ2McHylLVRhUq7xZDGjb2r6OV8UAj4O6EisSrwRRwmshESnqWQMVX3+dSGgEKlaBAI5JGBAD22+/fdp5553L5+abb15iDNSvW65PqUvZIxAIBAIrCyEGPgA0LG0R/42IfsT59NNPF9J/6KGH0iO


Chunk 592:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 65PqUvZIxAIBAIrCyEGPgA0LG0R/42IfsT59NNPF9J/6KGH0iOPPFLEAFJladdgP0DWiJulLSFYpItsEeegQeulfffdLx166KGFYNdbb1Am/YYXwL7NSf5K/D6htmnHtnWjSOpeExGzyJtBKOQ6OBfi4Pnnny/n9NRTTxUhw7th++uv5brm816Q80MNRlxv0KA0fMst0y677JJGjRqVdthhhyJqiALnqb5d9XoHAoHA6o4QA6sATryQaREBC4plzYpGkoifB+CJJ54owwC8ArwDRIK8gAgJgOpmF5DHtc76R7KTJk0qHgSWuuGAAw44IJ1wwglp//33L+P0ZgOs7JYq55jPj+eCcFH/GuTonJzbtGnT0uOPP17OtYidLA7kW5jPc60sRvr361fOz7ltu+22aeTIkUXQbLHFFsW7QThU8RIIlA6lXaRWoVo7tOa/67bFwf1UU/4rf77dt/hsFs2BwJqK93pOVhRaWgxY2Ifl/urMmcVafuyxx4oXAIH7j


Chunk 593:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: dt/hsFs2BwJqK93pOVhRaWgxY2Ifl/urMmcVafuyxx4oXAIH7jiR5AYiE5jF2wXdc5sbTN9lkk0L+PokByXb7Xnrppemaa64posIF3WmnndKnPvWpdOSRRxZL+4NuJ+KAMCAKnD8xQBRUYVDPXz71R/rOe7vttku77rpr2m233YqnYMsttyzBiH4PtC5qR+JZKeKzPXbF81NjWebM8TmnbKsxNfXZQuzVM+bT8JhEcDdSz7ROj3VSz3YPHIE9MN93PUKMBtZghBhYwdCgLBKJZY/gXn11Vpo+/flC/nfeeWe64447ikcAAcoHOhkdUQ32q+Pp22yzTdpxxx1L8jcy7JE7KGfuWK+/8Ua6+KKL0gUXXJBuueWW4lUQob/vvvum448/Po0dO7aUtzpB5/xiPvfHsnj5y1/+km6//fb04IMPFi+HYYYqipyfcyEK9tlnn/ThD3+4xBWYrijeQQceWLPhHnAvlKGofN/4fDM/U54rQ2iz8v3inudpc+9IM31


Chunk 594:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: ijeQQceWLPhHnAvlKGofN/4fDM/U54rQ2iz8v3inudpc+9IM31m4e37LIGt7cNohtTcM8Rk86fnrsbOuK/69eufevfpXYJZB2RB7lkUy2LYSsyNPERC3H+BNQkhBlYgNOZ8nVUmM5awDooA4Ma/6667CuEJuNMx6dSqEAAdD7Jnye+5557Fuq/BdL1798kdV/dOA+reyse8J5c/fvz4dMkllxRyZe0MHz68CIETTzyxuNtXJ6hzXQK5eg2IoxtuuKGke++9t3Tk2sc11mn3HzAgbTZsWDrooINKPARvgbYJrNnwnLgXxKAYavLJm8SzxqPkGTMUVYfVyvLaORmyMvS0Tn5mioDIz5xnx3ATIoeGt8BQwsIsFBrTY4v3IN9z7juLY/ns1bNXGrLxkLT77runD33oQ8Vb5fkalO+/WEArsKYgxMAyQCdTA/aKBZKJXWKZSOW7bfl7sUpyJ1WslmwFC55jxdSIelbFgGxlbJ4FABe4MXIdjCl3ZgGwQAgEL


Chunk 595:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: yJ1WslmwFC55jxdSIelbFgGxlbJ4FABe4MXIdjCl3ZgGwQAgELstu3dpdkzqn9ro0g4fhxhtvTP/1X/+Vrr76qnzsWWX/PfbYPX3yk59MJ5/82dQzWz35aq+yC/5eqPXQFhIrz7RJwybEwK233pruv/+BNGPGC0Uw6Mh5TcQT8A7stddexftBFLDodPzaKLrmrgfX3zVe9Ly0z0pB/mJL/C35jdfIvQJEImJnqddZM/W5cU+4Z+S97bbbiiA3xHbssceWT/tWMfDWW8RAYxVO9RAMO3/evHIsz637ktfqzTfnpr59+xTPlGfVc8tjJ6aF98AxA4GuihADSwFWwyuvvLwo4p9FopNA+KzaRqrT6Bpz7VkipWPJnYpOhsXSLXdOOiudES+AMfCtttqqfIoH4IYUJ8BtSSzozKT3gvJ5HS6//PJ04YUXpsmTH8gdXFuJKxCZz5JGpKszEALLTFvqeM2qeOCByWl6tgRnz5ldftcWOn9DB9qM94QnRZvV9uI


Chunk 596:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: ALLTFvqeM2qeOCByWl6tgRnz5ldftcWOn9DB9qM94QnRZvV9uIVqePA8ko66Uoa2qF+FhGxigXlmg4ditQg2rcXpPIcSJ4Ln54NzwkBIBHYninC1idhbTtCdo2Qt+GuMkyWnw/PkeTaF3f+gMZ0Wd9dW/nty1PGa8ZDd/jhh6djjjmmPH/KcE+pq8/G/ae+by/NrY7uxzrVd9q0p4tA0Rf43bGJd89w83PMY8AjEQh0JYQYeA946HUA9913f7rzzjvSPffcU6bL6SR0FrXjqJ86wNqo3dbOpLROYwEgHReLghWx9dZbl1gAnYcgwNqB6aAMBSzrWTnai7nDEo8gdmDChAmlA0OILBbHVHZXID5tVzthLmGCy9/NN6r3GyCG9bMo0H6+VzHgs1s3QqAxDoz4/e6zkoZrwYpcN6c+ud2bBYKkDG0VQmHJcEXc7y9lq9395joVEYBY8/PQLAC48ZF/DfSTED7iZ/ETA655zaMchE74EbVIFvFKrp0hItfRN


Chunk 597:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: QLAC48ZF/DfSTED7iZ/ETA655zaMchE74EbVIFvFKrp0hItfRNXVvyyvV+wC5E+0XXXRRCa6V52tf+1rxKBEU8tV7qvlTN9XW1hAHkue63I+5fs/lfqAGvirbOaur43rOPM/bbLtteb6HbLRROU65N/PvcScFVnc097ErE11SDOgMXn75lXTzzTeVDoW78fHHnygeAJ2VTqfZGq1kpONBKjoC5FNFAAHAvahz03nozLjvV8S4ow5LB3XllVem3/72t9mqfqBYVrU+6tfVgEh0xq5DR7j22tu5VfL22mTb1lqLZ6BxTXTU8ki+EwgsOh21a7DBBuunwYM3LEMzhIVr5Zq5frwJyq7lB94JpI/Eb73llvyM3FyIvQpin1UMuDerCKjXtJKs7TWGxm/2db1dO88HD9qIESNK/AxB4DoRAkSC34m4eg/U5xCICkMDv/jFL4p4Hz16dPr6179erq/8y3o91cm5EgBEao1h4CkUw+DcdabqxEtg+KAuomUNjV7


Chunk 598:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 79erq/8y3o91cm5EgBEao1h4CkUw+DcdabqxEtg+KAuomUNjV7t95L7MxBYHRFiYAnQYYn8r9bFww8/XDowHQ6LBKnUhFyqq1KH4G/TkXyyYHVCxrv9jWRWdKfgQiJ/4+3EAFFgHYNKpLWTXJPQsOTeLRSWBO1OFLgGxn8HDGh4CRAMcUCouU7VCvU34rFPCIK3oe0ROCv5vPPOS7///e+zpf9KJvOGRe334iHIeQSLepYq0S8N3K+Em+fF8JlPz5xtRHZ5vvJz5tnr5/mrz1z+RPa8DBMnTkxXXnFlGrrp0HTSSSeVYYL3IwQqnINAVyKAcCFEnJ+YAtN6DdX5dGyihYfAOhmGsnxfP99fREHcR4HVESEGFgMNwxJArBdffHG6++67i1tTh0T5m+5m7FHn0ywCfOqgims6Jw//OrkDMkdZR6STW1nWgQ5K4BUhIJiQkOFmB8dWb3XWoS6p7fxWf19Wsl2VqGKgY0I6HS1QiQVax6x14vZ3Par1X70Gh


Chunk 599:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: WoS6p7fxWf19Wsl2VqGKgY0I6HS1QiQVax6x14vZ3Par1X70GhBsrtMZ1SFUYuMbytjq0Hev7+uuvT7/85S/L0JR2zb/k397OU776bE9Li3oP1mvjOtVnx9+NYZ367o0+RQgQdZ4/Qo8XgneMt4yVTggI0vW7fV1rz4S8ku/1OJ09n+rO+heX47nSN7gf3BvEo3oQ4wSB+ATxRZ495fJsEAS8gs3BwXVoSp4lPY+BwKrAsjyfy4MuJwaQhQjmf/u3f0t//OMfywMO3P0f/ehHy+p+XIDND3RNrMjaiYlyX1XTjxAh0uMeFTtAFPBmuMjqaFrefvvtV0hO3TqikmmF9tU5rq5wXrXO1eqsQgAZVPLnitZ566xZbTUhM/kWubDnzivn3CuTjOuq0yYMXGcuX9HjBFUVBdqmVTvxBbmdX37ppfSrX/2qCE+xNNp+VaDel3UoqBEj8jap2+b5RcY+CTtCgIehDi9UMq4ePuLdM6KM5udXIg4cUxChAF0eB6K


Chunk 600:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: +b5RcY+CTtCgIehDi9UMq4ePuLdM6KM5udXIg4cUxChAF0eB6KbAHE/7LHHHmVWi3uFl4BHTuCi5Ls6eebcR4SDfDxR1RtV6yOpgxiDcsz28w0EVgVCDCwGCEVU+z/90z8VUjXuyJrgETB/f++99y5WtgfYA+23lWXxLwtcUJ3WtddeWzoulhvS08mZVWCqoSl5OiDt56LoMOdkQkSQSLN26jpFHaW8FjpaHTsoAqCKgvrJG1A9Az6rV6BZFCAKkesEnw68vkeB96eW5XxdV50+z0B1++r4iQOWqN9bTRRoG22qvf75n7+Xrrji8nLP2b66wbVxjSrZVxFgm2Qbb0Hzds9K3af+7Vlwj1x33XVlzRBiQNkEBqtfYCLL3/3lnjJccP/995eYAs+fsqv3wqfnijCpsSoS0eJTYGvf9ro47urQr6zOaDzzlnlvzDqq1zGwbFhVz2+XFAMefmKAC5S7kcKn7j34HlwPNkFgUZ8tc0dg+dLVgRRYuTqiiy9ur


Chunk 601:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: efmKAC5S7kcKn7j34HlwPNkFgUZ8tc0dg+dLVgRRYuTqiiy9urEw4depjpV477LhjOvaYY8rKhDwcOhoXxWptgqC4VQVG6ejl11HqnCy04rNX/rsrL7LiZicQCAMdNLcvIYDIWHE33XRTcfP6rdnK1RYS0uAW1h48Q4SB+0DHri1bBdoG6YlP+Yd/+IfSboRWV4NrimiRfc+ejWGDHj0aw3nVa9AsEMx68Jy4Z+r5yssYqP2BdiGq/U50Sp6nZjQfl9Cuw1KeSYKCF8HfNViyeAuaPBTLA88A8e8aLqnzr3Wsx12doa9+5ZWZ6bHHppbv2pAHZnnbqtUQYmAx6EwMeIgck0XgAWE5GI8UnORdAJT+6vLgIPUbbpiYfve736XLLvtDUcxmLhx88MHpr770peIdQG5uAEMJhkLkZdHovJyHTohF/L/+1/8qywHrtNYk0qs3v46ReOLuFizK4iUYKlxvkF9CEISAa26JZGPCrL3VvdNcUfBsiKoXVPu9730


Chunk 602:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: izK4iUYKlxvkF9CEISAa26JZGPCrL3VvdNcUfBsiKoXVPu9730v3Xfffe2/rBlYUr+yuA6zs30Wl7cj6r4+eRoMPbi/rBHis8Ymue/KEMIS6rckqI8+jEeMB8x1XBzcy45JjDju6gwCjAdGH0bIf+xjHyv9W6s8jysKS3u/Li+6nXnmmd9u/77CsKSHdnmBIKh66/0jSN8RKjhubTiWAYvAEsIsidXlBlQP6Y03ZherF8HrCGwjYri8LfFrm4jwP//5z2XVPw+Tbc7Vp/0sv0oU6BxYCmsKXMeaiBxJpDiiIwZcY+fbp3ef3Fb9S9tpE94FXgWiQfvwDBADLL2VeU+uLkAirGNj51zmiGVpoX20Y233wNtwv82dO6+QtT6nxh0YrnSfEQK8j3VYalnbz73rvvViM7Ee//mf/1lmgUy4ckK64sor0pU5GfIxLEroqYf4GEbA6gzt4fkzdGfJaB6WGCZYfbFGiAEPK/JHphJPABXPOqTgKejVpYPzgEiIi


Chunk 603:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: zdGfJaB6WGCZYfbFGiAEPK/JHphJPABXPOqTgKejVpYPzgEiIixscyelQLLMKrNkNsurnLfDiF8FgOnj5q5uUyHF+hx12WPlEetVKXpPgmhECyJwYMh6M4AgCYoB1xEpjobkHuIsRYo2xsL8Ok1VXO+o1FSSwduE1qwGq2mxpoF3cWzxS2rTeT561KrRbHdrBvWWYwT3oXuSVJNgtemS7Z5PhsazPovav97q2J3qfePyJ9NGDP1os6b322jPf57uXuCjlu076idVdDBCX9ZwYLJ7jwOqLNUIM6MiM5YkZ4AmQBBKK0De+hzhWFyJQD51FtWYnT55cOu3qIdAZG1dD+KZAVvLnAdAZGP5AgDoJyfk5/9Xl/FY0tJXz05GwiHSUggx1ztpGu/gUj+G36jkhBnxqO/dGHd9dU+EFQN6UOXXKlGJhEppLcjeDexCxsNgEX3pmxF3Ul3FpO+1PiHruqtdtTQCSYjSIAzCW7XxZ24jLebp3FieEbCe8BCRqZ4k


Chunk 604:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: +1PiHruqtdtTQCSYjSIAzCW7XxZ24jLebp3FieEbCe8BCRqZ4k4cM9BvV+XVhR4du3j+O5VZT3x5BPpi1/8Ygkudl088559wtczLx9hEAisKKwRYsBDwWV+8sknl6h8U/X8LbhueYjSQ+94OgapWkq1U3y/5daHnxfDmDiSQ2bK1vHqnGuQkk+dwF65kyZuBMgRAdxuAqSW5/w+CGg7aVnaVafK3cj60lEi/SqgeAPcAzVATDmgHX3XKddpY2uyi1Ibagvi0pr/vte26AjtSxhpU8R/yCGHlHcDHHXUUWnMmDGLyIf1SWi98cbr5f6sbdqV4dydE1I19VBf8ZGPfKQ8W7u0D7u53wipKtCXBO3u/jM05b0dBIK2JSocR1lL+3y6/93jpoMKmD3wwANLPT3j2l5yzQgBIk65jm8fgsRzoS+pBpJ6+N2zYbvkuyEJn/VZrK8ot5/fqkfJ8Wzjmaxl+12Z+i7QTu61elxeE2XZrmxlaBP3T/UUBFZPrBFig


Chunk 605:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: J8Wzjmaxl+12Z+i7QTu61elxeE2XZrmxlaBP3T/UUBFZPrBFigNXHXY4ot8odGMWPBLovw4PYEW5kb0NzY3sA3PC+e/DUwY29LA96M6xx4OVI6ujh4W70wHiAvPN9yEaNKU3VO2D6oIeP6JHKokmZ2N7v8T9IaDtWFeLu2K5eSLP22o2lipvhHC0Qpc10VHWmgY5aJ+Me0Lkp23Wr8N11Iq4kbdfV2mtp4d7RYROXddpqc1s0w32DTHjSzGCx8A9RgGh03khk4EDTNjdL2267Xb7XGgv3uFe1d1eGc3Ouxx13XDEeDjjggDKUSPhss/XWZTqidSvcK87ZvbYkAaSN/V6J07CfmQ2EBCHvGXYPLs19p6zOxIDnxXCEVRa1PyNA3+F+d03k//a3v12WeBZrYHl2/aF+0O9ijgQhS9Y6IRYt1gbKOOecc8oryi1dbX+/EROeTfEnP/7xj0vZpkQLBpyd6yBw2TkZlrKPReC8odUQ1dVXX11Eu9+JhJ///Oe


Chunk 606:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 0vZpkQLBpyd6yBw2TkZlrKPReC8odUQ1dVXX11Eu9+JhJ///Oel7oYK9Gtr6jPY1bFGiAHEqLM3PEA1e8nNstbBA60DpfA9DDpUsxXc2OYweyhM1fLA3H777SWAyGt93eyOVcRHJrGlOao89tEpq7tj6kSK9ZU7dWVtsMHg8q72QTqTnFf+5tRVgI5m53YleAS16TC0qWTNBe36pz/9qVxPwZL33DOpLCSlXaFaODpU33WYrhPi09HVzrijEADt5J33W2+zdQnMRIDafE0E8jFTxQwC9ykCWRy0A2/Txz/+8TIswIIVY3DZZZelq666qpCH6WDeFMh1jdSqBSj53hXhHjIc4rx5A3je3GtWLvQyMf2JdkRYdYaOe0y7Lg3cf0QBoUvc65t4pbSve/i9nttmMYCcES1i95wgcYKXOFA/vcgLL8xIDzxwf7neRAyPjnNybPX2TJiWbH/PmLo5f8n7WNwjVqnUp3m2PCN1mNUxnLc2gSOOOKIMp3iOX873w


Chunk 607:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: PmLo5f8n7WNwjVqnUp3m2PCN1mNUxnLc2gSOOOKIMp3iOX873wMb5d/uMGzcu/eEPfyiLODk2jyWPi/Mm9j3v+lLGjEDCmFq4+mKNEQPG/NzEHhaegqUB6liYrUsPrykwOgU3rwdHh3h3fgAfyJYWVetYyN8nle7FSE888WR5OLjZ1MPDrC6VvN4La2diIlwQmwfPp46WxbtettJ0whsMHlyWTu5q0BZI2xh2bVcJ4evYWLC1XRvt+Xh6zGduY5YVgaRNeA1cc+3KQu3RszEey3Jj+eigWEEsO2sN6JB06JWweFF23HGH0hHJ79qsiaieARale3dxhO38ETzLzoqdhAH3dkP0XlvWKCivA89E5Lnwu2fK9XQtWMqIpquBCGSZmt5GCLDYkS6iuu22W/N531eeb22ojapnDmEaeulMbHYGwtT9p+0k96n2Q4bv9ebTZjHAIKn3s/6s3vPKcv0ItSlTHl1EtgSHa64v8kw9+eRTJZ++2DV79tlnyvkTQgj


Chunk 608:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: vPKcv0ItSlTHl1EtgSHa64v8kw9+eRTJZ++2DV79tlnyvkTQgjbcIi6OrdH8/Ueno+1//77FXHoWerff0D+fWE5rmeMB4VHwrM5K98HQ4duWuItPMOEpD6Lx4U3g9AkprRDHT7Rnjww9gkxsHqiZcWAB8+D/kJ++D0QlPg1WUHfklUy6wo5eYh0gEiN1S5R545J9fqdem9e8IQIUB9WxZLc+LZL1HUdV/PgONbcN+emnr16FvLyylWfXeUB0q46JRYkq0S7skxYL412faKQPELp2K7amkdgxgsvlHYltOT1m7Y0w6J/7hB1kK6zzlLHwyoSI4LwubrrGKgX8vTv3y/tnH/fKedbk8WAzth96V4kuHTEnZEX0tBuZtqIC+DORSY8Ajp2ZOjaef2x8rS7/MjMdXBdDNF0JdTnjPfw6KOPLlYxS5YnhHB69NEphSynT38+32+NmTvuL15G+/IaeLb1PUsDJOsZcF/7dI+6ZxHrkp7jZjGAQA3fIGeBw+qiD


Chunk 609:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: G+/IaeLb1PUsDJOsZcF/7dI+6ZxHrkp7jZjGAQA3fIGeBw+qiDCQrySeP+hNvhIv6OSYYbtt991FFEDgfz5U4BnEhI3jJ8nfX1n1yXxaAloU2zOpYyurRo2e5VwgAZehTXXvPtH5TWyJ/ZdvunD2DRCbir0Ny7hneFzN6Qgys3mhZMeCG5ppnpRoOMH/X9yczISvTg6lclpGHA5FQ1jpFRO/m9+AhNUIAafm0r4fIw+Dhfy9BUMUDK0JHThTUjlwZHiwP0JLKWV2gzuqOUHRo3IeGWFg5xaLIbaPddMzOTbvq4Ba1ayZqvysDmWuL5na1H7LXGeqgWDesluod8Lf2JDrsMz8TpE5PYJiOa00WA9oe8ej0Dce4nzojL22t49dpc9ny0NRrVAlP8nwQawsXLEzbbb9daW+/s54l16mrwD2hTyB+TMd1DxCpvCFEgbYSH+R5dp8hOOSlT3FfahvtavvSwvWQHxlym+uX3PNL8hjap4oBBCquQZ/GUtf+nhX


Chunk 610:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 3FfahvtavvSwvWQHxlym+uX3PNL8hjap4oBBCquQZ/GUtf+nhXPjHJcGyKAVa4/8ipoog1JE8UEjxVYkbk6EMjO2+yEwXmbetS6aAvPkHryRsrnN5/6nNdffyMLjwfLfUIM6IvEkiB/+fTD6k1IIHxt7Vlt7NtYEdN9RxSFGFh90ZJLQenskAX3NeugBtSwSE3RcuO6sUUYGys79thj02c+85l0wgknlAeUq81v8iAnHWMt75JLLy1LDRMWDbf/kjsQD4aOB1kZy1OeB8y+OgQdrweqs459dYPOD4HrzAQreavk7blNELr6OzdE1LFdJd+PPPLI4o4sFn7u9Fj22tW1cY0EL/nOK2MJ5kG5c9R2xIFPHZ9jEGG1o2sV6Ljdt8hAp8sK7AyEZxVf7qs6LQ75dwRyrALVfapMCfl0pQ5dXZETsUjc83zwglSCR8JQCdyz57lzn9U21W7Lek/x1lRBW0XG+4FrSwTUWTHaH9SH4bHFFpsXj8dnP/vZkjxLg


Chunk 611:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Lek/x1lRBW0XG+4FrSwTUWTHaH9SH4bHFFpsXj8dnP/vZkjxLgkOb8y4rCEvX3jDEZZc1+kcCRBvV9gqsWVhjesylvUkRtweU9cQyEB/AQ2C7B05HSj0jqFNOOaWkz33ucyXy2PLGHra6zUNnGWHRxzoZD78FSHgZvFJVkOFLL71Yti8JlLaOirpHlghNB6BOSPS99l8doJ461+oR0LZFBOS661ARvLYiprSdNvz85z9f2lTb+l7bW4S78ewd8j6uibJZNgKdtK1rx42dL3jp6GtqZSAGFpn7SNvVaYFLgufFPba456b83vZ2cKZjdAUPVUfU+8N7DrxJESkTP869Myxc2JilghCRKUvc87is0Gae3YbnYfHHW1o03+fq0/BSdm+PEWh4OMCxDG0gcwT+fsAzxwtgeI/3iGeD4BCkuN56g8p5vfDC9BVyXoHVA11SDOiUavJwuBndlCydovTb83WEfB5yMQKIBamwjHR2XMjciJ/4xCfSiSeeWAKN6li


Chunk 612:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: CydovTb83WEfB5yMQKIBamwjHR2XMjciJ/4xCfSiSeeWAKN6lifDlC5koeQBcqSNz+bt8DbEo1bIy55kGCNkOfKU68lwTmsn8vcdbfdimUswEd5CJQ1oJOvncDqCufImjKGedOfbirWprbQJs6F1U9gEVr+ZrVoSx2upI21tTbneTkh5/XJFenauEZiCHhfbsrXzjW0X0ciWxyxtQIQl4BTopL3hSBw73hOKlwTz4q2Y+3q2AWLdUZ2rGnXpHoZ7Mcd3BXEaTOKqMnPfqPujZU8EalXLXeG7t0bAkD7ONfl8cw5rjKqoOoMtrsePBLihnxqa/e78XjE7Lo1w3U1hLjppsPSzJmvFle/mQeeC+LAPoSAYUweDp4JieFDNCjP346nT6wxUPZRZ3mIe3/ztHkGbZfmzZubxfmzxf1fZ/4oyzPvu3Mx1OfYtmk/3xlhPpURWP3Q5WIGPDgeFG4r5OOmh759+5Ugm6GZPFnpxYIpv7wNN+lzzz2frr32mjKlj


Chunk 613:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: eFG4r5OOmh759+5Ugm6GZPFnpxYIpv7wNN+lzzz2frr32mjKljavQTctyNdYlYKda+joMD4qHy00v+U6Bu/F1ngjNOF11x7npPcgCAF+b9VrqljsbZFgjk5fULsrzgA/MdSc2DEEY80OO9l+dLTIPN1FFYAkW/Mu9fykdoPNB5kSTRAQ4T52O4CdeBN4TEfDaVUfhPAkg11I7yK9duXZ15qWzyh0rgjLO6Rj2AR1crYdOdF7+u1ViBgDpm97aL5+zDvzN9k5em1Uycv48T4SCNH9+IwJd8iyA+4zF6b7eY489S7Ch50HHT0ALrl0csa2O0C4i+Q0luR/dO57Txx9/LLfN24GB8tXhBDEV+gH3HE+X+0r7LOt5a2+BeYS94yu/Izw/jqOPqbNt1M997Xg1Bok4qVhrLWP6jfse4eqrXEOfCN+18/woh4XPKEH6rr3+RL2IhiuuuKL0o45TZ47Ioz7KkTyXID9vg7+Vpc+zr7Jt035iDghIfa38zsfzrT9


Chunk 614:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 45TZ47Ioz7KkTyXID9vg7+Vpc+zr7Jt035iDghIfa38zsfzrT9Qvjr5fU1+DrsqupwYAA8vInfjU6LEgQVrPByCY9bPnT7l32wRAbV73333Fhc+IjJfVj5jgoQA61UgmhubBSrCmluacEAwtnGZVeJSPkvM/h4wnYXfCBQPgwdA+chNWhKh26qj8DAaKiBOfFYLenUWAs5buwjE9PBrG3UWgHXkkWPT2LFHloAmpE7E1eh1AUk8KFya9kcyyMu+Ogydsk8kr6MjwhCbv7UVIaZzQXxgu86rVcUAWJNC27hvtMvsTHblGZnbeMGT6+V39yvBiRjdp54N18d9hnS0e0Mgf7x8IhxCwHODYNz7XSU5p5rcB55XRMYwcM7Ov563+4mHkIdO0B6i88ZM9562k6+zY3SWPO+ugWEbYkCbOm5HVDFAGAt+9Z1w0K+pk2vpb31JRbdujWEhRobf9TuuEaGtPJ5Nv/nbM6HPJChce+fvuiN2njz1chzb/KY8ZXueP


Chunk 615:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: f9TuuEaGtPJ5Nv/nbM6HPJChce+fvuiN2njz1chzb/KY8ZXuePI+eJd5Ogtx25Tg/wYDKd1+pm3MlBjy3jqd/JiC1md8ZXc7D7521Q+CDRZd7hTEgH0Ti1bbcY246x3QTW17UuLS5xM0Pj5OU73e//W0J8BOFi7CRryVY/9t/+29FCHjolfmjH/2oCA6dZLUcKhzLA8RyP+7Tn05js4igpi1G5K1jAuc8OIjH1CDDDoYf1kQy0mFw/11yySXlbWuIXWdkfv/JJ52UTjjhxNJOr702q4gwq5VVy6dju+pgdDRE0Ne+9rXi6tZB6VD+7//9v8XrQBTokHSuYjY+ffzxjSlgeX9CgWD77ne/W1ym1uofmq/vSSefnI7P14kl3Gxdrcl4K3fQSOWe3NZWjzP2SxR4dlhoIsc/ndvEGLDrQMQJpvWMAKHgGZK0mQDOGhCqnetCWF0BRQS1C6GvfPnL5byREiJ0T7LICXjPMOFTh6fc1+5XK/BpA/fnspyz/O7


Chunk 616:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 6GvfPnL5byREiJ0T7LICXjPMOFTh6fc1+5XK/BpA/fnspyz/O7f//N//k/pY/Q1iyPBKtR8ShWOV4/b2bFr/rovyFe9ZZ39VsuzrfkZrL9JUPfz2Rnk83stG5r3r/vW3zuWH1g6NLfvykSXFANuMGTCfYeEPNQ6Og/a0KGbFFX/qU8dUx7qeuMhfm7+//iP/0iXX3FFmp6VLuLx0Hv//TGZWDxAV2eLFakZ7y8u6fYHoZ5T/btar2YAnHbaaaVTRYwC5yzvSXU7pk7V3F4dEJJb08gI4XD5/+Y3vylkwpJyjptkgj77O99JH87Xgltfe/7sZz9reGSytaGtoLnjkFwD++s8BRb6NKvgkkxEOmb7uy46VhbX3/7t13O7blOuB+HG0/CDH/ygiAJ1M2z0uc9+tghEYq96EVoB7lVtwmpEeJ4RrlvXi0XHAvZOAvcwi9HvSFFbIjHk6JkSUa7tiTJCwjPDgoZV1VEtD9xrM9oFK6+hGB/vMPH86kdYve4V7


Chunk 617:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: SUa7tiTJCwjPDgoZV1VEtD9xrM9oFK6+hGB/vMPH86kdYve4V7nj3FQvWsAChQAT5TV/C2FgWMS+vNjTMQrASsSu7bwyseQgx8B4wh/zh3KmxFo17sWxY9T16rJOsqS6IijWOADzkXG5ccCz3mrd6BUS36xhZRb/+9a/TeeedVzoJF4F7n4u7zvXXISK84ipda+3iSjz2uGNLGY7FDX7WWWeVT+5ybjMvFlIXY+ZrohhANFypRJl24QHRnmeffXax4I0Znn/++YVQuCwBmRjzJ5CQliECnpj6Oze2mRss/2H5GjiG68IjJI8OG4l95StfKXEVOl6dOoGoEzeWCY5hhoL2dw2XpTNfE8AiJkrNvnCttB13v+dGe/HauD99VsvV8ysvL4xnxlCZZ8O9Lq/xdEIauoIYMAxAgBIDxKL+AEHzPLkn/E2Uug+NhXu+5SvepddfLy53sUTu2WpxLw2qsLWfz7CIA+8HIQbeAxpIh8XKMQaNBBCGh9dDiIR1cPW


Chunk 618:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: WpxLw2qsLWfz7CIA+8HIQbeAxpIh8XKMQaNBBCGh9dDiIR1cPWlI3U8n4Xqk7WAvM1396pQD6yH31rdrAHle4BZn4LfuA9NS/rzn28r1j9XN0uKBWUo4O///u9LoNCUbHn98z/9U+lAdabKMA5nhoAxbp1tV+hAlwauc21Xlnj1hrCCLO7yd9/8ZtokE4jr82//9m9FLLBU6xCLIR3k4lq4dl5yguwJN9cQgfMOcGmzWLm7eR8IsoYXaGhZ/lSHrqM1ZipegLAg9ggG7f6FL3yhuLu5hluxQ64ucmto8KoQSsTANddcndvptfKs8GAZRjHOLC/ylM91IfCIKgTK60YcN3t0Vneo4xzGQD6XcRdckO7M96pt+gbnTXjyGLnvPLPEq3vMObp/Tj75s7mvGF5Ew7L0bfJKlh03rBIIvB+sqmesy4oB0GkhF0oeiVD+YgEQhweZW84DzBqUF3ERCz79zmo1d/bUU08t5CMAjvUpYNDvOkm/cfN7iYcIXgGIx


Chunk 619:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: UF3ERCz79zmo1d/bUU08t5CMAjvUpYNDvOkm/cfN7iYcIXgGIxk55I3QaOhEdyv/+3/+7iIbnsrX1Hz/5SREUrClExwXbr58Am16rrG1WFdyoc+ea1tl4pXANUDKe/6W/+qsSwGexoO9///slqlkelryATe266667FctNZDfvwk9/+tNC6solFkxH9KndCDV5lON6Oha3dXX9a2v3A0HiehJ7rDp1KaIw16XVu2Tt5hkg3AgnsQSGDghr7UncFvLMFjJy1O6eE9fAUFhdmrirwXkTQrwihDqjgJjXN7h/3C/uQ+3gOd944yFl4R4egVGjdi/Lg+sTgtIDqxqrSgx0ydkEFY7DQmT5szY91Fzzgvc8+IjHJysRSejgPPDgwTYGiCSof39zU7OEWLrK1jEabuDeJwx0lspmNUjGXXUijs1q8vrkN3JHa4jg4YceTrNenZXmzW/MKnBsdVCXNSk5p9mzG++6d9PqXFmXorZ3HzWqtBmBpvNF8jpl7maCQLs


Chunk 620:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: VCXNSk5p9mzG++6d9PqXFmXorZ3HzWqtBmBpvNF8jpl7maCQLsOG9aYZaF95BOE6ZqB2QjyIXXlujYEWC1H8FNtV8l+6lH35akhBhzHPeIVyK2O+szUaWS8Ku5tHixt55pqV54y15AI4OWxLG5Z82IZrePVBeqsf3BvOg8R7TxHnnv9hHOvXjzDIIceelgaM6bhBXHvlIDJ9rICgTURXdozAAhI58Xa4R5GJtQ/YkcarBvEIbEYdXbV4mHtGyZg/eskbs6Exfr84x/+UMrVCegI69QgpIXoWRbc4sRA35xnzz32SGeccUYRBOrwve99rwwlcDmqm06mTr3pih3pkqCdtKv2R8zdu3Uvne1hHzss/fe/+7vyqlMu6X/5l38psRoIG/mwuhCNIQBiCclfddWEdOWVE0pZOmmBXoYJDMPY5tpceuml5drmhnzbM5A/5UdyBJzjC3JDYD6HZPHhvQdrWtsvD8rzgPyzYJ0x44USP8BS9nxoS+1IEHChG17xL


Chunk 621:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: HZPHhvQdrWtsvD8rzgPyzYJ0x44USP8BS9nxoS+1IEHChG17xLGhvv3VleB5Z/0S9c/YM16nAzk/An/vTeesTeBc994HAB4VV5Rno8mIANJbO7Q1BT88+W8Y6JQ874tcBSDq7Sffck5584omyvY45WwpXTIB9fn3++WX6oWlFOr4RI7bKQmCHYmn625CEWQkCsRxTx2Fs+/Nf+EKxIv6Sy//Od75TxIJxV50q16pAOZ0rC3dVXdyVDdd5/vwFuWN9ucRucDcj7Q3W3yDtudeeZUrVttttV14DbfgFket8tSOr1PCKdnFtXJMHcru6BsSBdrW6o1kY2o9YUAYhJtLb1EWkv8fuu5dOW4dNDLD2WLu8CXWdgjVRhK0IuAu9iMiKcp6HaiFrK22p3QjZKgLWlDb03LrH6jkTArY5R+fqnJ27Noj7JvBBI8TA+0C1UrmMDRf49NDXh9/ynuIKjJXyGgzoPyCN2n1UGVM2dUgeFr2X7PAusJJ0CgiGBeq8lMu


Chunk 622:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: uIKjJXyGgzoPyCN2n1UGVM2dUgeFr2X7PAusJJ0CgiGBeq8lMuSUCai4UoV9X7AmDHFlSh24Z/+6Z+KaNDZIDJBV6xg5KWDWZOgveuqjs7dEAtrirv1zDPPLBHb2sHaDb/NIkt0OpGkLQklRO66aetXZ2YLbf684kUxPEMImO1BQInnsD6E6/JaFhwbD9k47bPP6DJFjLDQkRMEOnLWnWumHvYNBAKBrooQAysYyBtRIXpT3LiaWaQsfkT9hWzZsyittsWtTTQgdIKijocD0kHops+xau3LM8CSNeY9bty4Mo2uBidxg5vjLg/X45ooBggrAZPIHllr1wED+qdTTvliCdBkpfMICLrUttpJ+1SLDNwz2oY72vvWj/nUp4oQ0GZ1yqfod9dwnXV6lFiPY445tgiGTYZuEuO5gUBgjcSqEgNdOoBwWaBOiIeVilAQGGsUISF75I54uJYLaffoUSxNbsPqOmSxcvWz9k2NM8QgpsA+yM0UOpHzdRaBvPIJZ


Chunk 623:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: LaffoUSxNbsPqOmSxcvWz9k2NM8QgpsA+yM0UOpHzdRaBvPIJZBMIp5zVsW2WB86H6DE84LzFSWhX5y+GQLtqH0GDUnW/2senNiEAtBVhZu2AQ3ObHnbooWUYx3US8W754scee7xcPx4Zwzra3rVwbQKBQCDw/tEyYgDqmKB4ACTD5S+YCGkJbDMNkYtZwJQ518gJkUm+iwkQlMbtTwiYhYCYxCaweC12Y/0BZfbMBGVN/DE5r2A5AsPx1zRUMUBoEQTaVnsQA1UUIH3DAWZvGCrRlrwwyN4nDwvRpF291XD//fYrAkFsACGgXb27gJeGIHMdBGvyyrheEeAVCAQCy4eWEwMsU25sRGXsWlQxMeBvyW+ICMkjMOPR3Nzc0qaoiREwCwGxgTiEZiFAYCDHzbfYIh04ZkwhLeRl/HpNheuN8H0SBLwDNUK7Y7uKvdC2hEBtV1a+tPXW25T2JtgebV87gkegLmHs+hESpn4RAgIQ10RvSyAQCKxqtJQYqKT


Chunk 624:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 5T2JtgebV87gkegLmHs+hESpn4RAgIQ10RvSyAQCKxqtJQYqKTFA1At2ZcyySArngFWrYTQ/S1P794NbwEC80lMEA9iC5C/BYoEHXrjGMJSPs8Cj4AFS+o67ms6YTlvbWPBljfeeL3MtuAVYM1rlzpt7e127f2Odu3Vq3cWD3OLN8D0TQKLGLC2gH2NmxEQphkSArwtggRDCAQCgcDyo2UCCDvCFDYR7sb4vYiFh4D1qu6sV9Y/IjcmjdyRF0IiBAwx2N8UQ1PqEB6CY6UKJLQuvxkGe+65V/nb+xJaAfPmzS9DBHfccXtZOphYIq6IAla9djUkYM0Gy9tqm+ox0a7yWqeBJ0DwJvGgXQkNsQe8LGZ9iMFwXQKBQGBNR8wmWMnwJjzEdfuf/5wuvPDCYt2zYKsgkBCY8WhufwlMP5TkQ1QuVL1YAuSMe3/5y18uwwks3rpfq0DbIHEzBrw90DROMwma27W2qfZF9Npvce0qHzHGy+INe3vsuWfaaMMNy


Chunk 625:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: zBrw90DROMwma27W2qfZF9Npvce0qHzHGy+INe3vsuWfaaMMNy76BQCCwpqPyy8pGy4oBsOAIa9Q0Q8vlelOZefCVvCoqiUElKbANWRkGEAXPYjWv3lg2Fzai6yptsaKgbbxR0rLMPCcCALWrYRQeFV4CeWq7dNauQDCIH+CdqW/KE9RpWMFwRCAQCLQCmvvFlYmWFgOFuDI5GccWCChi3UtwiAMxATwF4gqIBlara9K9e2NhGy5vIsA4NpLiCRAMZ7EdMQnIrNWEQIV2Zd1z/XP7a1Ntq13FEtT4AdMEFyzgMWiQP5K3EBHCN+tA4KV2NS3TjA4zCYivVm3XQCDQeggxsAqBuBC+sX9eAW90s7SuqHixBASByHgQF1AJi+WK/I1/Ewa2EwpBVg24ibWrmQW8AoSBNR6qIGgIrXlFDGhXZC82oM7gMHNAXIF2JRRiaCAQCLQaQgx8ANDoSJ+ngDDwyXqVeBCcF9c/whf4xgNgupzUikMCSwtiq3pgCAC


Chunk 626:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: NDoSJ+ngDDwyXqVeBCcF9c/whf4xgNgupzUikMCSwtiq3pgCACfvAZ1XXhobleiQJtqXyIhREAgEGhVhBj4AFEb3yciEz9QtyEmaS2f7ecZImDpUNvwrfzZlttV20qgDQ0VlLaNdg0EAoGCEAOrEd51MfL5BU0tH0qLdmjXIP9AIBB4J0IMBAKBQCDQ4lhVYiAGYwOBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFocIQYCgUAgEGhxhBgIBAKBQKDFEWIgEAgEAoEWR4iBQCAQCARaHCEGAoFAIBBocYQYCAQCgUCgxRHLEb9PeJOh1xt7Ne/GG2+c1l133fKGvdURXgY0e/bsUldvY/R6YG8I9GKgWDo6EAgEVl/EuwlWMryJ0OtzvUbX63UXh+7du5d36UvIE+R/+umn0x133JFuuummNHbs2DRy5Mi0/vrrl99XN6jvs88+W+o6ZcqU9LnPfS5tuOGGRRDE64EDgUBg9UWIgZWMF198M


Chunk 627:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: vs88+W+o6ZcqU9LnPfS5tuOGGRRDE64EDgUBg9UWIgZWMF198MT366KPpwQcfTE8++WT71ndjk002STvssEPafvvt0wYbbFC2TZs2LU2cODH94Q9/SA899FD627/923TooYemoUOHlt9XNyxYsCDNmDEj/eUvf0lPPTUtHXHEx9OgQYOKwAnPQCAQCKy+iBcVrWRoYCT56quvFkFw2WWXpXvuuaeQ5uuvv17c6c8880z605/+lC6//PJsVd9crGv7cLvzKhgqqH+vzuDR4LXYa68PpSOPPLKIGkMayyIEeBdefvnldP/99xchtSRvSiAQCAS6FlpWDPTt2zdtueWWxb3P6kfqW2yxRRozZkwhzCOOOCIddNBBqX///mny5AfTxInXFxc7EbDeeuuVcfcRI0a0l7Z6A+kj/0GD1ksbbzykfF/W4QHi6JFHHkm33XZbeu6554oQCgQCgcCagW5nnnnmt9u/rzB0BdfzOuuskwYMGJD69etXPAH33ntv+vC


Chunk 628:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: cCagW5nnnnmt9u/rzB0BdfzOuuskwYMGJD69etXPAH33ntv+vCHP5w++tGPpl133bUIA25/3oFHHnm4WMXDhw8vIsB+vAHPP/98uvvuu9Po0aOLMBBfIKhQeTwP2oHIELw3a9asEp9gOyKWfEeqiNbv8sljv/o78WH7nDlzijXukzfjjTfeKOdR86qPbX6X33f1sA/PgHrYJokVqHWr+cVQyFu9IuoBypaHELrllluKZ4CXQcBkBCEGAoHAmoGWFQMViPDxxx9Pf/7zn4uHAKn36dOnkCMLeurUqempp54q57TLLruU3421I/2HH354kRjYKm8nGJTDgkbO9q/lG683BKGc3r17FzGCcMUfTJ48ucQeOI48NWAROZsBILZBPoJB4CLh8thjj5Wy5FOW4yhHGfI65hNPPFGIfeDAgaW+frff5ptvXvZVnjyIXl1mzpxZ6qH+L730UsmD8JV34403pquvvrrUj+jhWSGknEsNrAwEAoFA10SIgXayvvnmm


Chunk 629:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: D8JV34403pquvvrrUj+jhWSGknEsNrAwEAoFA10SIgXayvvnmm8sUQVYvEkaekkBBBLjtttumffbZpwQUVg9AsxgYvuWW6Y7bb0///M//nMaNG1csamPzCBaJ/uxnPyskTmjUSP7p06en66+/Pl100cXpiivEJdyU7rzzzuJ5QODI+tZbb02/+c1v0g033FA8EWYw/O53v0tXTbgqLXxrYamvMgVB/vznP0/XXHNN2UdZPo3v83IQI7/4xS/ShAkT0tFHH12uEeK/5JJL0oUXXljqKbjwssvGLwqM5JnYdNNNSzmXXnppObYhAgKJANhoo41KIkYCgUAg0HURYiCLAdZyJUCW/VVXXZXJ+YoSOIgQ99xzz3TggQcWQYB4EX1HMbDNNtuUIYTBgweXcfWtt9560TYCYl4WGKxvcQpEB3f8tddemy3wl3P5e5Q4he22266Uxy0vL6sdqV933XWlnsrZf/8Pp8MOOyz17NWz/KY+8rPclYesDWeY7vjZz34


Chunk 630:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: V933XWlnsrZf/8Pp8MOOyz17NWz/KY+8rPclYesDWeY7vjZz342jRo1qgiCiy66qAgKFv0JJ5xQ6k8YOM/77ruvtMGQIUNy+funnXbaqQgRdTn44IPTjjvuWDwQPB+ExUknnZQOP/zwd7RHIBAIBLouohdvB5I0DHDIIYeko446qpCpAEIkeO+996WJE28ohMlr0BkIIFYyguQ6r4KIK32ToUPT0EzkdV4/omfJEyATJlxZPlnoPAgEAHe+4QHlIPbNNtusCAHEvv/++6UxYw5Ixx13XIknUI7yuP5Z8fIRIQIjd99990Lkw4YNK/Xye0W//v0LsSufOCEADsjl7rvvvmUfQZI8F8SQ35VhOiKxQ7TYV9xADBEEAoFA10eIgQzEzS3PKmZNf/WrXy3py1/+ctptt93SjBkvZKu74X43ZLA4QbC0ELgnCNBYPYuflc6iZ+EjcMdE6AgcSYtlQPbWO9g8k7C6CnI0DKEuBIe/xTPIh7wNNRAeBAgRoFzei


Chunk 631:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: cSYtlQPbWO9g8k7C6CnI0DKEuBIe/xTPIh7wNNRAeBAgRoFzeior1MpEjfeXbhwgwDGJfsQ5SIBAIBFoDIQaaQBQgUNauuADWMe8AkYBUrUfAckfgywvHQeKf/OQn07e//e30ox/9KP3kJz8p6YwzziizGrjt5VOvRalpf5a5MozZ13z1syM6bq/lNX9fu2lbIBAIBFoHIQaWAARqTJy73uwCQXai85fXM9AMZTkGK19goU/DEeIRjPUvCUSLfaVw1wcCgUDg/SLEwHvgrrvuSrfccnMhZmQtel8cweLAo8BFj+SN6RsOeHratOJReOCB+0sw4O233168C8brzd2fNGlSKd+aA2IFBOoZMiBATFEUE2AogRixn+A+8/2tGWAcX6wCz8XKhHO2ABOBpH6mOFqRkTgKBAKBQNdGy84mQGKC70TMW3L4vnvvK0Quyp9l/sADD5R0xeWXFyIXxPeRj3ykjLEjeQRuWqAZCIYTCAVue7/ZH2FLyN1xDDFMn/5


Chunk 632:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: yIXxPeRj3ykjLEjeQRuWqAZCIYTCAVue7/ZH2FLyN1xDDFMn/5CCcCrY/tc/DwAjikWQT77CtozRCCoEfHWgELW/4svzkj33POXMmMBOX/oQx8q4kNdzRYQ9Ie41UVsAfI2dZLoMFNCOergGqm76YLWMSA65Ncujic+wn5iCIgNv/tbUj/HkN+xI74gEAgEujZaVgyw2BEvCxsZmrPfvVv3NOu1WcXitdCOJLBvo0zMBxzQiLRHjqx40/EQ/NqZoJG61QqRZh1SsGgPa5717m+kzKsgyl+q0xDr0AMxYB9iwvRD0foIF7kTDERBt25r53rPKqJhxowX08c+dlgJNuRRsIaBBYcQNYJWtiEHHoPHH38in+d9RQj4Wx4CxrlZYZHnQf0ICKRvPQMCiDiSX13VnbjRJshfOwhMtD2GKAKBQKBro2XfWoiEERtBgBSXBEQvkt/0OsF68rPELb7jXG2rqxey5hE4sYDcESqS9Skpw/Q/AgKJK4e1rT5+5/bnf


Chunk 633:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: ELb7jXG2rqxey5hE4sYDcESqS9Skpw/Q/AgKJK4e1rT5+5/bnfZAH+bLmLfhDLFjvwP4EAwL2/gSkrz7EABI3POF48hEUAweum8XDE0W4KAPU2TF4JAgEgsU2+9iXUCI4bCMGzDLgqbDNcYgUQxzKJyoCgUAgsHLAeFsVaFkxoIFZwTUtCaxm5OsT6j4sfnC+NQ8o22/1Itb28Fnz+qx1qOWA7crxSVQQA+PHjy95P/e5z5UFjnz3O/GgrFqG7fU32+txan2lCseo+SXoWC+ox6nlOE7H8gOBQCCwclD755WNlhUDqztY64YvrCxopUBteswxx5RFkXgNAoFAILDmY1WJgZhNsJpCHAH3v2RmgaEBixQZyw8EAoFAYEUixMBqCm584/dWIvRehDFjxpQx+pU9hTAQCAQCrYcYJggEAoFAYDVFDBMEAoFAIBBYJQgxEAgEAoFAiyPEQCAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQIsjxEA


Chunk 634:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: CAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OWGcgEAi8L5SOo7M50Pn51wO8Y350+7bF4R1lLW3eTvL5LXqfwJqEWGcgEAisttA96aTKZ/PfzakpT/7i38ViUf4O3zuF36W33np7n7K5fbs8gUBgmRCegUAgsMx4K5NuRyDnAhZ7ex9QtrV/929nfUNHEm/O43vzHv6W1xs0fXpzZi2/GR33CwS6KtznqwIhBgKBwFJDZ7G4zqlY6vm3QsRIOqO8WjtvL+Kh/TeoZfiUB7nDWms1Xo0Na6/9tqior8xeu74mXJnt+9T8OfOi/J1h7SX8FgisrggxEAgEVkvoMBaReSbkBQsWpLlz55bP+fPnl+Q7sva2Tck2f9vPZ/1OCNjX3+AFXY3UPae1y3dk73OdddZJPXv1Sut07566N6X6m9Q9J393hhADga6IEAOBQGCVonY6nT2/RQC0k/jChQRAg/Rnz56dXn311fTSSy+V1


Chunk 635:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: IEAOBQGCVonY6nT2/RQC0k/jChQRAg/Rnz56dXn311fTSSy+V126/+uqsnGaW70j+zTffXPQKbsQPHcWA3xpioGHZI+1u3QmABtFXMdCzZ8/Ut2/f1Lt37/LZv3//kvw9cODAksrfffqkHj16NMSC/XPq1u5V6CgIyhl36Gyj/wqsTggxEAgElhv14a5P5GIf9tzh1DgAz6/8tRNqJu033nijEP7LL7+UXnzxxTR9+vT0/PPPl8+ZM19Ns2ZJs9Lrr7++SAAsXLig8C3LHXEj9s6EB5FBXLz22qwsIGaXPGuv3e4RyELA67uRvL/7ZMLv169f+ZS87rsmgmC99dZLgwYNav97QE79yrHt7/j1uKUeTXXw7R39V3tbBAIfFOqzsrIRYiAQWEOhE6kPN4u4+e+OKL+x1vN3T6+/q9ufdY/cX3755fTss8+mJ598Mj322GPpkUceSU888UR64YUXCokvWLAwk22D8Atxs+T79M5/9ykEjZwHDx5cvgOLHzk


Chunk 636:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: UR64YUXCokvWLAwk22D8Atxs+T79M5/9ykEjZwHDx5cvgOLHzkjet95EqZNm5YeeOCB9PDDDxdR4fcNNtggbbTRRoXYeRnUicBQvzffnFsER6PDXKsQfa9ePctxhg0bljbddNM0dOjQtPHGG5dtgwatX44vTxUGUiX9dwmBnGJ4IfBBonFvr3yEGAgE1lA0k38ltOZtHeG59ftbmVyRLssf8T/66KPp/vvvLwT91FNPpZeyKJiXiZvlX2MBECuS3WabbdJ2222ftt5667T55pulIUOGFPc9tz6BQChw39fjNX869pQpU9JVV12VLrzwwiIKiIADDjggHXHEEWn06NGlXo5HABAphiiIhuqlqGKFqJj5ysw0u93D0Csfd5MsCHbccce000475Tpul4YPH17q17dfv0V1glIf7dA+lFGDFm2Pni2wquEeXBUIMRAIrKHQidSHuzn6XqQ/S9xzivhZ5Cx/4/5ItJLpM888U8jV9ldeeaWQrmGCeVkAKA+xb


Chunk 637:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: /S9xzivhZ5Cx/4/5ItJLpM888U8jV9ldeeaWQrmGCeVkAKA+xb7zxkCwAtk277LJLJv/N0yabbFIscFZ8deOXwL5MtmXMvv24HaEe6sXTcHUWA7/+9a/T3XffnQYMGJD23XffdMwxx6RPfvKTi84BCAJiRJozpxGbMHv2GyVeQX2nPz89PfPsM+U8nsnnMf3558u58kSo34YbblgEwZZbDk9bbLFF2myzYcWDQByUoYx2MVBnRhQxEIIgsIoRYiAQCCwzasdRn8HyV96GRKUa+Y/UZ86cmWbMmFEs6ueee64Qv0/Jdr8TCfLbT9nIvHev3mmjIRsV65qlLfEEcOcTADwABEAh/nbyL7Wp/YI6dugjFubyiYWnswi57rrr0q9+9at0++23l9kBu+y8czrqqKPSKaeckvpncSAYsEAZuSz1Eu/wdoBjQ+AQBYY2DGM4RyJHevrpp8s24obHgijYZJOhxZNBFBheMLRA1AwYODD1yHUAbdksBjr2c0U45G2


Chunk 638:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: obHgijYZJOhxZNBFBheMLRA1AwYODD1yHUAbdksBjr2c0U45G21zZvRMW8gsLQIMRAIBJYJhRSRoZTJFYHXNOfNN9PsLAAQIEtf0B+LuXoAnst/v5gFAOu6kn8VEBKwqI37b7H5FmmPPfdIe+65ZxEErGkioI6/Vw/A4lBJsxkI3D7P5rpMnDgx/eIXv0h//vOf04K8fctM0AcffHD6yle+krYaMaJMLYTO+plKxI6hTOdhOMM5GVIgBAxFGPJ46KGHivCxXfBinz5Z5Gy0URYEW6Ztt902bbPN1kUcbJBFwcAsQogc4qSeW8fjl/Py2fhzEUqupegT3ztHoBURYiAQCCw1dBiIc24mfRY9gmMZ8wD4ZAmz/CVCwN/V8hf8RyyIA6jE7xlGepX4fLL8jbcfeOCBZRx/s802K/EANQBwUaeV913WPqAOE6jfDVkM/PKXvyxigDhhuRsq+OIXv5j222+/Mv6fD7bEY1TXfgWvAXGgPILI+QuCNBQhiYvgR


Chunk 639:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: q+OIXv5j222+/Mv6fD7bEY1TXfgWvAXGgPILI+QuCNBQhiYvgRQDeAufKO7DVVlulHXbYoSQeA2LI1EVTFjseA0ob1Lq11698z9veq6ONQMVAZwgxEAgElhpIHOk//vjjxc1u7N24OfKTiIMabEcA2FaD/2pn07HTQfAl0j4D4RsK+MhHPpKOPPLIIgqQpjzNz7vvdWjgHWgnwyX1DX57Mtf7mmuuSf/5n/9ZSFqQIMGx8847p8985jPp05/+dIkjqHXtjJBhUcCk1HRevkvO/bXcFjwFgiMNSdxxxx1lhgQB5bzFO5iiyPNhVgJBsOuuuxZviNgIoqSKpY4oR8916NimEKQfWBZ0dg+tDIQYCATWACC36dNfyIR2e/rJT35SxABSq+5+rvI6ZMBCbhYBncEzjOgq2REDgu322muvEtVvPF1cACivuSz7FkHQ/ndFEQ7t298hIvKnOADDG2IGEDNBQ9g4L+75YZsOS/t/eP8iBrjye/YQlGhIorHi4Dv


Chunk 640:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: ADDG2IGEDNBQ9g4L+75YZsOS/t/eP8iBrjye/YQlGhIorHi4DvKy2iufx2+cOxF+XJ11dvwwYsvziiegbvuuivdcsst6b777iteE23kHOu6BgQBQSRGgjDwd/WM1DoIlDSzojlosrRHo1KL2qSztm+u/4qG8yR8pk6dWq6d4Q9C54OGe5Q3SN0EpGrbOvV0RaEGxvL81CBXx+oqCDEQCATeE+XhzZ2FaPonnng8XX311emHP/xhCZhDpLUj6dihND+jHX9Dgn5vJlgEh4S33HLLQiQIGgE6BuvdZzOay/e9pkqazWXX7chZxy3Ij5jhySBe/Dag/4C0+Rabp/33379MB+zdu7EscbfuyDqT7trvjFNQJjJukHmPdoLO+fN3eRehTXzF/OIxefKpp9Kku+9Ot956a4mjcF7K1D4+EQjyRyYjsigQy7D++uu/Y8aEdkFmkvxVTBAMfvNZ83YjFrRBbYfy72KQ8xhKqUM+6kvgLQnqqX7q7nyuv/76dM899


Chunk 641:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Z83YjFrRBbYfy72KQ8xhKqUM+6kvgLQnqqX7q7nyuv/76dM8996Tdd9+9DPUYAvmgwXtFiInh0GaurzqvKGgvHibX1DNx6KGHpg996EOrhRBaWoQYCAQCS4QHt3YUiPP+bNFefPHFJfjOUEBzJ+KZlCoJI9iKtkyIuoGav6Ol7zsy5Z43Zo4Qa1l1QSKfFfJLzd6H2ic0i4Fal/pdXhZsiWGYM2eRF8N2vyNTBKce1RqXeAgQa7dM8vU4YB/1fpuIs4Do0bN4CNSNJ6Jt4YKcr2GxO4fnnns2PfjgQ4U4kG2tI9TzQebIBGkhMMdo1KP7oqEFv6mn7VVEmM7YWBGxf8kneddCQ6g0BIIjOV4Z/sjHqlfBNvV5JlvQyM1wBlGgjRyvYz21Iw+OYQ3nz9tx/vnnl/0Q4rHHHluGXlYkHFcbaqN6Tu8F9071DGiPFe0Z4BG4/PLL05VXXlk8A+JOnD9h21VQn6GVjRADgUAXhIe2uZMQFHjzTTeV+fkW7Wk


Chunk 642:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: lk8A+JOnD9h21VQn6GVjRADgUAXhIe2uZMQFHjzTTeV+fkW7WkmMrm44ZtJB0nVx1QXoCOvyb4SIgaf9kVuyEysgHyOLx/ybhYDfuMpQEjN3glQH8dWt+a6+LvWV1IGorN/TfWY0Ex+vlvtUB275+8wP+/71lsLy3kjQ0F/vfJnJSjlzZ49J82bvyD/1Qj4Uw40i5BaH8nftvvdefusdZJKPXIdtBFXPAEgP5LznUAgppC32Rc+/V23y0O0qCNxkwst5Zfj57K151NPPpluytf55ptvTndPmpRmZREomNMxav3lQ7DiO/ymPsSAe8OiUR/72MdWihhwDwjKdM7iLARhftBQn8suu6yIAV6Cv/qrvwoxsBh0O/PMM7/d/n2Fwc0bCARWPriZdRbc6zr8P/3pT0UYIJHyOzLMBNOvb79icUl1MSAWq1S/+0RGLOy2LBAWZjJtJrpCqjmfz0qQdbsya1IOkq/EWTuzmh9hyqcuBEZzver+ylQX5VSSq8Trb


Chunk 643:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: fz0qQdbsya1IOkq/EWTuzmh9hyqcuBEZzver+ylQX5VSSq8Trb6n2M8jT/utlMl0vl9cv798tb+vVfa2Seq7TPW/rlfr0yAJk7bdS97XeSmu3LUhvEjGZ1C1pXAMtkbz61RUTCRXJNsdRL995E9SltnOti7+LpyLvMz8LDSsjNpZxfqbEQLDM77333nTnnXeWT38jbjEKVTjZt8xWyGXWtst/lGERAkLsBvGgvtrnrLPOKkGdSO6jH/1omXlh5oNElJiNYWgFuORtZ4GvSEJ0rZ0r0jXMo36rA+G6v7QjDwQx4LwNj7jPAu9EiIFAoAvDs4aAiAHEIiLegkG2IZXios6W4YBMlsitQWRvW+D1WfUpf7XWCQEWZiXgSojIsFrXttf9K/xtOxFQPQPKsF256lBd5ki/WMHtxNecCiHm/I4p+W57LSufQeJU75kN6L69eqT1+uZy+3ZPA3qvndbtuVZOa6f1+nRP66/bI200aJ00tF/3tO7AddO6/fqmdfv


Chunk 644:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 3qvndbtuVZOa6f1+nRP66/bI200aJ00tF/3tO7AddO6/fqmdfv2SQP69U69+vVMffNnb8foloVC9yxsejcEkfrzeAhwq0KB5Vs8Jgu8fGnhonOrbSRpW8MR2snfr2SSfyMTkbKsjvj6642pnsjfdTKWL6jv/gceSA8+9FBZJdFxnK8ylKctiD7eAd+Va4gAsRsesiAT74L2rG3JKt94k00KKbsHnI84DAJE4GOzGHAu6kS0qJd4BNvqdah5tIHf5ZNHUs9ynq+8kiZPnrzIHU+ANHsGCIVGG8wuZSDnej3Fu7z++muljspzzsrUvsqVnKdrYVtpj5xHnWp5NY9y5VFOzWfIRxv73H777UMMLAYhBgKBLoTOHIbGvqsYMJ7sOyC1Ynnnjq+SPOhEkdq8ed4vMD/Nz8lLhnTAnt0qBCTbdKrrdG9YxVIlAHl14MhLRy2/7xLi8Ldj1TJMxateAHV7d30arnd/K7sSoXw9snXfe521cuqW+vVIaWDvbmmDf


Chunk 645:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: MxateAHV7d30arnd/K7sSoXw9snXfe521cuqW+vVIaWDvbmmDfuukjQf2SJtu0C9tvnHvtOXGfdOWQwamzTfsl4YOWT9tPWyDtM1mG6Thmw5KQ4eul9Zfb1AmzP5lBoJhhNnqO2dumvPG7EwgC3JvmAm4V+9ybOeDaJyDOiEYyfcF7S9JqudW20J9tbNz006AZOt5aWP7NL43hhq0E1J9JRMoIWDs3Dg3sldmswArcQQZ6sGb8OCDDxaSPeigg8ox1YNXCPHJPzCLH0JA+zmOxZb+8pe/vEMM2AeRWujpD3/4Q/k0DKFO9rXiIyHib54nZH/FFVeUWRfy8jwoQ11st03dKilrE5/nnXde2d9CT1dffU2uxz3F+6KNH3rowRLgV0WEc3EPaQcuflNNb7jhhnRv3l8e3hF1IwKUKWjWsQ2dEMPaTjnuM8e3sJRnQ9uEGFg8QgwEAl0JueN9B/LfOlSuWR09McBC0gnqDIkBhFKfSR0zYtBZ2+/NbJUhPML


Chunk 646:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: eN9B/LfOlSuWR09McBC0gnqDIkBhFKfSR0zYtBZ2+/NbJUhPMLAp9+hQSYDC2FIQzYaUj5ZoDpxJM2q02lXF6xya6okKg+og/3UR0de64McZ2cynp0t5jftl8tbkOsgjmHtbK3L1z1/9smW/uD1eqRtNs8Ev0nvtN3QHmnHTXumHTfrnXbcon/acet+acfhA9O2w9ZLw4asW1YNHLJ+Pl7fBonOyc322uvz0gsz30zPvfRaem7Gy+mFF19JL78yK7fX62luFgNrde9RiEidiQHnhuhqPX33m+Rvn+qHrLV1scY33rgEOSIjpFWFgetR95X8LdV2ULZ24y1AghKycxxtpxxxEXmHco0QHCsfuSI2QxAi8k3LdA/UGAVt3S23n7I7igH1nZ23izWZNGlS8VIQF4gT6fJwKMd1eOihhwshK8P9pZ5I2MwSHgn569smnUuNg/BdfX7605+WOqqHthW0Skg4T+Ua3tIe6qW93M8WnaoiYUaulzUoxDxY7Erd7


Chunk 647:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: WOqqHthW0Skg4T+Ua3tIe6qW93M8WnaoiYUaulzUoxDxY7Erd7Uu4yKNOVRAjfce3WqXhlhADS4cQA4FAF0NZhz8nsOywThEJ3HjjjcXdjEB0rDq85k7PPghOxyn5zjpFLjURATpp8+l32223sq6AiPSdd9m5zK3XkTam9jXmadufqKgu3GoFO5ZUiU8Hry5Ik5DQR9T6FBfya5mQszCZ92YWJkRJPgcWrbTOWm+lvj3fSkM37ZM+us/W6YBdB6UDtl8n5a9p1616p522Xidtu+XANHSj3mlgP8MH2bKct1aaPuOV9MDj09Kf75uSbr/jsXTLfU+mOx94PD2Utz3zwovplVmvpzfm5OMtfCu1rfX2GgHgXNQdmvsz3yuROydkaaVCyxebsidpN6TmDY4jMiERUYSQ8kG5zr2WU9tC8t31c31Y24jMdseRlKFutlsXgSVuuiAy9Z3FjvTUAWlq887EAFIkzAQk/uu//mup43HHHZfGjBlThEdjCGJmvh9


Chunk 648:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: vTUAWlq887EAFIkzAQk/uu//mup43HHHZfGjBlThEdjCGJmvh96lBiSa6+9tmzzsqjTTz+9TM9DsM7Lceq7KbjiLUj1uc99rgQwqsuPf/zjQu7qwotxxhl/n8aOHVvOHeGrO2tenUaOHFnuCSRPbOy9997py1/+cjr0kENKvYgfxyO4eFEMfRBe3/zmN9OHP/zh0r5VSB2Q/3aNQgwsHUIMBAJdDJU4pBm5E9WZcpMSBFbVs30dlnju8Po2jSMjaaSAvJF2BQIkAEaNGpVOOOGEdPzxx5d3AejUdbQ6/BoQxhIkCryXgEggDJCTjrlaz80JEJiOnAVZvQJ+Ux914YJeYL5/yuJhrZzyp6GK7t253HumHt0zYea/F2ah0GPhy2nDAd3SRuvNTesOeCX16DM/H7dHenbG/HT3o6+nP90zM902aXq644Hn050PT093P/R8evCxF9OLr85NL8+eXzwAjt3cR1VylrQVAlFPSdtUwgaf1WuiDYzXi8w/7LDDy


Chunk 649:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: NL8+eXzwAjt3cR1VylrQVAlFPSdtUwgaf1WuiDYzXi8w/7LDDypsbtQePANHjU7CedjV/HrFpP8dxzrwntS7aDqoAsViRa0hk8bzIpzzXQ7shOJHy2va73/1uIV/XrgYPEiJVtC1ODNjX4k5c/0TEFVdcni6++JJyPyFP7eC6m5Vx3333lvMgdtQBmdYFqAQ0aiNkS5g4d/cGgcRCJ4hs3yb/fUi+r7beeptyXeVzXzh/9xCvinsLkbuXeRL+9m//tmwnXKxxoR133nmXtP4G65f7kehSJ9+1kSEPngRlEh4hBpYeIQYCgS4GFOsJ4yHwciFuWGOlyAFRVMLqk4lAZ+hvHSXCQUCGBogB23WyOlRR6EcccUQhEMSuzNtuu61YblzIdbzXeDQrjzWPOOXX2bOOWbNIpxKbY0Il2OLqzgRX+wf53nxzXnrtNaQoUE1+ydh4FgI9e6SemTTsX7asvSAtfO311K/H/DR44Ny03oD5paxuvQanl+evkx6fsSA


Chunk 650:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: TTsX7asvSAtfO311K/H/DR44Ny03oD5paxuvQanl+evkx6fsSA9/OTs9OQzr6Xpb6Q0fda89OKsOWnWnHlpfubYhdZSyOWAujWn2kbaT72k4tVYy7TFHkXIOAeEjORMz7NwD6JCXkgWibLWBdKxohGQv1n4LGZg0SIkhArIXps5fs8ePYvwQlREHCI3g6AKN9fK/kicJc0lTyhYptnKegiRdS7PppsOK4KlR491StkdxYAllV0PJI0seRIOydY3siUoeAicn3Lr+dmHB0AbuCbIXF21i/vKMIPybCOMCB8iAdFPmDAhDcn1I6CGDdu03J/KIIrsp33UXdsidPeboYiTTjqpXdA07iFtUAJPcxuUezx/1x7aQlyC+AJlESEhBpYNjYiUQCDQZYBKm8lLRy9VgodCq+1EV/5uJ7zye/5fJ40sdNo6fVaeue1EwDXXXF2sxRq4ZUyWMGCtER2Sbf4mDhA88jj88MNLZ6vDVn4zGnVpjJdnGZPrIdnmN3WWP


Chunk 651:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: tER2Sbf4mDhA88jj88MNLZ6vDVn4zGnVpjJdnGZPrIdnmN3WWPxN7/tp7nVy3fj3TBgN7pE026p223KJ/LndQ2nXkxmnb7dZNQ4Z2S33Wy3kH9MhpndRj4LzUb+O+af3NN0yDNt0gdRvYJ83Opc1ekMXPwiyC8jEW5APV4ZWOCXxWsVRjICpRz8tkhtB4WpwfkkGYyBTpayNrO3ClEwTaBXkiRkFtrG9keP1115f3IGibugogEq7kWkRAv76FwFynUp98TVm6XN9EGKLUhvVaKkv+6slAqCx1n6YzEog1oLQzOIbr57w++tGDyxRF3g7XE/EjfMeTz71WPReObb9K6v7uCNv8VgRgrqe6SjVv/Vsen81wrbS5IQMBsvZB+s7V1E1/E6SmaAoy1M68TI4lX2DZEWIgEOhq0Jm2d6iAyGonXVHf7V8JT+dZO14dJuJBGkgNIdnOVUwE3H5744U9PAQsUJ0uQkFE1Q2LmJCdfXwiCuTGFc5NXQmt1KW9fgi


Chunk 652:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: H5744U9PAQsUJ0uQkFE1Q2LmJCdfXwiCuTGFc5NXQmt1KW9fgilrW1Brrs3FApSzATYt0faYP1+aaPB/dOmQwelEVsOTjttt2HaY9eN0l67bJI+NGrDtO/eg9MBH94w7bf/JmmfDw9JW2+XSXP9bI12753eWLt3er1tblqrX6+07ibrpyFbDE4DhwxKa/dcJ7WV4xNNpqpJjbpIHdFcx0pCyGX2nNkNcZATl7m22mOPPcr5CeCzxK82qK9DZs2ypFm8NahNdP3DDz2cbrn1ljIOTkhpL8Mw3Oxc+oQFIeA6EHWOTZC8ntOruSxlVs9LZ/WvQISur2mMrGteCtevM8hriicBZHqfv1nUPAvawrUmiIgM9wuL2zZ1r3AvqJdt9ld/110b2lc7LAuUwdrv17exsBWLXznO2bXRLo6nXPcoAWaIzDZeB7EMvFXyajNtuaT2CryNEAOBQBdFsZayZVbd78i+dny182wQcEMM6KiRGGuPe9sYsIAt+7NYr88Wr


Chunk 653:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: FsZayZVbd78i+dny182wQcEMM6KiRGGuPe9sYsIAt+7NYr88WrGht5Kdzt68OWWqG8nT2iIHrGbkhRWSpMzami2QcLx+45C/ltC1Ma3dfmOvMwuuT1h04IG2+2fpp5K5D0957bpEOGrNd+vjHd05jP7FLtk53SIcfvGP68OhN0167D0o77ZDrPGSdtO4G66S3cpkvvrJ2euzpHumBx3uV9MTTmZheMQWwLRNKr9S9m8A3FqJpkNZFeOfYf2fwm6Sute20g4DGXEjaesSIsmofVz7XPw8AsiUCkJTzrG1dhRcoj6hApvIjMEMvYIgGiWkv19A0T+QnESSIugRU5vZWF59FpOTkN9uIBmRYEwHHw8NrQYzUcuxjfwQpOR7XPwJ++OGHspV916LgO0KQiFF+nSGBmIke51HFCcFhbF/5SNy9ZB/Htd39gbhLO+Zjqos6aJO6zb62+dTmPEuOSYQQT4IcHY/QEjCoTPepT38rRzvyZNT93MN16mX1mjlGbQv


Chunk 654:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: EuOSYQQT4IcHY/QEjCoTPepT38rRzvyZNT93MN16mX1mjlGbQvHCbwTETMQCHRRFJLNnZqO0nho7fgqrJLHpSoiHDlVsaDTNp4r+lpkNiufdatj1ZHq0PNDvEgM2AexIQ9igmu4liUPklEHFq7xaJ0zInkxE4JpgvoD+w0c0DdtPHjdNCR32BttuHEanNOQjdZNWwxrS5tstm4aOqR3GjSwR+rTO1u3Pbul+fm/2XPnZ2KZl56cNic9+NCraeq90zMJzE73PTw/3X7f3HTnA77PSQ8/MSc9Me219Nzzb6SZWRTMet1iNixexFct2bdXQ2xGJe6amlEFgvMyDCI+AmEaDkCgBFElN2XLa5y+V6/GS4oq0fmtthdylJ8VLtYC+SFORPV6FgGmV8on1b6U2x/ZCfREgtzi4hFcd8cg5ggAbnNDOoYmEKAxf+UTBpdeemkhdMcemIUY4nR8xO2cWNqGFUwRtE39eI/kA3WUjyDgDXI8114+9dOO9lU/96H7g


Chunk 655:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 8xO2cWNqGFUwRtE39eI/kA3WUjyDgDXI8114+9dOO9lU/96H7gOdAXQgned17YhsMURFRhpyqZ8XvRJHfCUvtYVYBcaNcnhgigJh1Trark/vedUP+2sE5EA68GdrQNrEvVTgYlpEibuCdCDEQCHQ1tBOLpAP2vLHuqjsXYegs/Y2Q6rgu6BxtF0TF3W0/HbEOE3lUspcPyeg8K8lV0qzHbE4IRieOPORnWT7zzLO5jMY7C5QpGNCiPr169stkn8VJtwVp4VvZOszW/Ly587KgeCO98Nwr6flpL6fnnpmZHn5sdnpsykvp4YdeTvfdPyOTwTPpvgdeSPdPeTU9MHVWmvz4rPTIU6+mp194M814ZWF6ZebC9NqsbCnPTunNuQvKgkJvvmnMXx0a7yBQt45Qf+elfrXNmgWC76LcTZXr169/iawXE/DCCzMKYWmrWq6y5OexUU5tm1Jebuv6SuMGIWcBlMnY9SKeJOWVgI/2LtR+RBjRZogCuRMgrHLlC0T


Chunk 656:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: uv6SuMGIWcBlMnY9SKeJOWVgI/2LtR+RBjRZogCuRMgrHLlC0T06VohUx6FKgiJM/VGnsgRkSNubvShQzcpgtB39wosWNCIjVCHRpDh9iWP391X7hXHIWbkcTwWuXtJuerh+O4jpE5E+BRXUkmeh0GZ8lcRy4onEgiK+rvj88AQJdrKOTXabGA5pnyujftTcg3UXVvJU8WTeivfb+pAZGhDdavnHWggXlQUCHQhIJ2adI4SNzEri4Wvw/c3wtCR+q6zlE9HrrNGeIjFDAKdOre1OeqVRHSs1fVsH38jJZ09r0F1B/sbQYBPnoZjjjmm7MsKFVRnsZhc2ZLXa4f79O2d+vfL5NLfYjyZGNfOfUVbLjsLg7ZMRgszkbyV64sQZ7+VRck85DQvvT47k13u1AXz6bJwr8DAhW1vLSKA/v36pl75e9vajVcrv/7a62nWa7PKeemTFrVdqfHbsMJeObdcv968IpmJa5vVJKDuC1/4Qj7PdUrA2vXXX1fIGyFWw


Chunk 657:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: bsMJeObdcv968IpmJa5vVJKDuC1/4Qj7PdUrA2vXXX1fIGyFWwQSOg6T6ZCKz8qPvyFEbCoR7a6HyFpbjia8QsCd4749//GN5ux6irWLBPuDcDOmYvkjAiV+Qz7VZUk+LfJGea0MMsKrV07GRsdgO5Igs/cbSVibYb5NNhhYC5WGyHXG7z1zfCoKDIHFPaQf3oHZxTGW7T3gKnA84F8IGWSvHkAPL3n2orWxXL0TtHA2rVCFQr7MphgSO3x1LvR1bu9X7EZTl+I5DeMjjdwKsK4mBem+tbIQYCAS6EMrD2k5qOkh/e9oQh05TB1k/X36ZBTmzEJZtOkUiAXSoBAGSKJHu119fhIJlern25befch3HM12s29xh65R1sj51rtV69Ya8E074TCbiBUUIWATJ8dSxrK1f+oW35/L36NE9W8tr502NxYUEMJWew/nlDzMAkDoybSybPC9vb5x7Lqzklc++i6biZaLzu/2cg8C4+fMbsyxEqCu7IyqJOAf1Ur/


Chunk 658:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 5x7Lqzklc++i6biZaLzu/2cg8C4+fMbsyxEqCu7IyqJOAf1Ur/aAfvUBgL9PvWpT5Vyf//735exbESGJGte5fguNcRPY2qn9qm/LcxiYGF5bfLaZQbHJz7xiXIdtD9B4FyJreqhkZC6WAXiwRh/8TA4/5zyP0sUBIGuj3p/rWyEGAgEujAQnOetsyeukE8mcgForD8CgYWEJJE5y41FaE16swhYfixvYgDpVcu0GY5VRQECRUyWfOXS/fSnP10W4JHHeLIxXMcq9WvvE2rH1lyG3+onyEOAOH610G1r7hSby/SprEq6UPf3KUHHMirsB/at9fJZy7OPWAEzL7ThRRddlMaPH1+sZORd95Waj1fLqaKplqVeLFziyTQ+RK+9JNMvBwxorDYoaVdigDXLAve3Msrx2utNGATWXHR2z64MhBgIBNZA6EDKg50/K0HVTqVuB0FVxABrFxnVfFIltY7wfFeiBNbrJptsnP7H//j79JGPHFREhlgBRFmPWcirf


Chunk 659:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: VfFIltY7wfFeiBNbrJptsnP7H//j79JGPHFREhlgBRFmPWcirfb9afv1tcaj7NOdt3r+WV9GcD5q/LwnKgZq//g22IW+WOnImmHg9zj///OLC5j2px+14vOZtzWUSCaYVEgIf//jHy7g+rw0vQxUPkhUIEX1p65ze9q60o/3vEANrNjreVysLIQYCgTUIteN4x0ONlHw0CYL6jIoLEAxnHHzcuHFl/LaKgGYyq9/tVwgIQeVPQsD465gxB6TTTvubtOWWw1PvPr0XufAL2ssApLa43qEco/370hJcPY9azxUN5WoP9e6Wk6EUIuA3v/lNiZIXKGm8u+ar+9T6+KxtBTwEAgaN/xsiMK1w0PrrLxJhtX2ldwmd9vKh5Gn/PcTAmo16L61sxGyCQGANgS6jdhuewEXEUja0P5N1m5TJRNS7ZYsNG7CATQVj0Vdiq2jukOzLuhXEVdeLN57O2u3bt08hTb8vSs1/ZzL0Kaoe2fmsv9W/y/BD0/YlJXWxX/2


Chunk 660:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 57O2u3bt08hTb8vSs1/ZzL0Kaoe2fmsv9W/y/BD0/YlJXWxX/27lNGcmvIuOmaHVH/vLF/zdm2nbtrKuL42Yc3zDlRBoD6FuDt04Mqyj+C4/fbbr4gBsQKEgd/sZ4ldxynXJu9Tyb6i+ZrmfxZ9b7+ygcByIcRAILCGoD51i0ij6Tv4LG5nv7UnxIfcRI0PHmy9/LZFbupKggiQB4CrHKHJa+oX8icEDj300EJwfqvlLkrtxy0J6ZWavBN+a/+yaJ9lhf06tZ+ay+tA0BWLjl/RIV/93b++awdT8rSDtqvCQdJWtb20oSBCc9pNhTMcsM8++xQhwCOw4UYbFQFQREQuu4qCWueOx+00lRyBwPIjhgkCgRYGIuIFWLCwsa6AtyByg9d35Zu6JQhQQCFhwBuA2Mz1FvhWX9GLFOtzX4lxTeoHirXf+FK+g3Yxo8CCPtrLWg3aS7CmwEdtQCARTtqLeNJehlWqiIDSXrltw90f6AwdvUwrCyEGAoEWRkeSQ


Chunk 661:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: tQCARTtqLeNJehlWqiIDSXrltw90f6AwdvUwrCyEGAoEWRkeSQ/rIzJRAMw9Ez9smIS0kZn62IMH+/QeUYQFWcH3mffIAVO/DmoRFbdUOf5upIY5AO9U2IxKqGLDojcBDXgTtRhw0txf4HmIgsDiEGAgEAisVHckNbFuYiawKAKQmlsB2z3V1gdchhGLV5kQAWBp50d/tfcCijiz/vSb0CrW96nnxqjhvUzhNyeRdqdMhtYO2qkl7ddY3hhgILAmLnqGVjBADgUCLoizC04RFBJ6JzC+lE8rpHYTenmxfhPw3IkOAJS5Anvafmo+wJvUK2qScW1PblG3arn1baSdt0Z6gtmmFb34hBtak9gmsOCx6/lYyQgwEAi2Kd3QyCKv9a9m6LB1Q+77Ka6Vnf4mddFObLAm1rXkRAoHOEGIgEAgEAoEWx6oSAyFHA4FAIBBocYQYCAQCgUCgxRFiIBAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMU


Chunk 662:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: RFiIBAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFocIQYCgUAgEGhxhBgIBAKBQKDFsVKWIw4EAoFAINB1EJ6BQCAQCARaHCEGAoFAIBBocYQYCAQCgUCgxRFiIBAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIFAIBAIBFocIQYCgUAgEGhxhBgIBAKBQKDFEWIgEAgEAoEWR4iBQCAQCARaHCEGAoFAIBBocYQYCAQCgUCgxRFiIBAIBAKBFkeIgUAgEAgEWhwhBgKBQCAQaHGEGAgEAoFAoMURYiAQCAQCgRZHiIHA6oU7z0kjRoxI59zZ/vcaiRlp3BdHpBFfHJe/BTrDjAtPzffBqWnc8+0bVjZa4r4LBBaPEAOBTjHpu5mscuf4jrTL6DT2q+ekcXcGhXWKudPSpAvPSacdfVgaVdtsz8PSyd88N926qkit1bFwR


Chunk 663:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: kcXcGhXWKudPSpAvPSacdfVgaVdtsz8PSyd88N926qkit1bFwRr4GP0inf77pGqyU+3ZGGv+1Ufn6fitNeKV9UyDQhRFiILAEDEtjPntaOu1vGunk/QanGTedm751/Jg09oeT0tz2XIFMDTeck8bueWA69oxM/LMHpzGnaLNT09gdU3rg4nNy252TJrXnDawczH3kvHTa3qPzNfj3NGFKjzT6xI737eg06mvnpSmz23cIBAKLEGIgsARslQ75q2+kb3y9kc788fh0603j05kHpTT5R/8znfdIe7ZlxfMT0w++eWw6/eI1w8Mw7eLT0qFfPDdN3eG0dN7EB9LdE85L3/+WNjsjff+XV6a7H7g+nfc3Q9tzB1YKnh6XTjvmrDRh7sh02i9vTQ/k+/Tfz2q6byfdnS747yPT3MvPSkd9Y1yatrB9v/eNwWnsv96dptxxdjp0vfZNKxNzJ6fxZ56WDvtRSMrAykGIgcCyYcAO6eS/+Woalqak8bdNad+4jHj


Chunk 664:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 6fxZ56WDvtRSMrAykGIgcCyYcAO6eS/+Woalqak8bdNad+4jHj6tvTvF09Ks5a7Q14NMPW8dPo3J6S5B52drjz/G2n0pj3bf2hCz2Fp9NdPTiPb/wysYCycks77xrfSxDQmnX35Bekb+w1u/6EJ3QakkX99QbryH8akudd+K53V1YToK/en8edPSFPeaP87EFjBCDEQWHYMHpy2yh+Tn3ut8XfLYlaa8CPu/zHpzLOOS8O6tW8OrFLMvfX8dE42mEf+3RnpuE3bNy4Gw449PX0j55n4o3Fp8pogRgOBFYQQA4Flx4LcAeePYQNYwbPS+K8J1DotjV9MINWk7+7Y+P3hcelUAV3Hn1u2TzxjdCPAK6dOo7hn3JrO/eaxafQu8uyYRh9/Vhr3yGIiFRbOSLf+7PR08qGj2svM+ceels65bPK7PRDPN+px6oXZOpw9JY3/7mnpsD3rMU5P502a1Z7xPTDr1jThsrmp52e/mI4a0r5tebBwVppy7bnvCH7bc


Chunk 665:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: P502a1Z7xPTDr1jThsrmp52e/mI4a0r5tebBwVppy7bnvCH7bcb9j01kXdnIOGcbBm/OOOvTkdPpl09p/bWDW5PHpnK+ObW9D5Y1Np/1kUr5qS4FlqU9zNP47rtuodNhXz0kTn2xk64hG/Wr573GNF4O7bzwv34+HppOPHNG+ZQnotkM69JQdUnr6wnT9w+3bOmLW5DTuzFp/7eo+mvKuGJlGkG3nsSDvOq9yL767jIqO1/Lt6zQpnWPbfjwfGT87tvwulfu3Hct1nQOBjBADgWXGlOsuSLemnunQPXOnmgakQ487Of81IY2/thPX69xb0/jzcxd44rHp0I23TgcIRjy64TAfdtDJi4ITR3b07E7JhH3QaemGPoemb3zn7HTGiaPS3DvPS986JouKjpH5T09I3zp8dDr5uxPTm7scl878wXnp+2ednEb3zKT0jdxBfnUxY8Szrk/nHH1UOvf5YenE//39dOYph6bBuVM969jj0r9Pbs+zJDwyKY3PH8c


Chunk 666:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: H1UOvf5YenE//39dOYph6bBuVM969jj0r9Pbs+zJDwyKY3PH8cdMCqf//JjxsWnp8O+/IN0a898Lmd9P517TrZ0R0xN550xNh3303dWaNqFp6Yxn88km0anr57z/XT235+WjtpiSpow+cX2HLnpM0EfNzaLm6dHpKP+/vu5TU5LJ4/umW6989HFklIzlqU+i/Cu67ZjeuGqc9OpR57+rqh75zA61+/cOwano/7mzPT9H5yRjhp2fzonX+P/t9SCYFqapio77512WMqx+xHb7JP/nZamPtEJVb6ayffYY9OvZ+2evpjvibP//uQ06o2J+T46Kh37o6ULmp12+bdKu4+bsUM6rrTbN9Jhg9yLh6WjOgbeLpxWZiWMdi3fyPn//ux07g/OTKces3ua+8i0nHeDNNIz8tkxaZj8IzPJtz8zB2zRuOuW9zoHAgVvvfVWW6RIHdNd/zi8bfjwU9p+9+zb2+bMfLLt5n89sW234cPbtv/SJW3Ta/45N7eduVPO/


Chunk 667:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: wU9p+9+zb2+bMfLLt5n89sW234cPbtv/SJW3Ta/45N7eduVPO/4Xfvb2tPc258cy27Ydv33bmjXPe3n772bns4W2njJv+jrzNvw0ffkjb2bc37ZPTk785pfx2xC8fadr+ZNuvPpXz73RK268eemf+t96a0/bIL0/Jxx/e9qnznnx7+7O/azulHOOAtjOvfmcd5txydtsB+bftv31z25ym7Z2l6eMa9Tn79s5/X3ya3va7L7y7vaZf+dO2390/sylfTvMfaT+/s9vuml+339X2Pe39N5e0zWzOm9PMmbUNprdd8qWc58P/1nb/ov3a08yZ73lu0tLXJ6dF1+2Utp92uA71ur3jGjz+q7ZPaecv/a7tyQ71m5PLOqS9rOb7r/N0V9vZ8nZy7y02dXb/tW/bfqcj2s6+peM5P9n2uy9tn3//VNuvHn97e+MZye3QnLf93jrl3Ec6tPGcnP+Qd5Vx/7/btn3bKb9papvOUr1n//GuDr8t/3WOFEkKz0BgCZi


Chunk 668:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: P+Qd5Vx/7/btn3bKb9papvOUr1n//GuDr8t/3WOFEkKz0BgCZiYvrVfw+1YXI+jDkwn//DutOnx309X/nhsWmTMZ8vx2L/KdssNF6Qr3+EOnpUm/v68NHfTr6Zjs6WyLOj5pX9MZ+zxzn2GHX5sGps/J099+m2L577x6eft48Unb9PxGD3TiBNPT1/dNKVJPxuf3mXLHv6N9I2D3umS6Ln32HRMzj930pRsO65aDD7k1HTcDgPa/2pHtxFp94/ktp09JT3a5HiZa3rcc6+mFzp4PAaUoZsG3lyQ/3n5xfRCR/NwwICl8mQsS30qhn399HRqh+tQr9ukR6Yuum6TL/95mpRGpjO+9e5Yi557fCOdcWL7H6sax/zP9I29O57zsHRcCZqdlC64YclBs5Mv/kmauOk30umfG9GhjXumkcecmHbIZdxwd3vDzZ2YLvheLu/wf0zfP77Y/e8Ly3udAwEIMRBYAt65zsAZ55ybxk+8O43/h7Hv6sB3OPKU3LVPS


Chunk 669:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: Ly3udAwEIMRBYAt65zsAZ55ybxk+8O43/h7Hv6sB3OPKU3LVPSuNvaqLQVyam8Zdlov7S2LTDMgbXjd6mk0iwARumDX0+/cKisdAZD0/KpL1DGrvvYsaLu+2QRh2SP5+elKZ0IK9h249IHbr9jMFpsOjI+55J7xUe2bNP+9464xWFudPS5GvHpXN/+IN01lfHprGHjkpjv9dRluyQDv2bfL6TzkrHfdFiOtPS3HcNgwxOBx4xJvWcfV467fjT07nXTnl/szeWqj5vY+TwpbluM9L9uc5p0zFp983Lhg7omXbceUz79/dCz9TfITMRvtnYsNQYNvjdV3/0zh1JvB3b7JjUaMlBs+3n9fQP0tht3xbRi9KhZzUEab0O992WzssfYw8b08l9uLRYQdc50PIIMRBYAt65zsCpx4xJO3Q2dQ42H5PGjsz8dOHEVG2nWTdNSBOyRBi737JbPSOGdTI9rBM8PVVY1YapZ9/G352hV7sQebNDJ7nV4KU7xuIwYIu


Chunk 670:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: SOGdTI9rBM8PVVY1YapZ9/G352hV7sQebNDJ7nV4KU7xuIwYIutyjjuhEmLGT9fRlivYNSOB6axXz4nXXjD3enVTQ9MB3722+n7At7egWxlfv3SdOUPTk2jppyXvnX8gWnHvY9Np//s1jSj6RwHH31umvjrM9KhPSemc758WBo1cnQ69bvjl3rRnaWvz9vYcMOlobWn0zN35I+tsvBqbFgObJV2PCB/3HZDmrSUKwFOmXxL/neHNLyTe3mx913PgUthZT+dHrshf2SRc3K7gO4s1bH+GdMaT8rStdnisbzXORCAEAOBFYRh6ajPHpqtnQvSrVP9PS1d+p8TUjr85HRUp9bfisEGm4/O/76Q5r7n/OsBaWCf9q8rCtseWIYUpv3X+DSpo4t2WfFkY72CgSf+e7r14bvTlRe3L1x04thsbQ9sz9SMnmnEkWekc2+yyNG56Rs7Tk3jv3tyGvPV8e9438HgvU9N37/g7vTAxAvSmWMHlxkXhx39g/eu7zLXZ


Chunk 671:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: yGvPV8e9438HgvU9N37/g7vTAxAvSmWMHlxkXhx39g/eu7zLXZ1nQP23A+7IEa37GjHITLQV6plF75Psuy87zLl+8x2IRFk5OE36exdvIY9MYdeiAuW8spmGef7SI3J5LVAQbpOF7+xyZjm0X0J2lk/dokP/gjRverMUecxnwvq9zINCOEAOBFYYBh5yUTu4zOf38qtzZPjkxjZ/UM5183KHL4QJ9bwwbsWOmg8lp/G2LIYLc+d9yRf4cOTJ1HP5ebnTbIR33N2OyQXhu+p/LuTzzjDuyZZs/Dz3y0DT4HUMqc9PUB29t/945Bmw1pqy6d+6JPdPca8en6zt5D0LPTUemk/9hfLr0f49Maeq/pwn3tf+wGCxPfd4bw9KwXfLHYq35aemu65aC2Nsx4JBT02mZ2Cd996w07un2jYvBtAu+n37wdM809gtHFa9OR9w1tfPjzsrtwQd13MjFe0V4qDYkMJ6emO5azFTKd2DjoWUhqnE33L1c904zlvU6BwI


Chunk 672:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: MjFe0V4qDYkMJ6emO5azFTKd2DjoWUhqnE33L1c904zlvU6BwIVIQYCKw49R6VDjumZLeXrcwd3QZrU5+Q0trPAwT79S0c8dUYnEWjLipFj01fbieC8Tqaj6fx/vITOf3kx+Ogz09kH9UxTfnZSOvb/G5cmdzaxe9aUNPGH5xVyXSzaCXfac+8sYO6dP0jnnN/+xyLMTbPeRaI904aDy8h8+7CIPO9ujw3Xk6dn6vkOgu8Ey1SfZUXPNPrgxnTUH/xw4juGNmDahWelHzz83k75Reg5Mn31O6emEbMnpm8dfVo6775OLoJ1KH54bDrs/5uYeh70j+l/Ht65Mpz8w3PSuI5EPmNi+sH3JqS06alp7N5Lqlc+ryNPzffZpHTO2Z1MZzWN8GcT3vbcbH5UOvXILOD+86x0zg3v8Sz0GdgQ1c+/HS/TwHJe50CgHSEGAisQuTM8+qtp2NM/Tmf98+Q07K/GppGddUZbjUpj+uRO/ydnpNN/OC6N+9np6dwlM


Chunk 673:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: /Tmf98+Q07K/GppGddUZbjUpj+uRO/ydnpNN/OC6N+9np6dwlMuUSkK3z0/7f2WlMttvOOnx0Ovab56RzL5yYxp9/Tjr92FHpwP/v1rTV3/w6/eNiOv/lhkjzH1yazjy8Z5r8u2+lsaNGpcM+f3o654c/yER3Vjpt7Oi046jD0qk/f3v+f2cYPObYNDa3yYRvjEknn3leGn/ZeekHZ56cRv+vlE75Vsdgusnpx3tazCeT5vnj063teU/64bQ04kunpUPLsPesdOXfjSqLKJ3zs3FpoiDAs09PJ/2vCZkMz0zHsMyXgGWrz7Kj535fLddk2vmnpjFH5zrm85h44bnprC+PTof9bMf07b8z/LP06LnHGWncr09LO8ydkM46elRZHOn0s12DfB98fmwaPXJ0OvlHk9NWx//7O2fCdMBxJ45IPz9ydDr1zHPTuMsabTb2oFPTeS/n++yfv5FGvodGMRPi+18aUZY8PnDvk9NZWQSOb2/7Y/c+MJ1+ZzNxD0i


Chunk 674:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 5FGvodGMRPi+18aUZY8PnDvk9NZWQSOb2/7Y/c+MJ1+ZzNxD0iHfufX6dStpqTzvjgmjf5y0/WU/xtNQz4DduDcSumy/5lOy9dj3IXnpHMu8+vyXedAYBE6m28YKVJn6wwsXXqk7VdH2fed86k7pjkP/a7tzE/tlvMNb9t+n5z3ofbf2ud7dz53fwlzymfe33bJP/512xH7mA/u+Lu1HfKVs9t+d3snaxnUueCdrXNQ1wDoOH98KdL023/X9r3/fmLbIbvbX9q+7UNH/HXbmedd1/bIzOa8na8z8Nbj17Wd/ZVDyjoO6v+p//7Ttpun1/UMmq/FzLa7/u/X2048uNF+9ThnX3p/28xFc83ntD3yh7Pb/vqID5V1FuTb7eAT277+f29um95xPvri0lLXJ6f3c93mz2y7f9yZbZ+q12z3Q9r++h8vaXvk9cUcY2nSzEfarjvvzHec9/b7HNF2Yq77dY92WD+gOTXVf+b9+d48rn3/nT7UOO9O6tHpOgPta


Chunk 675:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 1, 'num_chars': 512}
Text: 7HNF2Yq77dY92WD+gOTXVf+b9+d48rn3/nT7UOO9O6tHpOgPtafqNP237ei0jp0bbd7wP2tP86W03d3Y9/9BhrYLpN7f925fay8xtdebVzmcFXOdIkXJayz/tuiAQWH4snJz+/aCx6Qe7fD/d/a9jV2q8QCDwQeLWs0ekk39+arpgyhll7D8Q6MqIYYLACsXcWy9IP346pUMPXp6504HA6o4paYrpkXsPTxs0NgQCXRohBgIrDgunpHH/YsXBU9Oph4QUCKzBmDwhnX9fSiM/NnqlBKYGAqsaMUwQWG5M+vnpacKsDdML152Xxk8elk793aXvWko4EOjqmPSTU9P4uTumAbOnpet/Nz5N3vjUdMHFZ6SRK3r9ikDgA0B4BgLLj9kT07k/OjdNnDsmnXHBuBACgTUS/fu+mi790b+nf//drann0Wen8ReEEAisOQjPQCAQCAQCLY7wDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4Qgw


Chunk 676:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 21, 'num_chars': 512}
Text: AQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OEIMBAKBQCDQ4ggxEAgEAoFAiyPEQCAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQIsjxEAgEAgEAi2OEAOBQCAQCLQ4QgwEAoFAINDiCDEQCAQCgUCLI8RAIBAIBAItjhADgUAgEAi0OEIMBAKBQCDQ4ggxEAgEAoFAiyPEQCAQCAQCLY4QA4FAIBAItDhCDAQCgUAg0OIIMRAIBAKBQEsjpf8fHuxYuWQUPyQAAAAASUVORK5CYII=)

# In[ ]:


class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def myfunc(self):
        print("Hello my name


Chunk 677:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text:     def myfunc(self):
        print("Hello my name is " + self.name)


p1 = Person("John", 36)

print(p1.name)
print(p1.age)
p1.myfunc()


# Objects will have combination of attributes and methods.
# 
# Attributes - what object has or is. - name,age, height
# 
# Methods - what object does - eat,sleep,talk

# Here's the syntax to create an object.
# 
# objectName = ClassName()

# In[ ]:


# create class
class Bike:
    Model = "Yamaha"
    year = 0

# create objects of class
bike1 = Bike()
print(bike1.Model)


Chunk 678:
Document ID: 2d0428ce-5aaf-4424-baaf-920b200f807a
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 58, 'num_chars': 352}
Text: objects of class
bike1 = Bike()
print(bike1.Model)


# Python Class and Objects
# Assigning values

# In[ ]:


# define a class
class Bike:
    name = ""
    gear = 0

# create object of class
bike1 = Bike()

# access attributes and assign new values
bike1.gear = 11
bike1.name = "Mountain Bike"

print(f"Name: {bike1.name}, Gears: {bike1.gear} ")


# 


Chunk 679:
Document ID: e2c447fd-101f-441a-8305-112d269bdd1b
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 47, 'num_chars': 439}
Text: 


class Car:
  # class variables
  company_name = 'ABC Company'

  # constructor to initialize the object
  def __init__(self,make,model,year,color):
     # instance variables
    self.make=make
    self.model=model
    self.year=year
    self.color=color

  # instance method
  def drive(self):
    #print("This car is driving.")
    print("This " + self.model+" car is driving.")

  def stop(self):
    print("This car is stopped")


# 


Chunk 680:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 71, 'num_chars': 512}
Text: 


car_1=Car("Chevy","Corvetter","2023","Blue")


# In[ ]:


print(car_1.make)
print(car_1.model)


# In[ ]:


car_1.drive()


# In[ ]:


car_2=Car("Ford","Mustang","2022","Red")


# In[ ]:


print(car_2.model)


# In[ ]:


car_2.stop()


# try writing
# print("This" + self.model+" car is driving.")

# # 6 Functions

# ## 6.1 Python Function **Declaration**

# The syntax to declare a function is:
# 
# def function_name(arguments):
#     # function body
# 
#     return
# Here,
# 
# def - keyword used to decl


Chunk 681:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 86, 'num_chars': 512}
Text:     return
# Here,
# 
# def - keyword used to declare a function,
# 
# function_name - any name given to the function,
# 
# arguments - any value passed to function,
# 
# return (optional) - returns value from a function

# In[ ]:


def greet():
    print('Hello World!')


# In[ ]:


greet()


# Python Function Arguments without return type

# In[ ]:


# function with two arguments
def add_numbers(num1, num2):
    sum = num1 + num2
    print("Sum: ",sum)

# function call with two values
add_numbers(5, 4)





Chunk 682:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: function call with two values
add_numbers(5, 4)


# with return Type

# In[ ]:


# function definition
def find_square(num):
    result = num * num
    return result

# function call
square = find_square(3)

print('Square:',square)


# ## 6.2 Python In-built Functions

# ### 6.2.1 map function

# The map() function applies a given function to each element of an iterable (list, tuple etc.) and returns an iterator containing the results.

# The map() function takes two arguments:
# 
# * function - a function



Chunk 683:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 107, 'num_chars': 512}
Text: takes two arguments:
# 
# * function - a function
# 
# * iterable - an iterable like sets, lists, tuples, etc
# 
# You can pass more than one iterable to the map() function.

# 
# The map() function returns an object of map class.
# 
# The returned value can be passed to functions like:
# 
# list() - to convert to list
# 
# set() - to convert to a set, and so on.

# In[ ]:


# Return double of n
def addition(n):
	return n + n

# We double all numbers using map()
numbers = (1, 2, 3, 4)
result = map(addition,


Chunk 684:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 93, 'num_chars': 512}
Text: ap()
numbers = (1, 2, 3, 4)
result = map(addition, numbers)
print(list(result))


# How to use lambda function with map()?
# 

# In[ ]:


# Double all numbers using map and lambda

numbers = (1, 2, 3, 4)
result = map(lambda x: x * x, numbers)
print(list(result))


# In[ ]:


num1 = [4, 5, 6]
num2 = [5, 6, 7]

result = map(lambda n1, n2: n1+n2, num1, num2)
print(list(result))


# ### 6.2.2 filter function
# 
# The filter() method filters the given sequence with the help of a function that tests each element 


Chunk 685:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 98, 'num_chars': 512}
Text: th the help of a function that tests each element in the sequence to be true or not.
# 
# The filter() function selects elements from an iterable (list, tuple etc.) based on the output of a function.
# 
# The function is applied to each element of the iterable and if it returns True, the element is selected by the filter() function.

# The filter() function takes two arguments:
# 
# * function - a function
# 
# * iterable - an iterable like sets, lists, tuples etc.
# 
# filter() Return Value
# 
# The filter


Chunk 686:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 89, 'num_chars': 512}
Text: es etc.
# 
# filter() Return Value
# 
# The filter() function returns an iterator.

# In[ ]:


# function that filters vowels
def fun(variable):
	letters = ['a', 'e', 'i', 'o', 'u']
	if (variable in letters):
		return True
	else:
		return False


# sequence
sequence = ['s', 'u', 'p', 'e', 'r', 's', 'e', 't']

# using filter function
filtered = filter(fun, sequence)

print('The filtered letters are:')
for s in filtered:
	print(s)


# In[ ]:


# a list contains both even and odd numbers.
seq = [0, 1, 2, 3, 5,


Chunk 687:
Document ID: 2db1b750-09d5-4fb4-9c85-51dc061e028c
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 60, 'num_chars': 298}
Text: s both even and odd numbers.
seq = [0, 1, 2, 3, 5, 8, 13]

# result contains odd numbers of the list
result = filter(lambda x: x % 2 != 0, seq)
print(list(result))

# result contains even numbers of the list
result = filter(lambda x: x % 2 == 0, seq)
print(list(result))


# # 7 Files in Python

# 


Chunk 688:
Document ID: 6da69119-4f3a-44a4-81ce-4ce1d6a13702
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 38, 'num_chars': 258}
Text: 


fout = open('output.txt', 'w')
#print(fout)

# The write() method puts data into the file.
# f.write(string) writes the contents of string to the file, returning the number of characters written.

line1 = "This here's the wattle,\n"
fout.write(line1)


# 


Chunk 689:
Document ID: 5a042689-f8f3-4a53-b647-a450003ad392
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 34, 'num_chars': 177}
Text: 


# Again, the file object keeps track of where it is, so if you call write again, it adds the new data to the end.

line2 = "the emblem of our lands.\n"
fout.write(line2)


# 


Chunk 690:
Document ID: f5102214-a913-4e58-b941-b2f45e6db986
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 14, 'num_chars': 77}
Text: 


# When you are done writing, you have to close the file.
fout.close()


# 


Chunk 691:
Document ID: a996ef3b-9d9c-4665-b67c-a35b71dea944
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 12, 'num_chars': 94}
Text: 


f = open("output.txt",'r',encoding = 'utf-8')

# read the entire file

print(f.read())


# 


Chunk 692:
Document ID: 4e930457-8581-4a1d-a7c3-7be01422a349
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 31, 'num_chars': 183}
Text: 


# We can read a file line-by-line using a for loop. This is both efficient and fast.

f2 = open("output.txt",'r',encoding = 'utf-8')

for line in f2:
    print(line, end = '')


# 


Chunk 693:
Document ID: 740118e8-04e6-46e6-8074-63d9c09867fc
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 91, 'num_chars': 512}
Text: 


# The readlines() method returns a list of remaining lines of the entire file

f3 = open("output.txt",'r',encoding = 'utf-8')
print(f3.readlines())


# # 8 DataFrames

# A DataFrame is like a table where the data is organized in rows and columns.
# 
# It is a two-dimensional data structure like a two-dimensional array.
# 
# 
# We can create a Pandas DataFrame in the following ways:
# 
# * Using Python Dictionary;
# * Using Python List;
# * From a File;
# * Creating an Empty DataFrame.

# ## 8.1 Creating 


Chunk 694:
Document ID: 740118e8-04e6-46e6-8074-63d9c09867fc
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 82, 'num_chars': 512}
Text: * Creating an Empty DataFrame.

# ## 8.1 Creating dfs

# In[ ]:


import pandas as pd

data = {
  "calories": [420, 380, 390],
  "duration": [50, 40, 45]
}

#load data into a DataFrame object:
df = pd.DataFrame(data)

df


# Pandas DataFrame Using Python Dictionary

# In[ ]:


import pandas as pd

# create a dictionary
data = {'Name': ['John', 'Alice', 'Bob'],
       'Age': [25, 30, 35],
       'City': ['New York', 'London', 'Paris']}

# create a dataframe from the dictionary
df = pd.DataFrame(data)

print(


Chunk 695:
Document ID: 740118e8-04e6-46e6-8074-63d9c09867fc
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 78, 'num_chars': 512}
Text: rom the dictionary
df = pd.DataFrame(data)

print(df)


# Pandas DataFrame Using Python List

# In[ ]:


import pandas as pd

# create a two-dimensional list
data = [['John', 25, 'New York'],
       ['Alice', 30, 'London'],
       ['Bob', 35, 'Paris']]

# create a DataFrame from the list
df = pd.DataFrame(data, columns=['Name', 'Age', 'City'])

print(df)


# Create an Empty DataFrame

# In[ ]:


import pandas as pd

# create an empty DataFrame
df = pd.DataFrame()

print(df)


# Pandas DataFrame From a File



Chunk 696:
Document ID: 740118e8-04e6-46e6-8074-63d9c09867fc
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 9, 'num_chars': 53}
Text: ame()

print(df)


# Pandas DataFrame From a File

# 


Chunk 697:
Document ID: 5e2722be-4894-4ee9-ae7a-3f48c9a81031
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 92, 'num_chars': 512}
Text: 


# import pandas as pd

# load data from a CSV file
# df = pd.read_excel('/content/Walmart_Store_sales.xlsx')

# df.head()


# ## 8.2 Indexing
# 
# loc gets rows (and/or columns) with particular labels.
# 
# iloc gets rows (and/or columns) at integer locations.

# Pandas provide various methods in order to get purely integer based indexing.
# 
# Like python and numpy, these are 0-based indexing.
# 
# The various access methods are as follows −
# 
# * An Integer
# * A list of integers
# * A range of values


Chunk 698:
Document ID: 5e2722be-4894-4ee9-ae7a-3f48c9a81031
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text: teger
# * A list of integers
# * A range of values

# In[ ]:


import pandas as pd

data = {
  "calories": [420, 380, 390],
  "duration": [50, 40, 45]
}

df = pd.DataFrame(data, index = ["day1", "day2", "day3"])

df


# The code df.iloc[0] is used to select the first row of the DataFrame df using integer-based indexing (position-based indexing).

# In[ ]:


df.iloc[0]


# The code df.loc['day1'] is used to select a specific row from the DataFrame df using label-based indexing.

# In[ ]:


df.loc['day1']


#


Chunk 699:
Document ID: 5e2722be-4894-4ee9-ae7a-3f48c9a81031
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 76, 'num_chars': 512}
Text: bel-based indexing.

# In[ ]:


df.loc['day1']


# loc takes two single/list/range operator separated by ','. The first one indicates the row and the second one indicates columns.

# In[ ]:


import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(8, 4),
                  index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],
                  columns=['A', 'B', 'C', 'D'])
print(df)
# Select all rows for a specific column ('A')
print("")
print(df.loc[:, 'A'])


# In[ ]:


# Select all rows for mu


Chunk 700:
Document ID: 5e2722be-4894-4ee9-ae7a-3f48c9a81031
Metadata: {'file_name': 'Week_3_Demo.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/Week_3_Demo.ipynb', 'folder_name': 'Module 3', 'num_tokens': 36, 'num_chars': 264}
Text: loc[:, 'A'])


# In[ ]:


# Select all rows for multiple columns, say list[]
print(df.loc[:,['A','C']])


# In[ ]:


# Select few rows for multiple columns, say list[]
print(df.loc[['a','b','f','h'],['A','C']])


# In[ ]:


print(df.iloc[1:5, 2:4])


# In[ ]:







Chunk 701:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 68, 'num_chars': 512}
Text: # -*- coding: utf-8 -*-
"""Lesson_three_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oRMvrzwGp8ItGdT7XbD8Dv_Bvn9_LtXH

# 1. Python module and Python package

**Python Modules**
*   A module is a file containing Python definitions and statements.
*   A module can define functions, classes and variables.
*   A module can also include runnable code.
*   Grouping related code into a module makes the code easier to understand and use.



Chunk 702:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 87, 'num_chars': 512}
Text: dule makes the code easier to understand and use.

**Python Packages**



*   Packages are namespaces which contain multiple packages and modules themselves.
*   They are simply directories, but with a twist.
*   Each package in Python is a directory which MUST contain a special file called __init__.py.
*   This file can be empty, and it indicates that the directory it contains is a Python package, so it can be imported the same way a module can be imported.
*   Example: If we create a directory called foo,


Chunk 703:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 87, 'num_chars': 512}
Text: 
*   Example: If we create a directory called foo, which marks the package name, we can then create a module inside that package called bar. We also must not forget to add the __init__.py file inside the foo directory.

**What is difference between a Python module and Python package?**


*   A module is a single file (or files) that are imported under one import and used. e.g.,
`import my_module`
*   A package is a collection of modules in directories that give a package hierarchy. e.g., `from my_package.ti


Chunk 704:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 62, 'num_chars': 512}
Text: ive a package hierarchy. e.g., `from my_package.timing.danger.internets import function_of_love`


Any Python file is a [module](https://docs.python.org/3/tutorial/modules.html), its name being the file's base name without the .py extension. A [package](https://docs.python.org/3/tutorial/modules.html#packages) is a collection of Python modules: while a module is a single Python file, a package is a directory of Python modules containing an additional __init__.py file, to distinguish a package from a directo


Chunk 705:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 71, 'num_chars': 512}
Text: _.py file, to distinguish a package from a directory that just happens to contain a bunch of Python scripts.

1.1 Python moduels:
https://www.w3schools.com/python/python_modules.asp


1.2 Python Packages

**(1) Find, install and publish Python packages with the Python Package Index**

[Python package index](https://pypi.org/)

**(2) How to install a python package?**

in command:


```
pip install nltk(package name)
```


in Colab:


```
!pip install nltk(package name)
```
"""

pip install bert

# Package p


Chunk 706:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 57, 'num_chars': 512}
Text: ckage name)
```
"""

pip install bert

# Package pre-installed in colab
!pip freeze

"""(3) **The structure of Python packages ([from python documentation]**(https://docs.python.org/3/tutorial/modules.html#packages)):

Packages can contain nested subpackages to arbitrary depth. For example, let’s make one more modification to the example package directory as follows:

![alt text](https://files.realpython.com/media/pkg4.a830d6e144bf.png)


The four modules (mod1.py, mod2.py, mod3.py and mod4.py) are defined 


Chunk 707:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 72, 'num_chars': 512}
Text: od1.py, mod2.py, mod3.py and mod4.py) are defined as previously. But now, instead of being lumped together into the pkg directory, they are split out into two subpackage directories, sub_pkg1 and sub_pkg2.

Importing still works the same as shown previously. Syntax is similar, but additional dot notation is used to separate package name from subpackage name:



```
import pkg.sub_pkg1.mod1

from pkg.sub_pkg1 import mod2

from pkg.sub_pkg2.mod3 import function_name

# Rename a function
from pkg.sub_pkg2.mod4


Chunk 708:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 76, 'num_chars': 512}
Text: n_name

# Rename a function
from pkg.sub_pkg2.mod4 import qux as grault

```

**Remember, the usage of packages and modules are very similar!**

# 2. Python Advanced Recursion

Problems (in life and also in computer science) can often seem big and scary. But if we keep chipping away at them, more often than not we can break them down into smaller chunks trivial enough to solve. This is the essence of thinking recursively!

![alt text](https://files.realpython.com/media/santa_claus_2.ecbf2686f1a1.png)
"""

h


Chunk 709:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 57, 'num_chars': 512}
Text: n.com/media/santa_claus_2.ecbf2686f1a1.png)
"""

houses = ["Eric's house", "Kenny's house", "Kyle's house", "Stan's house"]

def deliver_presents_iteratively():
    for house in houses:
        print("Delivering presents to", house)
deliver_presents_iteratively()

"""![alt text](https://files.realpython.com/media/elves_7.8d1af1cd85c8.png)

Assign titles and responsibilities to the elves based on the number of houses for which they are responsible:

*   \> 1 He is a manager and can appoint two elves and divi


Chunk 710:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 81, 'num_chars': 512}
Text: He is a manager and can appoint two elves and divide his work among them
*   = 1 He is a worker and has to deliver the presents to the house assigned to him

"""

houses = ["Eric's house", "Kenny's house", "Kyle's house", "Stan's house"]

# Each function call represents an elf doing his work
def deliver_presents_recursively(houses):
    # Worker elf doing his work
    if len(houses) == 1:
        house = houses[0]
        print("Delivering presents to", house)

    # Manager elf doing his work
    else:
   


Chunk 711:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 83, 'num_chars': 512}
Text: e)

    # Manager elf doing his work
    else:
        mid = len(houses) // 2
        first_half = houses[:mid]
        second_half = houses[mid:]

        # Divides his work among two elves
        deliver_presents_recursively(first_half)
        deliver_presents_recursively(second_half)
deliver_presents_recursively(houses)

"""n! = n x (n−1)!

n! = n x (n−1) x (n−2)!

n! = n x (n−1) x (n−2) x (n−3)!

⋅

⋅

n! = n x (n−1) x (n−2) x (n−3) ⋅⋅⋅⋅ x 3!

n! = n x (n−1) x (n−2) x (n−3) ⋅⋅⋅⋅ x 3 x 2!

n! = n x (n−


Chunk 712:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 68, 'num_chars': 512}
Text:  (n−1) x (n−2) x (n−3) ⋅⋅⋅⋅ x 3 x 2!

n! = n x (n−1) x (n−2) x (n−3) ⋅⋅⋅⋅ x 3 x 2 x 1!

![alt text](https://files.realpython.com/media/stack.9c4ba62929cf.gif)
"""

def factorial_recursive(n):
    # Base case: 1! = 1
    if n == 1:
        return 1

    # Recursive case: n! = n * (n-1)!
    else:
        return n * factorial_recursive(n-1)

factorial_recursive(5)

"""![alt text](https://files.realpython.com/media/state_3.3e8a68c4fde5.png)


(1,0)

(2,1)

(3,3)

(4,6)

(5,10)

(6,15)

(7,21)

(8,28)

(9,36)




Chunk 713:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 64, 'num_chars': 512}
Text: )

(4,6)

(5,10)

(6,15)

(7,21)

(8,28)

(9,36)

(10,45)

(11,55)


"""

def sum_recursive(current_number, accumulated_sum):
    # Base case
    # Return the final state
    if current_number == 11:
        return accumulated_sum

    # Recursive case
    # Thread the state through the recursive call
    else:
        return sum_recursive(current_number + 1, accumulated_sum + current_number)

sum_recursive(1, 0)

"""## **Recursive Data Structures in Python**

A data structure is recursive if it can be deﬁn


Chunk 714:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 63, 'num_chars': 512}
Text: *

A data structure is recursive if it can be deﬁned in terms of a smaller version of itself.
"""

# Return a new list that is the result of
# adding element to the head (i.e. front) of input_list
def attach_head(element, input_list):
    return [element] + input_list

attach_head(1,                                                  # Will return [1, 46, -31, "hello"]
            attach_head(46,                                     # Will return [46, -31, "hello"]
                        attach_head(-31,     


Chunk 715:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 68, 'num_chars': 512}
Text: lo"]
                        attach_head(-31,                        # Will return [-31, "hello"]
                                    attach_head("hello", [])))) # Will return ["hello"]

"""

1.   Starting with an empty list, you can generate any list by recursively applying the attach_head function.
2.   Recursion can also be seen as self-referential function composition. We apply a function to an argument, then pass that result on as an argument to a second application of the same function, and so on. Rep


Chunk 716:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 70, 'num_chars': 512}
Text: d application of the same function, and so on. Repeatedly composing attach_head with itself is the same as attach_head calling itself repeatedly.

![alt text](https://files.realpython.com/media/list.3df62a89243d.gif)

"""

def list_sum_recursive(input_list):
    # Base case
    if input_list == []:
        return 0

    # Recursive case
    # Decompose the original problem into simpler instances of the same problem
    # by making use of the fact that the input is a recursive data structure
    # and can be


Chunk 717:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 67, 'num_chars': 512}
Text: put is a recursive data structure
    # and can be deﬁned in terms of a smaller version of itself
    else:
        head = input_list[0]
        smaller_list = input_list[1:]
        return head + list_sum_recursive(smaller_list)

list_sum_recursive([1, 2, 3])

def fibonacci_recursive(n):
    print("Calculating F", "(", n, ")", sep="", end=", " "\n")

    # Base case
    if n == 0:
        return 0
    elif n == 1:
        return 1

    # Recursive case
    else:
        return fibonacci_recursive(n-1) + fi


Chunk 718:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 73, 'num_chars': 512}
Text: else:
        return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)

fibonacci_recursive(5)

"""# 3. Python File I/O

**3.1 Glossary**


**File:**

A named location on disk to store related information. It is used to permanently store data in a non-volatile memory (e.g. hard disk).


**Format operator:**

An operator, %, that takes a format string and a tuple and generates a string that includes the elements of the tuple formatted as specified by the format string.


**Format string:**

A string, used 


Chunk 719:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 77, 'num_chars': 512}
Text: rmat string.


**Format string:**

A string, used with the format operator, that contains format sequences.


**Format sequence:**

A sequence of characters in a format string, like %d, that specifies how a value should be formatted.


**Text file:**

A sequence of characters stored in permanent storage like a hard drive.


**Directory:**

A named collection of files, also called a folder.


**Path:**

A string that identifies a file.


**Relative path:**

A path that starts from the current directory.


**


Chunk 720:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 85, 'num_chars': 512}
Text:  path that starts from the current directory.


**Absolute path:**

A path that starts from the topmost directory in the file system.


**Catch:**


To prevent an exception from terminating a program using the try and except statements.
database:


A file whose contents are organized like a dictionary with keys that correspond to values.

**Python File Modes**

>Mode | Description
>--- | ---
>'r'| Open a file for reading. (default)
>'w'| Open a file for writing. Creates a new file if it does not exist or tr


Chunk 721:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 91, 'num_chars': 512}
Text: ing. Creates a new file if it does not exist or truncates the file if it exists.
>'x'|Open a file for exclusive creation. If the file already exists, the operation fails.
>'a'|Open for appending at the end of the file without truncating it. Creates a new file if it does not exist.
> 't'|Open in text mode. (default)
>'b'|Open in binary mode.
>'+'|Open a file for updating (reading and writing)

**3.2** **Reading and writing**

A text file is a sequence of characters stored on a permanent medium like a hard dr


Chunk 722:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 83, 'num_chars': 512}
Text: acters stored on a permanent medium like a hard drive, flash memory, or CD-ROM.

**Examples in the second edition**:

To **write** a file, you have to open it with mode 'w' as a second parameter:
"""

fout = open('output.txt', 'w')
print(fout)

"""The write() method puts data into the file.

f.write(string) writes the contents of string to the file, returning the number of characters written.

"""

line1 = "This here's the wattle,\n"
fout.write(line1)

"""Again, the file object keeps track of where it is, s


Chunk 723:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 97, 'num_chars': 512}
Text: ain, the file object keeps track of where it is, so if you call write again, it adds the new data to the end."""

# Again, the file object keeps track of where it is, so if you call write again, it adds the new data to the end.

line2 = "the emblem of our lands.\n"
fout.write(line2)

"""When you are done writing, you have to close the file.

To **read** a file in Python, we must open the file in reading mode.
"""

f = open("output.txt",'r',encoding = 'utf-8')

# read the entire file

print(f.read())

f1 = o


Chunk 724:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 77, 'num_chars': 512}
Text: )

# read the entire file

print(f.read())

f1 = open("output.txt",'r',encoding = 'utf-8')

# read the first 9 data
print(f1.read(9))

# We can read a file line-by-line using a for loop. This is both efficient and fast.

f2 = open("output.txt",'r',encoding = 'utf-8')

for line in f2:
  print(line, end = '')

# The readlines() method returns a list of remaining lines of the entire file
f3 = open("output.txt",'r',encoding = 'utf-8')

print(f3.readlines())

"""**Examples in the third edition: **"""

# Writing 


Chunk 725:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 51, 'num_chars': 512}
Text: **Examples in the third edition: **"""

# Writing our first file

with open("test.txt", "w") as myfile:
  myfile.write("My first file written from Python\n")
  myfile.write("# I like Python\n")
  myfile.write("---------------------------------\n")
  myfile.write("Hello, world!\n")
  myfile.write("## Python is really interesting\n")
  myfile.write("---------------------------------\n")
  myfile.write("# Python is easy\n")

# Reading a file line-at-a-time

with open("test.txt", "r") as my_new_handle:
  for th


Chunk 726:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 72, 'num_chars': 512}
Text: h open("test.txt", "r") as my_new_handle:
  for the_line in my_new_handle:
  # Do something with the line we just read.
  # Here we just print it.
    print(the_line, end="")

# If we try to open a file that doesn’t exist, we get an error:
mynewhandle = open("wharrah.txt", "r")

# Turning a file into a list of lines

with open("original_papers.txt", "r") as input_file:
   all_lines = input_file.readlines()
   print(all_lines)
all_lines.sort()
with open("sorted_papers.txt", "w") as output_file:
  for line in


Chunk 727:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 63, 'num_chars': 512}
Text: ed_papers.txt", "w") as output_file:
  for line in all_lines:
    output_file.write(line)
print(all_lines)

# Reading the whole file at once
with open("original_papers.txt") as f:
  content = f.read()
  words = content.split()
  print("There are {0} words in the file.".format(len(words)))

# An example

def filter(oldfile, newfile):
  with open(oldfile, "r") as infile, open(newfile, "w") as outfile:
    for line in infile:
    # Put any processing logic here
      if not line.startswith('#'):
        outfil


Chunk 728:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 83, 'num_chars': 512}
Text: 
      if not line.startswith('#'):
        outfile.write(line)

oldfile = "test.txt"
newfile = "update_text.txt"
filter(oldfile, newfile)

"""**3.3 Format operator**

(1) The argument of write has to be a string, so if we want to put other values in a file, we have to convert them to strings. The easiest way to do that is with str:
"""

x = 52
fout.write(str(x))

"""(2) An alternative is to use the **format operator**, %. When applied to integers, % is the modulus operator. But when the first operand is a 


Chunk 729:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 79, 'num_chars': 512}
Text: modulus operator. But when the first operand is a string, % is the format operator.

The first operand is the **format string**, which contains one or more **format sequences**, which specify how the second operand is formatted. The result is a string.

For example, the format sequence '%d' means that the second operand should be formatted as an integer (d stands for “decimal”):
"""

camels = 42

print(type(camels))

new_camels = '%d' % camels

print(new_camels)

print(type(new_camels))

"""(3) A format seq


Chunk 730:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 92, 'num_chars': 512}
Text: els)

print(type(new_camels))

"""(3) A format sequence can appear anywhere in the string, so you can embed a value in a sentence:

"""

camels = 42
'I have spotted %d camels.' % camels

"""(4) If there is more than one format sequence in the string, the second argument has to be a tuple. Each format sequence is matched with an element of the tuple, in order.

The following example uses '%d' to format an integer, '%g' to format a floating-point number (don’t ask why), and '%s' to format a string:
"""

'In %


Chunk 731:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 97, 'num_chars': 512}
Text:  ask why), and '%s' to format a string:
"""

'In %d years I have spotted %g %s.' % (3, 0.1, 'camels')

"""(5) The number of elements in the tuple has to match the number of format sequences in the string. Also, the types of the elements have to match the format sequences:"""

'%d %d %d' % (1, 2)

'%d' % 'dollars'

"""In the first example, there aren’t enough elements; in the second, the element is the wrong type.

The format operator is powerful, but it can be difficult to use. You can read more about it at


Chunk 732:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 72, 'num_chars': 512}
Text: be difficult to use. You can read more about it at: https://docs.python.org/2/library/stdtypes.html#string-formatting

**3.4 Filenames and paths**

Files are organized into **directories** (also called “folders”). Every running program has a “current directory,” which is the default directory for most operations. For example, when you open a file for reading, Python looks for it in the current directory.

The os module provides functions for working with files and directories (“os” stands for “operating sys


Chunk 733:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 88, 'num_chars': 512}
Text: es and directories (“os” stands for “operating system”). os.getcwd returns the name of the current directory:
"""

import os
cwd = os.getcwd()
print (cwd)

"""A string like cwd that identifies a file is called a path. A relative path starts from the current directory; an absolute path starts from the topmost directory in the file system.

The paths we have seen so far are simple filenames, so they are relative to the current directory. To find the absolute path to a file, you can use os.path.abspath:
"""

o


Chunk 734:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 55, 'num_chars': 512}
Text: ath to a file, you can use os.path.abspath:
"""

os.path.abspath('output.txt')

os.path.abspath('mnist_train_small.csv')

"""os.path.exists checks whether a file or directory exists:"""

os.path.exists('output.txt')

"""If it exists, os.path.isdir checks whether it’s a directory:"""

os.path.isdir('output.txt')

os.path.isdir('sample_data')

"""Similarly, os.path.isfile checks whether it’s a file.

os.listdir returns a list of the files (and other directories) in the given directory:
"""

os.listdir(cwd)

"


Chunk 735:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 86, 'num_chars': 512}
Text: s) in the given directory:
"""

os.listdir(cwd)

"""To demonstrate these functions, the following example “walks” through a directory, prints the names of all the files, and calls itself recursively on all the directories.

**3.5 Catching exceptions**

A lot of things can go wrong when you try to read and write files. If you try to open a file that doesn’t exist, you get an IOError:
"""

fin = open('bad_file')

"""Aif you try to open a directory for reading, you get"""

fout = open('/content', 'w')

"""To a


Chunk 736:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 87, 'num_chars': 512}
Text:  you get"""

fout = open('/content', 'w')

"""To avoid these errors, you could use functions like os.path.exists and os.path.isfile, but it would take a lot of time and code to check all the possibilities (if “Errno 21” is any indication, there are at least 21 things that can go wrong).

It is better to go ahead and try—and deal with problems if they happen—which is exactly what the try statement does. The syntax is similar to an if statement:
"""

try:
    fin = open('bad_file')
    for line in fin:
      


Chunk 737:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 81, 'num_chars': 512}
Text: fin = open('bad_file')
    for line in fin:
        print (line)
    fin.close()
except:
    print ('Something went wrong.')

"""Python starts by executing the try clause. If all goes well, it skips the except clause and proceeds. If an exception occurs, it jumps out of the try clause and executes the except clause.

Handling an exception with a try statement is called catching an exception. In this example, the except clause prints an error message that is not very helpful. In general, catching an exceptio


Chunk 738:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 93, 'num_chars': 512}
Text: not very helpful. In general, catching an exception gives you a chance to fix the problem, or try again, or at least end the program gracefully.

**3.6 What about fetching something from the web?**

The Python libraries are pretty messy in places. But here is a very simple example that copies the contents at some web URL to a local file.

The urlretrieve function — just one call — could be used to download any kind of content from the Internet.

We’ll need to get a few things right before this works:


*   


Chunk 739:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 92, 'num_chars': 512}
Text: o get a few things right before this works:


*   The resource we’re trying to fetch must exist! Check this using a browser.
*   We’ll need permission to write to the destination filename, and the file will be created in the “current
directory” - i.e. the same folder that the Python program is saved in.
*   If we are behind a proxy server that requires authentication, (as some students are), this may require some
more special handling to work around our proxy. Use a local resource for the purpose of this de


Chunk 740:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 66, 'num_chars': 512}
Text: y. Use a local resource for the purpose of this demonstration!

Here is a slightly different example using the requests module. This module is not part of the standard library distributed with python, however it is easier to use and significantly more potent than the urllib module distributed with
python.
"""

import urllib.request
url = "https://www.w3.org/TR/PNG/iso_8859-1.txt"
destination_filename = "rfc793.txt"
urllib.request.urlretrieve(url, destination_filename)

import requests
url = "https://www.w3.


Chunk 741:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 70, 'num_chars': 512}
Text: _filename)

import requests
url = "https://www.w3.org/TR/PNG/iso_8859-1.txt"
response = requests.get(url)
print(response.text)

"""Opening the remote URL returns the response from the server. That response contains several types of information, and the requests module allows us to access them in various ways. On line 5, we get the downloaded document as a single string. We could also read it line by line as follows:

"""

import requests
url = "https://www.w3.org/TR/PNG/iso_8859-1.txt"
response = requests.g


Chunk 742:
Document ID: 3a1bf3d0-e7ad-4f68-b0e1-5c1c721ac45b
Metadata: {'file_name': 'lesson_three_example.py', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 3/lesson_three_example.py', 'folder_name': 'Module 3', 'num_tokens': 9, 'num_chars': 93}
Text: 3.org/TR/PNG/iso_8859-1.txt"
response = requests.get(url)
for line in response:
  print(line)


Chunk 743:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 86, 'num_chars': 512}
Text: Lesson 4: Python Web Scraping
Haihua Chen, Ph.D.
2025/2/24
1
1
2
3
Intro to Web Scraping
Tools for Web Scraping
Ways to collect text data
2025/2/24
2
1
Intro to Web Scraping
2025/2/24
3
Web scraping is a computer software technique of extracting information from websites.
Web Scraping
2025/2/24
4
Transform unstructured data (HTML format) on the web into structured data (database or spreadsheet).
Web Scraping
2025/2/24
5
Why Web Scraping?
2025/2/24
6
Q: How does a search engine know that all these pages cont


Chunk 744:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 86, 'num_chars': 512}
Text: oes a search engine know that all these pages contain the query terms? A: Because all of those pages have been crawled
Why Web Scraping?
2025/2/24
7
What should we search? Every document on the web answers at least one question. Every time a search engine adds another document, the number of questions it can answer increases Adding many poor-quality documents increases the burden on the ranking process to find only the best documents to show to the user Web crawling discuss strategies for storing documents 


Chunk 745:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 73, 'num_chars': 512}
Text: crawling discuss strategies for storing documents and keeping those documents up-to-date!!!
Why Web Scraping?
2025/2/24
8
Why Web Scraping? Search Engine Architecture
2025/2/24
9
Architecture of a search engine determined by 2 requirements effectiveness (quality of results) and efficiency (response time and throughput) Important components of search engine are: Web Crawler: This component is used to find information on hundreds of millions of webpages that exists Indexer: This component is used to categoriz


Chunk 746:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 84, 'num_chars': 512}
Text: xists Indexer: This component is used to categorize and filter the pages based on content. Query processer: This component is used to translate high-level queries into low-level expressions that can be used at the physical level of the file system, query optimization and actual execution of the query to get the result. Algorithms: IR models to match the query and the documents Search Interface : The interface with which the user can access the search engine. Some Popular Search Engines Google Bing Yahoo Bai


Chunk 747:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 76, 'num_chars': 512}
Text:  Some Popular Search Engines Google Bing Yahoo Baidu
Why Web Scraping?
2025/2/24
10
We need to extract data from the web to conduct data mining and build a machine learning model
Web Scraping Applications
2025/2/24
11
Web Scraping Applications in Retail and Manufacturing
Web Scraping Applications in Equity and Financial Research
Web Scraping Applications in Data Science
Web Scraping Applications in Risk Management
Web Scraping Applications in Product, Marketing and Sales
Others: News and Reputation Monitori


Chunk 748:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 79, 'num_chars': 512}
Text: ing and Sales
Others: News and Reputation Monitoring, Academic, Employment, etc.
Manually
How to collect text data?
2025/2/24
12
Business tools such as import.io (https://www.import.io/)
Web Scraping
Others?
NLTK
2
Python Web Scraping
2025/2/24
13
API (Application programming interface) is the best way to extract data from a website.
Several ways to collect text data by Python-API
2025/2/24
14
NLTK includes a diverse set of corpora which can be read using the nltk.corpus package.
Several ways to collect tex


Chunk 749:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 77, 'num_chars': 512}
Text: e nltk.corpus package.
Several ways to collect text data by Python-NLTK
2025/2/24
15
Beautiful Soup
Several ways to collect text data by Python-Web Scraping
2025/2/24
16
Scrapy
Selenium
Pyspider
Comparison of the four tools
2025/2/24
17
3
Tools for Web Scraping
2025/2/24
18
The Structure of a Web Page
https://www.w3schools.com/html/default.asp
Download the web page
Steps for Web Scraping
Parse the page
Locate and download the text information we need
Save the text collected
import urllib.request
Libraries
p


Chunk 750:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 53, 'num_chars': 512}
Text: e text collected
import urllib.request
Libraries
pip install beautifulsoup4 & from bs4 import BeautifulSoup
pip install scrapy import scrapy
pip install pyspider from pyspider.libs.base_handler import *
pip install selenium from selenium import webdriver
urllib.request+BeautifulSoup collects Wikipedia page
The source code of the Wikipedia page
urllib.request in Python3 documentation: https://docs.python.org/3/library/urllib.request.html
Beautiful Soup Documentation: https://www.crummy.com/software/Beautiful


Chunk 751:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 58, 'num_chars': 512}
Text: ntation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
urllib.request+BeautifulSoup
urllib.request+BeautifulSoup
urllib.request+BeautifulSoup
Scrapy collects Amazon reviews – Why the need for scraping Amazon reviews?
Sentiment Analysis over the product reviews
Optimizing drop shipping sales
Web scraping for online reputation monitoring
Scrapy collects Amazon reviews
Amazon review web page
HTML code snippet for Amazon review
Detailed HTML code snippet of reviews
Scrapy collects Amazon reviews
Seleni


Chunk 752:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 70, 'num_chars': 512}
Text: t of reviews
Scrapy collects Amazon reviews
Selenium collects forum review
Pyspider collects text data- steps
pip install pyspider python -m pyspider.run http://localhost:5000/
Pyspider collects text data- results
Collecting data from Social media (via API) by using Python
APIs are designed to serve as a common concept between different pieces of software that need to share information with each other
Example: https://developer.twitter.com/en/docs/api-reference-index
Many modern APIs require some type of au


Chunk 753:
Document ID: b4bec4a6-60c9-4135-ac71-6e534f33cfae
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson Four- Python Web Scraping-updated-09112024.pptx', 'folder_name': 'Module 4', 'num_tokens': 64, 'num_chars': 399}
Text: nce-index
Many modern APIs require some type of authentication before they can be used
Understand the methods provided in apis helps us to get the data we want
Collecting data from Social media (via API) by using Python
Collecting data from Social media (via API) by using Python
Summarization of collecting text data using python
Packages?
Strategies?
Steps?
Structure?
Save?
Thank you
2025/2/24
36


Chunk 754:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 82, 'num_chars': 512}
Text: 


import pandas as pd
from bs4 import BeautifulSoup
import requests

# Set years for file creation
year = [2010,2017]

table_title = ['Day','High (°F)','Low (°F)','Precip.(inch)','Snow (inch)','Snow depth (inch)']

day_all = []
high_all = []
low_all = []
precip_all = []
snow_all = []
snow_depth_all = []

# Iterate across years

for y in year:
    # Reset the result list
    result = []
    # Using 6 due to fixed number of months in problem
    for counter in range(12):

        # Poll the site for html
   


Chunk 755:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 53, 'num_chars': 512}
Text: n range(12):

        # Poll the site for html
        url = 'https://www.usclimatedata.com/climate/denton/texas/united-states/ustx0353/' + str(y) + '/' + str(counter + 1)
        response = requests.get(url)

        soup = BeautifulSoup(response.text, 'lxml') # Parse the HTML as a string
        # locate the history data
        history_data = soup.find(id="history_data")

        table = history_data.find(class_="daily_climate_table") # Grab the first table
        # print(table)
        # print(len(tabl


Chunk 756:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 46, 'num_chars': 512}
Text: le
        # print(table)
        # print(len(table.find_all("tr")))
        # The first row is the column title information, we don't need that
        climate_data = table.find_all("tr")[1:]

        for item in climate_data:

          # get the day information
          day_cells = item.findAll("td",{"class": "align_left daily_climate_table_td_day"})
          day_all.append(day_cells[0].get_text())

          # get the high temperature information
          high_cells = item.findAll("td",{"class": "ali


Chunk 757:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 32, 'num_chars': 512}
Text:      high_cells = item.findAll("td",{"class": "align_right climate_table_data_td temperature_red "})
          high_all.append(high_cells[0].get_text())

          # get the low temperature information
          low_cells= item.findAll("td",{"class": "align_right climate_table_data_td temperature_blue"})
          low_all.append(low_cells[0].get_text())

          # get the last three columns
          last_three_cells = item.findAll("td",{"class": "align_right climate_table_data_td"})

          precip_tex


Chunk 758:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 29, 'num_chars': 512}
Text: ght climate_table_data_td"})

          precip_text =  last_three_cells[0].get_text()
          precip_all.append(precip_text)

          snow_all_text =  last_three_cells[1].get_text()
          snow_all.append(snow_all_text)

          snow_depth_all_text =  last_three_cells[2].get_text()
          snow_depth_all.append(snow_depth_all_text)

denton_climate = pd.DataFrame({
    "Day": day_all,
    "High (°F)": high_all,
    "Low (°F)": low_all,
    "Precip.(inch)":precip_all,
    "Snow (inch)": snow_all,
 


Chunk 759:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 74, 'num_chars': 512}
Text: (inch)":precip_all,
    "Snow (inch)": snow_all,
    "Snow depth (inch)":snow_depth_all
})
denton_climate



# Of course, you can also save into other format!

# # Exercise 2:
# 
# I want to collect some legal cases for text classification, I need the text and category information. If I use keyword and directly download the data from courtlistener (https://www.courtlistener.com/), a non-profit legal search engine, I cannot get the category information. Therefore, I download the data from westlaw, a business


Chunk 760:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 71, 'num_chars': 512}
Text: fore, I download the data from westlaw, a business legal search engine, which has cases under each category. However, since this is a business legal search engine, **we don't have the access to get the full text**. Therefore, I donwloaded the metadata first, then search from courtlistener by using the metadata to get the full text. The metadata is saved in the csv file Criminal_Law_Cases.csv, which can be accessed at: https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/web_scraping_examples/Criminal


Chunk 761:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 70, 'num_chars': 512}
Text: ALL2020/blob/master/web_scraping_examples/Criminal_Law_Cases.csv. Write a program to get the full text listed in the csv file and save them into txt files.  

# In[ ]:


from __future__ import unicode_literals

import time
import  urllib.request
from bs4 import BeautifulSoup
import string
import pandas as pd
import codecs
import urllib.parse


# read csv file from local, return the title list and transfer the titles into queries
from requests import HTTPError


def westlaw_data_process(filepath):
    df = p


Chunk 762:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 55, 'num_chars': 512}
Text: r


def westlaw_data_process(filepath):
    df = pd.read_csv (filepath,error_bad_lines=False)
    query_list = []
    title_list = df['Title'].tolist()
    print(title_list)
    for item in title_list:
        case_title = str (item)
        query = case_title.translate (str.maketrans ('' , '' , string.punctuation)).replace (" " , "+")
        query_list.append(query)
    print(query_list)
    return query_list

# Given an url, download the webpage (only the case content part) into local
def webpage_downloa


Chunk 763:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 47, 'num_chars': 512}
Text:  case content part) into local
def webpage_download(savepath, url):
    connection = urllib.request.urlopen(url)
    webpage_obj = connection.read ()
    webpage = webpage_obj.decode ('ascii','ignore')
    soup = BeautifulSoup(webpage,'html.parser')
    raw_case = soup.find ("article" , {"class":"col-sm-9"})
    case_infor = soup.find("h2" , {"class":"inline"})
    case_title = case_infor.get_text().translate (str.maketrans ('' , '' , string.punctuation))
    clean_case_title = case_title[0:50]
    # print(


Chunk 764:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 44, 'num_chars': 512}
Text:   clean_case_title = case_title[0:50]
    # print(case_infor.get_text())
    string_case = str(raw_case)
    print(string_case)
    if type(string_case) == str:
        try:
            # string_case_clean = re.sub(r'[^\x00-\x7F]+',' ', string_case)
            string_case_clean = ''.join ([i if ord (i) < 128 else ' ' for i in string_case])
            print(string_case_clean)
            file1 = codecs.open (savepath+"//"+clean_case_title+".txt", "w", encoding="utf-8",errors='ignore')
            file1.wri


Chunk 765:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 36, 'num_chars': 512}
Text: ing="utf-8",errors='ignore')
            file1.write (string_case_clean)
            file1.close()
        except UnicodeEncodeError:
            pass
    else:
        print("unicode value")

# use the query accessed from the csv file to search from courtlistener
def courtlistener_crawler(savepath, filepath):
    query_list = westlaw_data_process(filepath)
    for query in query_list:
        query_search_url = "https://www.courtlistener.com/?type=o&q=&type=o&order_by=score+desc&case_name="+query+"&stat_Pr


Chunk 766:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 32, 'num_chars': 512}
Text: =o&order_by=score+desc&case_name="+query+"&stat_Precedential=on"
        try:
            connection = urllib.request.urlopen(query_search_url)
            print(connection)
            webpage_obj = connection.read()
            webpage = webpage_obj.decode ('utf-8')

            # print (webpage)
            soup = BeautifulSoup(webpage,'html.parser')
            results = soup.find ("div" , {"id":"search-results"})
            # print(results)

            # tag_soup = results.article
            tag_sou


Chunk 767:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 33, 'num_chars': 512}
Text:   # tag_soup = results.article
            tag_soup = results.find_all("article")
            print(tag_soup)
            print(len(tag_soup))
            if len(tag_soup)>=1:
                for item in tag_soup:
                    tags = item.find_all ("a",{"class":"visitable"})
                    print(tags)
                    for tag in tags:
                        paper_url = tag.get ('href')
                        # print(paper_url)
                        absolute_paper_url = "https://www.courtl


Chunk 768:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 40, 'num_chars': 512}
Text:           absolute_paper_url = "https://www.courtlistener.com/"+paper_url
                        print(absolute_paper_url)
                        webpage_download (savepath,absolute_paper_url)
                        time.sleep(10)
            else:
                print("no results found")
        except HTTPError as e:
            if e.code == 429:
                time.sleep (3600)


if __name__ == '__main__':
    # change the path to where you want to save the data
    savepath = "D://Academic_Study//U


Chunk 769:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 59, 'num_chars': 512}
Text: ave the data
    savepath = "D://Academic_Study//UNT_PHD//projects//courtlistener_data//Corporations"
    # change the path to where the files you have saved
    filepath = "D://Academic_Study//UNT_PHD//projects//Legal_word_embedding_and_fine tuning//Data collection//Corporations.csv"
    courtlistener_crawler (savepath , filepath)


# # 8. Collecting data from Social media (via API) by using Python 

# Like many programmers who have worked on large projects, I have my share of horror stories when it comes 


Chunk 770:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 77, 'num_chars': 512}
Text: , I have my share of horror stories when it comes to **working with other people’s code**. **From namespace issues to type issues to misunderstandings of function output**, simply trying to get information from point A to method B can be a nightmare.

# This is where **application programming interfaces (API)** come in handy: **they provide nice, convenient interfaces between multiple disparate applications**. It doesn’t matter if the applications are written by different programmers, with different archite


Chunk 771:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 86, 'num_chars': 512}
Text: n by different programmers, with different architectures, or even in different languages — **APIs are designed to serve as a common concept between different pieces of software that need to share information with each other.**

# Although various APIs exist for a variety of different software applications, in
# recent times “API” has been commonly understood as meaning “web application
# API.” Typically, a programmer will make a request to an API via HTTP for some
# type of data, and the API will return thi


Chunk 772:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 90, 'num_chars': 512}
Text: r some
# type of data, and the API will return this data in the form of XML or JSON.
# Although most APIs still support XML, JSON is quickly becoming the encoding
# protocol of choice.

# **Example: Twitter API**:
# 
# https://developer.twitter.com/en/docs/api-reference-index

# **A little bit introduction of how API works**

# **Methods**
# 
# There are four ways to request information from a web server via HTTP:
# 
# 
# 
# 1.   **GET**: GET is the method you are using when you want to get information from


Chunk 773:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 98, 'num_chars': 512}
Text: ou are using when you want to get information from the web
# 2.   **POST**: POST is what you use when you fill out a form, or submit information, presumably to a backend script on the server (Every time you log into a website, you are making a POST request with your username and (hopefully) encrypted password.)
# 3.   PUT: A PUT request is used to update an object or information. An API might require a POST request to create a new user, for example, but it might need a PUT request if you want to update that


Chunk 774:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 92, 'num_chars': 512}
Text: ight need a PUT request if you want to update that user’s email address.
# 4.   DELETE: it is used to delete an object.
# 
# 
# 
# 

# **Authentication**
# 
# Although some APIs do not use any authentication to operate (meaning anyone can
# make an API call for free, without registering with the application first), **many modern APIs require some type of authentication before they can be used**.
# 
# Some APIs require authentication in order to charge money per API call, or they
# might offer their service 


Chunk 775:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 93, 'num_chars': 512}
Text: per API call, or they
# might offer their service on some sort of a monthly subscription basis. Others
# authenticate in order to “rate limit” users (restrict them to a certain number of calls
# per second, hour, or day), or to restrict the access of certain kinds of information or
# types of API calls for some users. Other APIs might not place restrictions, but they
# might want to keep track of which users are making which calls for marketing
# purposes.
# 
# **All methods of API authentication generally 


Chunk 776:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 93, 'num_chars': 512}
Text:  
# **All methods of API authentication generally revolve around the use of a token of some sort, which is passed to the web server with each API call made.** This token
# is either provided to the user when the user registers and is a permanent fixture of
# the user’s calls (generally in lower-security applications), or it can frequently
# change, and is retrieved from the server using a username and password
# combination.
# 
# 

# **Responses**
# 
# In json format or XML format
# 
# Example of XML format


Chunk 777:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 67, 'num_chars': 512}
Text: on format or XML format
# 
# Example of XML format: \<user>\<firstname>Ryan\</firstname>\<lastname>Mitchell\</lastname>\<username>Kludgist\</username>\</user>
# 
# Example of json format: {"user":{"firstname":"Ryan","lastname":"Mitchell","username":"Kludgist"}}
# 
# In recent years, JSON has become vastly more popular than XML

# # 8.1 Connect data from Twitter
# 
# 

# Twitter is notoriously protective of its API and rightfully so. With over 230 million active users and a revenue of over $100 million a mon


Chunk 778:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 95, 'num_chars': 512}
Text: ive users and a revenue of over $100 million a month, the company is hesitant to let just anyone come along and have any data they want.
# 
# 
# Twitter’s rate limits (the number of calls it allows each user to make) fall into two categories: 15 calls per 15-minute period, and 180 calls per 15-minute period, depending on the type of call. For instance, you can make up to 12 calls a minute to retrieve basic information about Twitter users, but only one call a minute to retrieve lists of those users’ Twitter 


Chunk 779:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 60, 'num_chars': 512}
Text:  minute to retrieve lists of those users’ Twitter followers.
# 
# Twitter API: https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference

# # 8.1.1 Extracting Tweets of a particular user from twitter

# In[ ]:


import json

# create a dictionary to store your twitter credentials

twitter_cred = dict()

# Enter your own consumer_key, consumer_secret, access_key and access_secret
# Replacing the stars ("********")

# twitter_cred['CONSUMER_KEY'] = '***********************


Chunk 780:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 27, 'num_chars': 512}
Text: er_cred['CONSUMER_KEY'] = '***********************'
# twitter_cred['CONSUMER_SECRET'] = '***********************'
# twitter_cred['ACCESS_KEY'] = '***********************'
# twitter_cred['ACCESS_SECRET'] = '***********************'

twitter_cred['CONSUMER_KEY'] = 'u7L1lnR7HN85dn1qnTFO1cegb'
twitter_cred['CONSUMER_SECRET'] = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'
twitter_cred['ACCESS_KEY'] = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO'
twitter_cred['ACCESS_SECRET'] = 'gxWMOodDq1nQAjix9mH


Chunk 781:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 66, 'num_chars': 512}
Text: itter_cred['ACCESS_SECRET'] = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce'



# Save the information to a json so that it can be reused in code without exposing
# the secret info to public

with open('twitter_credentials.json', 'w') as secret_info:
  json.dump(twitter_cred, secret_info, indent=4, sort_keys=True)


# Your saved json will look like the one shown below. It will be saved in the current directory under the name twitter_credentials.json.
# 
# {
# 
# "ACCESS_KEY": "***********************",
# 



Chunk 782:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 77, 'num_chars': 512}
Text: 
# 
# "ACCESS_KEY": "***********************",
# 
# "ACCESS_SECRET": "***********************",
# 
# "CONSUMER_KEY": "***********************",
# 
# "CONSUMER_SECRET": "***********************"
# 
# }
# 

# Coming to the code now, you have to do a pip install to install the tweepy module that will help us interact with the twitter api more easily. 

# In[ ]:


get_ipython().system('pip install tweepy')


# Once tweepy is installed, you can run this code to get the tweets of a person in csv format (with some


Chunk 783:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 62, 'num_chars': 512}
Text: et the tweets of a person in csv format (with some limitations.)

# In[ ]:


import tweepy
import csv
import json

# load Twitter API credentials

with open('twitter_credentials.json') as cred_data:
  info = json.load(cred_data)
  consumer_key = info['CONSUMER_KEY']
  consumer_secret = info['CONSUMER_SECRET']
  access_key = info['ACCESS_KEY']
  access_secret = info['ACCESS_SECRET']

def get_all_tweets(screen_name):

# Twitter allows access to only 3240 tweets via this method

# Authorization and initializat


Chunk 784:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 62, 'num_chars': 512}
Text: s via this method

# Authorization and initialization

  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
  auth.set_access_token(access_key, access_secret)
  api = tweepy.API(auth)

  # initialization of a list to hold all Tweets

  all_the_tweets = []

  # We will get the tweets with multiple requests of 200 tweets each

  new_tweets = api.user_timeline(screen_name=screen_name, count=200)

  # saving the most recent tweets

  all_the_tweets.extend(new_tweets)

  # save id of 1 less than the oldes


Chunk 785:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 66, 'num_chars': 512}
Text: (new_tweets)

  # save id of 1 less than the oldest tweet

  oldest_tweet = all_the_tweets[-1].id - 1

  # grabbing tweets till none are left

  while len(new_tweets) > 0:
    # The max_id param will be used subsequently to prevent duplicates
    new_tweets = api.user_timeline(screen_name=screen_name,
    count=200, max_id=oldest_tweet)

    # save most recent tweets

    all_the_tweets.extend(new_tweets)

    # id is updated to oldest tweet - 1 to keep track

    oldest_tweet = all_the_tweets[-1].id - 1
  


Chunk 786:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 62, 'num_chars': 512}
Text: k

    oldest_tweet = all_the_tweets[-1].id - 1
    print ('...%s tweets have been downloaded so far' % len(all_the_tweets))

  # transforming the tweets into a 2D array that will be used to populate the csv

  outtweets = [[tweet.id_str, tweet.created_at,
  tweet.text.encode('utf-8')] for tweet in all_the_tweets]

  # writing to the csv file

  with open(screen_name + '_tweets.csv', 'w', encoding='utf8') as f:
    writer = csv.writer(f)
    writer.writerow(['id', 'created_at', 'text'])
    writer.writerows


Chunk 787:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 81, 'num_chars': 512}
Text: ['id', 'created_at', 'text'])
    writer.writerows(outtweets)

if __name__ == '__main__':

  # Enter the twitter handle of the person concerned

  get_all_tweets(input("Enter the twitter handle of the person whose tweets you want to download:- "))


# You can see that certain messages are being displayed once you enter a twitter handle. These messages are actually the way the tweets are being downloaded, in packets, few at a time. Once the task is done, you can see that a file has been created in your curre


Chunk 788:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 99, 'num_chars': 512}
Text: can see that a file has been created in your current directory. The link to my file is given, so that you can download it and check.
# 
# Here are some other twitter handles for you to checkout-
# 
# 
# *   Donald J. Trump
# *   Amazon
# *   Other?
# 
# 

# # 8.1.2 Downloading all images uploaded by a twitter user:
# 
# Image analysis is in full swing these days, and what would be a better place to get images than twitter, where millions of people upload images containing texts and information every single 


Chunk 789:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 92, 'num_chars': 512}
Text: ges containing texts and information every single day. Here  is the code that can help you download images uploaded by any twitter user. Just like the earlier code block, for this one as well, you will need to have the file twitter_credentials.json in the same folder as the program, before you run it, so as to get proper results. Make sure you execute a pip install wget before running this program since we will be using the wget module to download the images found in the user’s tweets.

# In[ ]:


get_ipyth


Chunk 790:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 50, 'num_chars': 512}
Text:  found in the user’s tweets.

# In[ ]:


get_ipython().system('pip install wget')


# In[ ]:


import tweepy
from tweepy import OAuthHandler
import json
import wget

#Twitter Authentication
with open('twitter_credentials.json') as cred_data:
  info = json.load(cred_data)
  consumer_key = info['CONSUMER_KEY']
  consumer_secret = info['CONSUMER_SECRET']
  access_token = info['ACCESS_KEY']
  access_secret = info['ACCESS_SECRET']

  auth = OAuthHandler(consumer_key, consumer_secret)
  auth.set_access_token(acce


Chunk 791:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 57, 'num_chars': 512}
Text: key, consumer_secret)
  auth.set_access_token(access_token, access_secret)

  #Creating tweepy api
  api = tweepy.API(auth)

  user = input("Enter twitter screen_name - ")
  for status in tweepy.Cursor(api.search, q=hashtag).items(500):
    try:
       for media in status.extended_entities['media']:
          print(media['media_url'])
    except AttributeError:
          pass 


# Once you run it, you will be asked for the twitter screen name whose images you want to download. On entering the id, the progra


Chunk 792:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 94, 'num_chars': 512}
Text: u want to download. On entering the id, the program will run for some time and then start downloading the images one by one.

# # 8.1.3 Extracting tweets containing a particular hashtag from twitter:
# 
# The code given next, can be used to extract n number of tweets with a given hashtag into a text file. Both the number of tweets and the hashtag itself are user inputs and the scraping will happen only when you have provided both the inputs. I must mention again that you need to have the twitter_credentials


Chunk 793:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 62, 'num_chars': 512}
Text: gain that you need to have the twitter_credentials.json in the same folder as this file, to make sure that the program runs.

# In[ ]:


import tweepy
import csv
import json

# Twitter API credentials

with open('twitter_credentials.json') as cred_data:
  info = json.load(cred_data)
  consumer_key = info['CONSUMER_KEY']
  consumer_secret = info['CONSUMER_SECRET']
  access_key = info['ACCESS_KEY']
  access_secret = info['ACCESS_SECRET']

  # Create the api endpoint

  auth = tweepy.OAuthHandler(consumer_key,


Chunk 794:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 63, 'num_chars': 512}
Text: dpoint

  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
  api = tweepy.API(auth)

  # Mention the maximum number of tweets that you want to be extracted.

  maximum_number_of_tweets_to_be_extracted = int(input('Enter the number of tweets that you want to extract- '))

  # Mention the hashtag that you want to look out for

  hashtag = input('Enter the hashtag you want to scrape- ')

  for tweet in tweepy.Cursor(api.search, q='#' + hashtag, rpp=100).items(maximum_number_of_tweets_to_be_extracted):


Chunk 795:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 64, 'num_chars': 512}
Text: ).items(maximum_number_of_tweets_to_be_extracted):
    with open('tweets_with_hashtag_' + hashtag + '.txt', 'a') as the_file:
       the_file.write(str(tweet.text.encode('utf-8')) + '\n')
  print ('Extracted ' + str(maximum_number_of_tweets_to_be_extracted) + ' tweets with hashtag #' + hashtag)


# # Question: Why using Tweepy with Twitter API?

# You might have noticed that we are using the Tweepy module to interact with the twitter API. To know more about it, you can check out this [link](https://www.pyth


Chunk 796:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 59, 'num_chars': 512}
Text: it, you can check out this [link](https://www.pythoncentral.io/introduction-to-tweepy-twitter-for-python/). It helps in authentication of Twitter API using the OAuth method, which requires the consumer_key, consumer_secret, access_token and the access_token_secret. 
# 
# With only three lines of code the twitter API is authenticated using the OAuth format-

# In[ ]:


auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
# Creation of the the api 


Chunk 797:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 95, 'num_chars': 512}
Text: n, access_token_secret)
# Creation of the the api interface, using authentication
api = tweepy.API(auth)


# OAuth is a bit more complicated than the basic authentication that was used by twitter before. However, the advantages are multiple-
# 
# 
# 
# *   The user password is not relied on, and even if the user changes the secret keys, it still works. Thus there is no fear in case someone gets to know the secret keys. They can be changed.
# *   Access can be set as read only for a set of keys and tokens. I


Chunk 798:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 98, 'num_chars': 512}
Text: e set as read only for a set of keys and tokens. In this way, even if someone steals them, he will only get read access.
# *   Tweets can be customised in such a fashion that the app from which they have originated will be visible.
# 
# One of the biggest advantages of using Tweepy is the Tweepy StreamingAPI. It can be used to monitor tweets in real time, so as to track events as and when they happen. The main component of this functionality is the StreamListener object, that helps monitors tweets in real t


Chunk 799:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 90, 'num_chars': 512}
Text: tener object, that helps monitors tweets in real time and which allows you to grab them.

# # Sumary of collecting data from twitter

# Extraction of twitter data is easy, but utilising them requires a knowledge of **data cleaning, presentation, and more**. Tweets can be cleaned, data from them can be extracted and the data can be presented on a world map displaying which types of tweets are more frequent in urban areas. **Studies using twitter data can tell us a lot about society, mindset of people, the pr


Chunk 800:
Document ID: 2d94686b-500e-4f8b-9aec-b812f8eb3b0e
Metadata: {'file_name': 'Lesson four - lecture examples.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 4/Lesson four - lecture examples.ipynb', 'folder_name': 'Module 4', 'num_tokens': 57, 'num_chars': 312}
Text:  us a lot about society, mindset of people, the problems faced by people in day to day life, the latest advancements, and the latest news**. **Real time twitter data analysis can be used to find and help people in distress, be it in the event of a natural disaster, or saving a woman from an abusive husband**. 



Chunk 801:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 63, 'num_chars': 512}
Text: Haihua Chen, Ph.D. https://iia.ci.unt.edu/haihua-chen haihua.chen@unt.edu Sep 18, 2024
Department of Information Science
Data Evaluation and Improvement for Machine Learning
9/15/24
1
Content: Background Statement of The Problem Purpose of The Study Definitions Research Questions Outline of The Study Literature Review Methodology Domain-specific Machine Learning Dataset Construction and Quality Evaluation Data Quality Improvement for Performance Enhancement of Machine Learning Conclusion and Future Work Ref


Chunk 802:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 71, 'num_chars': 512}
Text: of Machine Learning Conclusion and Future Work References Acknowledgement
Background
9/15/24
3
The determinants of AI success Data ML models/algorithms The widely applications of ML and DL AlphaGo Amazon Alexa Chatbots Autopiloting
Background (Cont.)
9/15/24
4
AI researchers and practitioners overwhelmingly concentrate on models/algorithms, while undervalue the data quality. (Sambasivan et al., 2021) Garbage in, garbage out Ex.1: 0.8% error rate for recognizing the faces of lighter-skinned males, but as hig


Chunk 803:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 77, 'num_chars': 512}
Text: ing the faces of lighter-skinned males, but as high as 34.% error rate for recognizing the faces of darker-skinned females. (Buolamwini et al., 2018) Ex.2: IBM’s cancer treatment AI accuracy reduced. (Strickland et al., 2019) Ex.3: Google Flu Trends missing the flu peak by 140%. (Kandula and Shaman, 2019) Data quality for machine learning has not been properly and systematically validated. There are no “one-size-fits-all” solutions for data quality evaluation and improvement.
Statement of the Problem
9/15/2


Chunk 804:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 78, 'num_chars': 512}
Text: n and improvement.
Statement of the Problem
9/15/24
5
Data quality is essential for machine learning, nevertheless it has not been rigorously evaluated.
Purpose of the Study
9/15/24
6
We aim to explore how to construct high-quality datasets, evaluate and improve the data quality for domain-specific machine learning systems.
Definitions
9/15/24
7
Definition of Concepts Data quality Measure the quantitative and qualitative properties of data. Data that are fit for use by data consumers. (Wang and Strong, 1996


Chunk 805:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 82, 'num_chars': 512}
Text:  for use by data consumers. (Wang and Strong, 1996) The capability of the data for “fit for purpose” of building a machine learning system. (Chen et al., 2021) Data quality problems: a set of issues that can affect the potentiality of the applications that use the data Lacking enough training data Invalid data and label noises Significant imbalanced training data Duplication . . .
Definitions (Cont.)
9/15/24
8
Definition of Concepts High-quality data (“good data”) “Fit for purpose” of applications. Four bas


Chunk 806:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 73, 'num_chars': 512}
Text: data”) “Fit for purpose” of applications. Four basic standards for high-quality data in machine learning defined by Andrew: defined consistently, cover of important cases, has timely feedback from production data, and sized appropriately. (Andrew, 2021) Data quality dimensions: a set of data quality attributes that represent a single aspect or construct of data quality (Wang and Strong, 1996) Comprehensiveness Correctness Variety Completeness Timeliness . . .
Definitions (Cont.)
9/15/24
9
Definition of Conc


Chunk 807:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 68, 'num_chars': 512}
Text: .
Definitions (Cont.)
9/15/24
9
Definition of Concepts Data quality evaluation (“data quality assessment”) How to evaluate each data quality dimension. (Cai et al., 2015) Qualitatively method: performed by subject experts or professionals. Quantitatively method: a formal, objective, and systematic process in which numerical data are utilized to obtain information. Data quality evaluation results offer guidelines of data quality improvement. Data quality improvement Detect and remove errors, inconsistencies,


Chunk 808:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 73, 'num_chars': 512}
Text: ovement Detect and remove errors, inconsistencies, and other data quality issues. Data quality can be improved systematically and iteratively. (Andrew, 2021)
Research Questions
9/15/24
10
RQ1: How to construct a high-quality, domain-specific, and large-scale machine learning corpus using semi-automatic approaches? RQ2: How to systematically evaluate the data quality for machine learning? RQ3: What are the techniques that can be used to improve the data quality? RQ4: What are the best practice of implementin


Chunk 809:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 72, 'num_chars': 512}
Text: ty? RQ4: What are the best practice of implementing machine learning techniques for data quality improvement?
Outline of The Study
9/15/24
11
Figure 1: Outline of this study.
Literature Review
9/15/24
12
Overview Data Quality Data Quality for ML Data Quality Improvement Techniques Domain Text Classification Machine Learning Model Selection
Literature Review - Data Quality
9/15/24
13
Summary of existing data quality evaluation frameworks Wang et al. (1996); intrinsic, contextual, representational, accessibil


Chunk 810:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 61, 'num_chars': 512}
Text: ntrinsic, contextual, representational, accessibility; 15 metrics Bovee et al. (2003); credibility, accessibility, interpretability, relevance; 9 metrics Price et al. (2005); syntactic, semantic, pragmatic; 13 metrics Stvilia et al. (2007); intrinsic, representational, relational; 22 metrics Arazy et al. (2010); intrinsic, contextual, representational; 4 metrics Cai et al. (2015); availability, usability, reliability, relevance, presentation quality; 14 metrics Zaveri et al. (2016); accessibility, intrinsic


Chunk 811:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text: ics Zaveri et al. (2016); accessibility, intrinsic, trust, dataset dynamicity, contextual, representational; 22 sub-dimensions and 97 metrics Chen et al. (2019); intrinsic, contextual, representational, accessibility; 18 metrics . . .
Literature Review - Data Quality
9/15/24
14
Figure 2: A conceptual framework of data quality. It has been reused by many recent data quality studies. Meanwhile, modification is made to fit the DQ framework into different purposes.
Literature Review - Data Quality for ML
9/15/2


Chunk 812:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 78, 'num_chars': 512}
Text: es.
Literature Review - Data Quality for ML
9/15/24
15
Data quality largely determines the performance, fairness, robustness, safety, and scalability of ML and AI systems. (Sambasivan et al. 2021) Identify the data quality issues such as data errors and missing values in data mining and machine learning. (Hulse, 2007) How many percentages of label noise can affect the ML performance? (Lauria and Tayi, 2008) Trustworthiness or reliability is another critical data quality dimension of machine learning. (Dong,


Chunk 813:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text: data quality dimension of machine learning. (Dong, 2015) How do different data quality features impact machine learning systems. (Foidl and Felderer, 2019) A system for automating the verification of the data quality of large-scare data, aiming to fit the purpose for business and decision making. (Schelter et al., 2018) Insufficient training data has become a common data quality issue for machine learning. (Lourentzou, 2019) Other data quality issues: duplication, highly correlated variables, large number o


Chunk 814:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 82, 'num_chars': 512}
Text: ation, highly correlated variables, large number of variables, outliers, data source bias, etc. (Gudivada et al., 2017)
Literature Review - Data Quality for ML
9/15/24
16
Status of Data Quality Research for ML Improving data quality as “operational” vs building novel models and algorithms. (Sambasivan et al., 2021) Improving the data might be more effective than improving the model. (Andrew, 2021) Tutorials related to data quality in ML and DL. (Jain et al., 2020; Whang and Lee, 2020)
Research Gap Data qual


Chunk 815:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 73, 'num_chars': 512}
Text:  2020; Whang and Lee, 2020)
Research Gap Data quality of ML has not been appropriately validated. Discussions on the impact of data quality on the ML performance with evidence and measurement is rare. Improving the ML performance by improving data quality instead of model construction deserves more attention.
Literature Review - Data Quality Improvement
9/15/24
17
Semi-supervised Learning (Van Engelen and Hoos, 2020) Co-training (Aridas and Kotsiantis, 2015) Active Learning (Lourentzou, 2019) Expectation-ma


Chunk 816:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 67, 'num_chars': 512}
Text:  Active Learning (Lourentzou, 2019) Expectation-maximization (Liu et al., 2003; Brefeld and Scheffer, 2004; Chokka and Rani, 2020) Generative Adversarial Learning (Goodfellow et al., 2014; Croce et al., 2020) Transfer Learning (Zhuang et al., 2020) Few-shot Learning (Wang et al., 2020)
Literature Review - Domain-specific Text Classification
9/15/24
18
Legal Argument Mining Argument/non-argument detection for different text types, multinomial NB (Moens et al., 2007). Legal argumentation structure detection, 


Chunk 817:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 71, 'num_chars': 512}
Text: , 2007). Legal argumentation structure detection, SVM (Palau et al., 2009). Argument classification, five categories, C4.5 decision tree (Feng and Hirst, 2011). Argument extraction for legal information retrieval, six categories, a theoretical discussion (Ashley, 2014). Five years of argument mining, literature review (Cabrio and Villata, 2018). Argument mining: a survey (comprehensive) (Lawrence and Reed, 2019). Predict charges for legal judgment, four categories (Le et al., 2020)
Figure 3: The pipeline of


Chunk 818:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 77, 'num_chars': 512}
Text: gories (Le et al., 2020)
Figure 3: The pipeline of legal argument mining (top) and the Legal Arguments in a Legal Case (bottom).
Literature Review - Domain-specific Text Classification
9/15/24
19
Medical Concept Normalization Arizona Disease Corpus (AZDC); with/without rule-based NLP module (Kang et al., 2013). The NCBI disease corpus; pairwise learning to rank (Leaman et al., 2013). TwADR-S, TwADR-L, and AskAPatient; CNN and RNN (Limsopatham and Collier, 2016). TwADR-L and AskAPatient; CNN and RNN (Lee et 


Chunk 819:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 73, 'num_chars': 512}
Text: 16). TwADR-L and AskAPatient; CNN and RNN (Lee et al, 2017). ShARe/CLEF eHealth dataset and the NCBI disease dataset; handcrafted rules with CNN model (Li et al, 2017). TwADR-L, AskAPatient, and ChMCN; multi-task character-level attentional network (Niu et al., 2019). CADEC corpus and PsyTAR corpus; Text embedding and graph embedding (Pattisapu et al., 2020). CADEC-MCN and TwADR-L; BertForSequenceClassification and BERT+Highway Network (Kalyan et al., 2021).
Figure 4: Medical Concept Normalization (Niu et a


Chunk 820:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text: 
Figure 4: Medical Concept Normalization (Niu et at., 2019)
Literature Review - Model Selection for ML
9/15/24
20
Figure 5: Existing machine learning models organized by timeline (Li at al., 2020).
Methodology – Framework of ML System Construction
9/15/24
21
Figure 6: Machine learning lifecycle by considering data evaluation and model construction
Methodology – Framework of Data Quality Evaluation and Improvement
9/15/24
22
Figure 7: Data quality evaluation and improvement framework for machine learning
Met


Chunk 821:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 63, 'num_chars': 512}
Text: and improvement framework for machine learning
Methodology - Task Description
9/15/24
23
Figure 7: Machine learning tasks in this study (left: legal argument mining, right: medical concept normalization)
Medical concept normalization
Legal argument mining
Methodology – Data Quality Evaluation
9/15/24
24
Data Quality Dimensions Comprehensiveness Correctness Duplication Class imbalance
Data Quality Evaluation Approaches Quantitative methods Statistical analysis Experimental study
Methodology - Algorithms
9/15


Chunk 822:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 63, 'num_chars': 512}
Text: s Experimental study
Methodology - Algorithms
9/15/24
25
List of Algorithms Text Representation Text Classification Algorithms Handling Class Imbalance Data Augmentation Transfer Learning
Methodology – Algorithms (Cont.)
9/15/24
26
Text Representation TF-IDF ULMFit BERT Text Classification Algorithms Random forests Support vector machine Light gradient boosting machine RNN BERT
Methodology - Algorithms (Cont.)
9/15/24
27
Handling Class Imbalance Over-sampling Down-sampling Synthetic Minority Oversampling Te


Chunk 823:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 54, 'num_chars': 512}
Text: g Down-sampling Synthetic Minority Oversampling Technique (SMOTE) Down-sampling + over-sampling + SMOTE (Mixed-sampling) Data Augmentation Pseudo-labeling Co-training Expectation-maximization Generative adversarial network Transfer Learning Fine-tuning Task-specific Machine Learning Model Fine-tuning Language Model
Methodology – Performance Evaluation
9/15/24
28
Evaluation Metrics for Machine Learning Performance Accuracy Precision Recall F1-score
Domain-specific Machine Learning Dataset Construction and Qu


Chunk 824:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 73, 'num_chars': 512}
Text: cific Machine Learning Dataset Construction and Quality Evaluation
9/15/24
29
Data Collection Data Collection for Legal Argument Mining Data source Initial data: Harvard Law Library case law corpus. Data used in this research: 27,712 criminal cases from 1840 to 2018 in Texas (JSON format) Data cleaning LexNLP for sentence splitting. Feature-based ML for invalid sentences removing (0.9158 in accuracy). 542,763 validated sentences for annotation and data augmentation. Data Collection for Medical Concept Norma


Chunk 825:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 64, 'num_chars': 512}
Text: ntation. Data Collection for Medical Concept Normalization Datasets for Constructing Machine Learning Systems for MCN Dataset TwADR-L Dataset AskAPatient Datasets for Transfer Learning for Quality Improvement Cadec dataset, Pubmed dataset, Healthnews dataset, Big tweet dataset
Domain-specific Machine Learning Dataset Construction and Quality Evaluation - Legal
9/15/24
30
Legal Argument Mining Dataset Construction
Table 1: Annotation Scheme for Legal Argument
Domain-specific Machine Learning Dataset Construc


Chunk 826:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 65, 'num_chars': 512}
Text: 
Domain-specific Machine Learning Dataset Construction and Quality Evaluation - Legal
9/15/24
31
Legal Argument Mining Dataset Construction Human annotation An annotation scheme for legal argument Annotation tool: doccano Annotation procedure Inter-agreement evaluation: Kappa value Quality assurance of the annotation
Table 2: Summary of the dataset from the annotation
Domain-specific Machine Learning Dataset Construction and Quality Evaluation
9/15/24
32
Data quality evaluation – Experiment design Experimen


Chunk 827:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 70, 'num_chars': 512}
Text: a quality evaluation – Experiment design Experiments on annotation quality evaluation to check the correctness of the labeling. (Legal) Experiments to investigate the data sufficiency to check the comprehensiveness of the data. (Legal) Experiments to investigate the class imbalance issue of the dataset. (Legal) Experiments to investigate the overclaimed performance issue of the medical concept normalization system due to the duplication in the datasets. (Medical)
Domain-specific Machine Learning Dataset Con


Chunk 828:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 66, 'num_chars': 512}
Text: ical)
Domain-specific Machine Learning Dataset Construction and Quality Evaluation (Cont.)
9/15/24
33
Data quality evaluation - Experiment Results Results on Correctness Evaluation (Legal)
Table 3: Performance on different training datasets using BERT
Domain-specific Machine Learning Dataset Construction and Quality Evaluation (Cont.)
9/15/24
34
Data quality evaluation - Experiment Results Results on Comprehensiveness Evaluation (Legal)
Figure 8: The performance of BERT Model by using different portions of 


Chunk 829:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 69, 'num_chars': 512}
Text: ance of BERT Model by using different portions of data for training
Domain-specific Machine Learning Dataset Construction and Quality Evaluation (Cont.)
9/15/24
35
Data quality evaluation - Experiment Results Results on Class Imbalance Evaluation (Legal)
Figure 9: The performance of using different methods to handing class imbalance
Domain-specific Machine Learning Dataset Construction and Quality Evaluation (Cont.)
9/15/24
36
Data quality evaluation - Experiment Results Results on Duplication Evaluation (M


Chunk 830:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 69, 'num_chars': 512}
Text: iment Results Results on Duplication Evaluation (Medical)
Figure 10: The accuracy of normalizing medical concepts on datasets AskAPatient and TwADR-L with different percentages of overlapped records in the test dataset.
Domain-specific Machine Learning Dataset Construction and Quality Evaluation (Cont.)
9/15/24
37
Data quality evaluation - Discussion The commonly used kappa agreement might not be sufficient to judge the annotated data quality, or the labeled correctness. Incomprehensiveness is another data 


Chunk 831:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 74, 'num_chars': 512}
Text:  correctness. Incomprehensiveness is another data quality issue of the legal argument mining dataset, which can be detected by using different amount of data for the model training. There is a trade-off between correctness and comprehensiveness of a dataset for training a high-performance machine learning system. Many techniques can be applied for handling the data imbalance in text classification. When data is insufficient, over-sampling is a better choice. However, when large categories exist, mixed sampl


Chunk 832:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text:  However, when large categories exist, mixed sampling will be more effective Tests the model with a different percentage of the duplication between the training and test datasets is an effective approach to understand whether the duplication between training and test data could contribute to the non-exist high performance of a machine learning model
Data Quality Improvement for Performance Enhancement of Machine Learning
9/15/24
38
Data Quality Improvement Using Data Augmentation - Data Collection (Legal) 4


Chunk 833:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 72, 'num_chars': 512}
Text: sing Data Augmentation - Data Collection (Legal) 4416 annotated sentences for training, 521 sentences for testing, and 542,763 unlabeled sentences were served as the data pool for data augmentation.
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
39
Data Quality Improvement Using Data Augmentation -Experiment design (Legal)
Table 4: Experiments design to test the data quality improvement using data augmentation for legal argument mining
Data Quality Improvement for P


Chunk 834:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 74, 'num_chars': 512}
Text: gal argument mining
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
40
Data Quality Improvement Using Data Augmentation - Experiment Results (Legal) Overall results GAN + BERT achieved the best performance, followed by LegalBERT and BERT model. GAN + BERT enjoyed a 0.0268 in F-1 score improvement than the general BERT model. Deep learning-based models outperform traditional machine learning-based models in the short text classification in the legal domain. The effect


Chunk 835:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 67, 'num_chars': 512}
Text: ext classification in the legal domain. The effectiveness of different data augmentation on performance improvement varies.
Table 5: Performance using different algorithms for data augmentation
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
41
Data Quality Improvement Using Data Augmentation - Experiment Results (Legal) Parameter Analysis - Pseudo-labeling with LightGBM
Figure 11: The performance of pseudo-labeling with different number of unlabeled data (left) and 


Chunk 836:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 68, 'num_chars': 512}
Text: ith different number of unlabeled data (left) and different confidence score (right)
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
42
Data Quality Improvement Using Data Augmentation - Experiment Results (Legal) Parameter Analysis - Co-training
Figure 12: The performance of co-training
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
43
Data Quality Improvement Using Data Augmentation - Experiment Results (Legal) Parameter An


Chunk 837:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 72, 'num_chars': 512}
Text: entation - Experiment Results (Legal) Parameter Analysis - EM with LightGBM
Figure 13: The performance of EM + LightGBM with and without handling class imbalance
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
44
Data Quality Improvement Using Data Augmentation - Experiment Results (Legal) Parameter Analysis - GAN-BERT
Table 6: Performance of GAN-BERT using different number of unlabeled data
Data Quality Improvement for Performance Enhancement of Machine Learning (Co


Chunk 838:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 77, 'num_chars': 512}
Text: or Performance Enhancement of Machine Learning (Cont.)
9/15/24
45
Data Quality Improvement Using Data Augmentation – Results Discussion (Legal) When the data quality is not high enough to build a high-performance machine learning system, data augmentation is useful for the performance improvement of the system to be built on the data. Techniques for handling class imbalance issue that works well for supervised learning are not always effective for semi-supervised learning. The fine-tuning can be used to imp


Chunk 839:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 74, 'num_chars': 512}
Text: vised learning. The fine-tuning can be used to improve data quality by taking advantage of the information brought by the source dataset. GAN can enhance BERT when only few labeled data are available.
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
46
Data Quality Improvement Based on Transfer Learning- Data Collection (Medical)
TwADR-L and AskAPatient for model training, validation, and testing Cadec dataset, Pubmed dataset, Healthnews dataset, and Big tweet dataset


Chunk 840:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 67, 'num_chars': 512}
Text: dataset, Healthnews dataset, and Big tweet dataset for fine-tuning.
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
47
Data Quality Improvement Based on Transfer Learning- Experiment design (Medical)
Table 7: The experiment design of transfer learning for data quality and performance improvement
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
48
Data Quality Improvement Based on Transfer Learning -Experiment Results (Medical) 


Chunk 841:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 69, 'num_chars': 512}
Text: n Transfer Learning -Experiment Results (Medical) Fine-tuning with AskAPatient or TwADR-L
Figure 14: An comparison of the accuracy of medical concept normalization that was fine-tuned or was not fine-tuned on datasets AskAPatient (left) and TwADR-L (right)
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
49
Data Quality Improvement Based on Transfer Learning -Experiment Results (Medical) Fine-tuning with Cadec, Pubmed, Healthnews, Big tweet, and their combinations
Fig


Chunk 842:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 70, 'num_chars': 512}
Text:  Healthnews, Big tweet, and their combinations
Figure 15: The average performance of medical concept normalization that was fine-tuned with different datasets on datasets AskAPatient (left) and TwADR-L (right).
Data Quality Improvement for Performance Enhancement of Machine Learning (Cont.)
9/15/24
50
Data Quality Improvement Based on Transfer Learning – Results Discussion (Medical) Fine-tuning the model can increase the accuracy of medical concept normalization on both datasets. It implies that the perform


Chunk 843:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 80, 'num_chars': 512}
Text: tion on both datasets. It implies that the performance of a model that is trained on poor quality datasets can be improved using well designed fine-tuning. The datasets for fine-tuning should be related to the target-specific task, otherwise the fine-tuning may produce negative performance impact. The fine-tuning can be used to improve data quality by taking advantage of the information brought by the source dataset. A powerful model, such as BERT, can better capture semantic information, thereby can be the


Chunk 844:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 72, 'num_chars': 512}
Text: r capture semantic information, thereby can be the prior model for fine-tuning.
Conclusion
9/15/24
51
RQ1: How to construct a high-quality, domain-specific, and large-scale machine learning corpus using semi-automatic approaches? Answer: Semi-automated approaches: a small high-quality initial data was annotated by a human; then, effective data augmentation algorithms were used to expand the size of the dataset.
Conclusion (Cont.)
9/15/24
52
RQ2: How to systematically evaluate the data quality for machine le


Chunk 845:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 80, 'num_chars': 512}
Text: matically evaluate the data quality for machine learning? Answer: Systematically and quantitatively evaluate the quality of the dataset based on pre-defined data quality dimensions can help developers to take further actions for the data quality and system improvement.
Conclusion (Cont.)
9/15/24
53
RQ3: What are the techniques that can be used to improve the data quality? Answer: We can choose the GAN-BERT model for data augmentation when a small portion of labeled data while a large portion of unlabeled da


Chunk 846:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 77, 'num_chars': 512}
Text: labeled data while a large portion of unlabeled data is available. The fine-tuning can be used to improve data quality by taking advantage of the information brought by the source dataset.
Conclusion (Cont.)
9/15/24
54
RQ4: What are the best practice of implementing machine learning techniques for data quality improvement? Answer: The amount of unlabeled data for data augmentation should be carefully selected for training, otherwise the new labeled data may produce negative performance impact. The datasets 


Chunk 847:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 74, 'num_chars': 512}
Text: produce negative performance impact. The datasets for fine-tuning should be related to the target-specific task. A powerful model, such as BERT, can better capture semantic information, thereby can be the prior model for fine-tuning.
Conclusion – Overall Conclusions
9/15/24
55
Data quality has a significant impact on the machine learning performance, the impact can be quantitively measured by experimental study. The data quality evaluation method should be designed based on different applications and differ


Chunk 848:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 68, 'num_chars': 512}
Text: esigned based on different applications and different purposes. Data quality evaluation results can provide guidance for designing experiments for data quality improvement. Both transfer learning and the data augmentation techniques are not always effective for data quality and system performance improvement. Only carefully designing the experiments and fine-tuning the parameters could produce the desired result.
Future Work
9/15/24
56
Explore more effective and reusable techniques for constructing domain-s


Chunk 849:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 62, 'num_chars': 512}
Text:  and reusable techniques for constructing domain-specific, high-quality, and large-scale machine learning datasets. Create and publish several high-quality datasets regarding legal AI, health care, cyber security, recommendation, and others. Exploring automatic data quality evaluation and improvement approaches regarding different data quality dimensions and applications. Explore more statistic methods for data quality evaluation. Develop effective and user-friendly tools for data quality evaluation and imp


Chunk 850:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text: friendly tools for data quality evaluation and improvement for different machine learning applications Data quality evaluation for graph data such as knowledge graph. Enhance BERT by combining BERT and external domain knowledge in the attention for domain-specific text classification. Explore other techniques such as few shot learning for data augmentation. Others
References (Selected)
9/15/24
57
Wang, R. Y., & Strong, D. M. (1996). Beyond accuracy: What data quality means to data consumers. Journal of mana


Chunk 851:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 85, 'num_chars': 512}
Text: a quality means to data consumers. Journal of management information systems, 12(4), 5-33. Andrew Ng, A chat with andrew on mlops: From model-centric to data-centric ai, 2021, [Online; accessed 09-01-2021] Cai, L., & Zhu, Y. (2015). The challenges of data quality and data quality assessment in the big data era. Data science journal, 14. Sambasivan, N. et al. (2021, May). “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI Conference on Hu


Chunk 852:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 75, 'num_chars': 512}
Text: I. In proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1-15). Lauria, E. J., & Tayi, G. K. (2008). Statistical machine learning for network intrusion detection: a data quality perspective. International Journal of Services Sciences, 1(2), 179-195. Dong, X. et al. (2015). Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources. Proceedings of the VLDB Endowment, 8(9). Lourentzou, I. (2019). Data quality in the deep learning era: Active semi-supervised learning an


Chunk 853:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 72, 'num_chars': 512}
Text: p learning era: Active semi-supervised learning and text normalization for natural language understanding. Doctoral dissertation, University of Illinois at Urbana-Champaign. Jain, A. et al. (2020, August). Overview and importance of data quality for machine learning tasks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 3561-3562). Li, Q. et al. (2020). A survey on text classification: From shallow to deep learning. arXiv preprint arXiv:2008.00364. Li


Chunk 854:
Document ID: 51b77b6e-d896-4de2-8b81-aeabdb9fd40f
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Data Evaluation and Improvement for Machine Learning-1.pptx', 'folder_name': 'Module 5', 'num_tokens': 40, 'num_chars': 286}
Text: deep learning. arXiv preprint arXiv:2008.00364. Limsopatham, N., & Collier, N. (2016). Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation. In Proceedings of the 54th ACL Meeting (Volume 1: Long Papers) (pp. 1014-1023).
Thank you! Question?
9/15/24
58


Chunk 855:
Document ID: a82aef0e-22ed-4c05-9460-8d8802653eb3
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 4, 'num_chars': 30}
Text: 


#pip install evidently


# 


Chunk 856:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 54, 'num_chars': 512}
Text: 


import pandas as pd
import numpy as np
import evidently

from sklearn import datasets
from sklearn import ensemble
from sklearn import model_selection

from evidently import ColumnMapping
from evidently.options import ColorOptions
from evidently.report import Report

from evidently.metrics import ColumnDriftMetric
from evidently.metrics import DataDriftTable
from evidently.metrics import DatasetDriftMetric
from evidently.metrics import ColumnCategoryMetric
from evidently.metrics import ColumnDistribution


Chunk 857:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 41, 'num_chars': 512}
Text: c
from evidently.metrics import ColumnDistributionMetric
from evidently.metrics import ColumnValuePlot
from evidently.metrics import ColumnQuantileMetric
from evidently.metrics import ColumnCorrelationsMetric
from evidently.metrics import ColumnValueListMetric
from evidently.metrics import ColumnValueRangeMetric
from evidently.metrics import DatasetCorrelationsMetric
from evidently.metrics import ColumnRegExpMetric
from evidently.metrics import ColumnSummaryMetric
from evidently.metrics import ColumnMissing


Chunk 858:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 38, 'num_chars': 512}
Text: Metric
from evidently.metrics import ColumnMissingValuesMetric
from evidently.metrics import DatasetSummaryMetric
from evidently.metrics import DatasetMissingValuesMetric
from evidently.metrics import ConflictTargetMetric
from evidently.metrics import ConflictPredictionMetric
from evidently.metrics import ClassificationQualityMetric
from evidently.metrics import ClassificationClassBalance
from evidently.metrics import ClassificationConfusionMatrix
from evidently.metrics import ClassificationQualityByClass
f


Chunk 859:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 37, 'num_chars': 512}
Text: ntly.metrics import ClassificationQualityByClass
from evidently.metrics import ClassificationClassSeparationPlot
from evidently.metrics import ClassificationProbDistribution
from evidently.metrics import ClassificationRocCurve
from evidently.metrics import ClassificationPRCurve
from evidently.metrics import ClassificationPRTable
from evidently.metrics import ClassificationLiftCurve
from evidently.metrics import ClassificationLiftTable
from evidently.metrics import ClassificationQualityByFeatureTable
from ev


Chunk 860:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 36, 'num_chars': 512}
Text: import ClassificationQualityByFeatureTable
from evidently.metrics import ClassificationDummyMetric
from evidently.metrics import RegressionQualityMetric
from evidently.metrics import RegressionPredictedVsActualScatter
from evidently.metrics import RegressionPredictedVsActualPlot
from evidently.metrics import RegressionErrorPlot
from evidently.metrics import RegressionAbsPercentageErrorPlot
from evidently.metrics import RegressionErrorDistribution
from evidently.metrics import RegressionErrorNormality
from e


Chunk 861:
Document ID: 5353df13-1b5d-4ffb-9920-63a7c9ad7229
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 30, 'num_chars': 306}
Text: tly.metrics import RegressionErrorNormality
from evidently.metrics import RegressionTopErrorMetric
from evidently.metrics import RegressionErrorBiasTable
from evidently.metrics import RegressionDummyMetric
#from evidently.metrics import EmbeddingsDriftMetric


# # 1 Data Drift Report

# ## 1.1 Dataset

# 


Chunk 862:
Document ID: 1f284879-e203-478a-b3a7-bedf0181bc5e
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 14, 'num_chars': 137}
Text: 


# OpenML dataset: 15 features
adult_data = datasets.fetch_openml(name='adult', version=2, as_frame=True)
adult = adult_data.frame


# 


Chunk 863:
Document ID: 0bb2e61f-31ac-473c-a268-5fd76db166e5
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 51, 'num_chars': 414}
Text: 


# Create current and reference dataset

## reference dataset: not these education types
adult_ref = adult[~adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]
## current dataset: in these education types
adult_cur = adult[adult.education.isin(['Some-college', 'HS-grad', 'Bachelors'])]

# Set the values of the first 2,000 rows and 3rd and 4th columns to NaN
adult_cur.iloc[:2000, 3:5] = np.nan


# 


Chunk 864:
Document ID: 327039a1-f8f3-4907-94e1-dac429168ece
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 2, 'num_chars': 24}
Text: 


adult_ref.head()


# 


Chunk 865:
Document ID: d1c5d272-45d1-4052-a3b1-bee43664aefe
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 8, 'num_chars': 58}
Text: 


adult_cur.head(2)


# ## 1.2 Data integrity metrics

# 


Chunk 866:
Document ID: 222173d6-207a-4dcb-ac58-c2563bf02200
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 12, 'num_chars': 259}
Text: 


#dataset-level metrics
data_integrity_dataset_report = Report(metrics=[
    DatasetSummaryMetric(),
    DatasetMissingValuesMetric()

])

data_integrity_dataset_report.run(reference_data=adult_ref, current_data=adult_cur)
data_integrity_dataset_report


# 


Chunk 867:
Document ID: 579b31e0-18ed-4eb0-a8b3-42f1faefaa38
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 23, 'num_chars': 324}
Text: 


# column-level metrics
# using regular expression

data_integrity_column_report = Report(metrics=[
    ColumnRegExpMetric(column_name="education", reg_exp=r".*college.*"),
])

data_integrity_column_report.run(reference_data=adult_ref, current_data=adult_cur)
data_integrity_column_report


# ## 1.3 Data drift metrics

# 


Chunk 868:
Document ID: 0c5fb2f6-3daf-4661-8381-424830f528c6
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 16, 'num_chars': 265}
Text: 


# Create data drift dataset report

data_drift_dataset_report = Report(metrics=[
    DataDriftTable(num_stattest='kl_div', cat_stattest='psi'),    
])

data_drift_dataset_report.run(reference_data=adult_ref, current_data=adult_cur)
data_drift_dataset_report


# 


Chunk 869:
Document ID: beb64ef3-7600-46a6-8962-9501ac7575d5
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 18, 'num_chars': 300}
Text: 


#column-level metrics
data_drift_column_report = Report(metrics=[
    ColumnDriftMetric('education-num'),
    ColumnValuePlot('education-num'),  
])

data_drift_column_report.run(reference_data=adult_ref, current_data=adult_cur)
data_drift_column_report


# ## 1.3 Correlation and distribution

# 


Chunk 870:
Document ID: e232eeeb-3b5d-4086-b30b-eaebe8970384
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 11, 'num_chars': 225}
Text: 


#dataset-level metrics
data_quality_dataset_report = Report(metrics=[
    DatasetCorrelationsMetric(),

])

data_quality_dataset_report.run(reference_data=adult_ref, current_data=adult_cur)
data_quality_dataset_report


# 


Chunk 871:
Document ID: b4d1231d-fa9e-4213-bed5-1cc0d54f90fa
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 18, 'num_chars': 512}
Text: 


#column-level metrics
data_quality_column_report = Report(metrics=[
    ColumnDistributionMetric(column_name="education"), 
    ColumnQuantileMetric(column_name="education-num", quantile=0.75), 
    ColumnValueRangeMetric(column_name="education-num", left=10, right=20),
    ColumnCorrelationsMetric(column_name="education"), 
    ColumnCategoryMetric(column_name='education', category='Some-college')
])

data_quality_column_report.run(reference_data=adult_ref, current_data=adult_cur)
data_quality_column_re


Chunk 872:
Document ID: b4d1231d-fa9e-4213-bed5-1cc0d54f90fa
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 13, 'num_chars': 107}
Text: ef, current_data=adult_cur)
data_quality_column_report


# # 2 Classification Metrics

# ## 2.1 Dataset

# 


Chunk 873:
Document ID: b235bc47-89ed-46b9-bc07-c104549158f1
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 15, 'num_chars': 168}
Text: 


#Dataset for binary label and probabilistic classifcation
bcancer_data = datasets.load_breast_cancer(as_frame=True)
bcancer = bcancer_data.frame

bcancer.head()


# 


Chunk 874:
Document ID: 7e610d71-ada8-45d7-9117-f716e58d06bd
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 24, 'num_chars': 263}
Text: 


# Create reference and current datasets

## Sample data 
bcancer_ref = bcancer.sample(n=300, replace=False)
bcancer_cur = bcancer.sample(n=200, replace=False)

bcancer_label_ref = bcancer_ref.copy(deep=True)
bcancer_label_cur = bcancer_cur.copy(deep=True)


# 


Chunk 875:
Document ID: 0778075a-124a-4197-91e0-02eac983ad7c
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 37, 'num_chars': 512}
Text: 


## Set feature columns and target column
model = ensemble.RandomForestClassifier(random_state=1, n_estimators=10)
model.fit(bcancer_ref[bcancer_data.feature_names.tolist()], bcancer_ref.target)

## Predict the probability of the positive class for each sample
bcancer_ref['prediction'] = model.predict_proba(bcancer_ref[bcancer_data.feature_names.tolist()])[:, 1]
bcancer_cur['prediction'] = model.predict_proba(bcancer_cur[bcancer_data.feature_names.tolist()])[:, 1]
## Predicts class labels
bcancer_label_re


Chunk 876:
Document ID: 0778075a-124a-4197-91e0-02eac983ad7c
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 13, 'num_chars': 246}
Text: ])[:, 1]
## Predicts class labels
bcancer_label_ref['prediction'] = model.predict(bcancer_label_ref[bcancer_data.feature_names.tolist()])
bcancer_label_cur['prediction'] = model.predict(bcancer_label_cur[bcancer_data.feature_names.tolist()])


# 


Chunk 877:
Document ID: a9334319-f8b8-43d0-afe1-04055966f3f0
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 2, 'num_chars': 32}
Text: 


bcancer_label_cur.head()


# 


Chunk 878:
Document ID: 31e66902-389e-4151-81b5-4fa5bbbdf9b9
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 19, 'num_chars': 512}
Text: 


#probabilistic binary classification
classification_report = Report(metrics=[
    ClassificationQualityMetric(),
    ClassificationClassBalance(),
    ConflictTargetMetric(),
    ConflictPredictionMetric(),
    ClassificationConfusionMatrix(),
    ClassificationQualityByClass(),
    ClassificationClassSeparationPlot(),
    ClassificationProbDistribution(),
    ClassificationRocCurve(),
    ClassificationPRCurve(),
    ClassificationPRTable(),
    ClassificationLiftCurve(),
    ClassificationLiftTable(),



Chunk 879:
Document ID: 31e66902-389e-4151-81b5-4fa5bbbdf9b9
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 12, 'num_chars': 251}
Text: cationLiftCurve(),
    ClassificationLiftTable(),
    ClassificationQualityByFeatureTable(columns=['mean area', 'fractal dimension error']),
])

classification_report.run(reference_data=bcancer_ref, current_data=bcancer_cur)
classification_report


# 


Chunk 880:
Document ID: 1126d67c-25c6-4464-b2eb-5917ecc8b156
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 22, 'num_chars': 512}
Text: 


#label binary classification
classification_report = Report(metrics=[
    ClassificationQualityMetric(),
    ClassificationClassBalance(),
    ConflictTargetMetric(),
    ConflictPredictionMetric(),
    ClassificationConfusionMatrix(),
    ClassificationQualityByClass(),
    ClassificationDummyMetric(),
    ClassificationQualityByFeatureTable(columns=['mean area', 'fractal dimension error']),
])

classification_report.run(reference_data=bcancer_label_ref, current_data=bcancer_label_cur)
classification_re


Chunk 881:
Document ID: 1126d67c-25c6-4464-b2eb-5917ecc8b156
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 7, 'num_chars': 85}
Text:  current_data=bcancer_label_cur)
classification_report


# ### Regression Metrics

# 


Chunk 882:
Document ID: 1f913343-a485-4476-9847-32aa3835d133
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 28, 'num_chars': 391}
Text: 


#Dataset for regression
housing_data = datasets.fetch_california_housing(as_frame=True)
housing = housing_data.frame

housing.rename(columns={'MedHouseVal': 'target'}, inplace=True)
housing['prediction'] = housing_data['target'].values + np.random.normal(0, 3, housing.shape[0])

housing_ref = housing.sample(n=5000, replace=False)
housing_cur = housing.sample(n=5000, replace=False)


# 


Chunk 883:
Document ID: 2b3c0c7c-b1ab-447e-9499-3adee3080093
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 18, 'num_chars': 512}
Text: 


regression_report = Report(metrics=[
    RegressionQualityMetric(),
    RegressionPredictedVsActualScatter(),
    RegressionPredictedVsActualPlot(),
    RegressionErrorPlot(),
    RegressionAbsPercentageErrorPlot(),
    RegressionErrorDistribution(),
    RegressionErrorNormality(),
    RegressionTopErrorMetric(),
    RegressionErrorBiasTable(columns=['MedInc', 'AveRooms']),
    RegressionDummyMetric(),
    ConflictTargetMetric(),
    ConflictPredictionMetric(),

])

regression_report.run(reference_data=h


Chunk 884:
Document ID: 2b3c0c7c-b1ab-447e-9499-3adee3080093
Metadata: {'file_name': 'Demo01_Evidently_Metrics.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo01_Evidently_Metrics.ipynb', 'folder_name': 'Module 5', 'num_tokens': 8, 'num_chars': 121}
Text: ric(),

])

regression_report.run(reference_data=housing_ref, current_data=housing_cur)
regression_report


# In[ ]:







Chunk 885:
Document ID: f73158ca-d492-4175-a825-be9946ba9ff3
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 99, 'num_chars': 512}
Text: 


TextBlob(train['tweet'][0]).ngrams(3)


# # 3.2 Term frequency
# 
# Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.
# 
# Therefore, we can generalize term frequency as:
# 
# **TF = Term Frequency (TF) is to just count the number of occurrence.**
# 
# But it has been observed that if a word X occurs in document A 1 time and in B 10 times, its generally not true that the word X is 10 times more relevant in B than in A. The difference is genera


Chunk 886:
Document ID: f73158ca-d492-4175-a825-be9946ba9ff3
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 69, 'num_chars': 512}
Text:  relevant in B than in A. The difference is generally lesser as compared to the actual ratio. Hence it is good to apply following transformation on TF:
# 
# 
# 
# TF = 1 + log (TF)   if  TF > 0
# 
#      0              if TF = 0
# 
# To understand more about Term Frequency, have a look at [this article](https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/). More about [term frequency](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/).
# 
# Below, 


Chunk 887:
Document ID: f73158ca-d492-4175-a825-be9946ba9ff3
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 18, 'num_chars': 115}
Text: 06/word-embeddings-count-word2veec/).
# 
# Below, I have tried to show you the term frequency table of a tweet.

# 


Chunk 888:
Document ID: 78bc07d4-b900-4af7-8b5a-15b6b4a017b5
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 88, 'num_chars': 512}
Text: 


tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(" "))).sum(axis = 0).reset_index()
tf1.columns = ['words','tf']
tf1


# # 3.3 Inverse Document Frequency
# The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.
# 
# Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.
# 
# IDF = log(N/n), where, N is the total number of rows 


Chunk 889:
Document ID: 78bc07d4-b900-4af7-8b5a-15b6b4a017b5
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 55, 'num_chars': 270}
Text:  = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.
# 
# So, let’s calculate IDF for the same tweets for which we calculated the term frequency.
# 
# **The more the value of IDF, the more unique is the word.**

# 


Chunk 890:
Document ID: 1ef9c7d6-d641-47fb-ba45-482f57dbeb46
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 39, 'num_chars': 305}
Text: 


import numpy as np

for i,word in enumerate(tf1['words']):
  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))

tf1


# # 3.4 Term Frequency – Inverse Document Frequency (TF-IDF)
# 
# TF-IDF is the multiplication of the TF and IDF which we calculated above.

# 


Chunk 891:
Document ID: 0f94de28-4e74-4e48-82d8-b9394a2212ae
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 80, 'num_chars': 471}
Text: 


tf1['tfidf'] = tf1['tf'] * tf1['idf']
tf1


# We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.
# 
# We don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:

# 


Chunk 892:
Document ID: 0f94de28-4e74-4e48-82d8-b9394a2212ae
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 3, 'num_chars': 9}
Text: n it:

# 


Chunk 893:
Document ID: 79d0ebff-8120-4a9c-9bbb-67ec6662f32f
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 60, 'num_chars': 512}
Text: 


from sklearn.feature_extraction.text import TfidfVectorizer
myvocabulary = ['life', 'learning', 'game']
corpus = {1: "The game of life is a game of everlasting learning", 2: "The unexamined life is not worth living", 3: "Never stop learning"}
tfidf = TfidfVectorizer(vocabulary = myvocabulary, max_features=1000, lowercase=True, analyzer='word',
 stop_words= 'english', ngram_range = (1,3))
tfs = tfidf.fit_transform(corpus.values())
feature_names = tfidf.get_feature_names_out()
corpus_index = [n for n in co


Chunk 894:
Document ID: 79d0ebff-8120-4a9c-9bbb-67ec6662f32f
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 82, 'num_chars': 512}
Text: _feature_names_out()
corpus_index = [n for n in corpus]
import pandas as pd
df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)
print(df)


# # 3.5 Bag of Words
# Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about th


Chunk 895:
Document ID: 79d0ebff-8120-4a9c-9bbb-67ec6662f32f
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 44, 'num_chars': 322}
Text: rom the text alone we can learn something about the meaning of the document.
# 
# To gain a better understanding of this, you can refer to [this article](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/).
# 
# For implementation, sklearn provides a separate function for it as shown below:

# 


Chunk 896:
Document ID: 0ce6232a-cf1c-460b-abbb-587c3948d679
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 64, 'num_chars': 512}
Text: 


from sklearn.feature_extraction.text import CountVectorizer
bow = CountVectorizer(max_features=50, lowercase=True, ngram_range=(1,3),analyzer = "word")
train_bow = bow.fit_transform(train['tweet'])
bow.get_feature_names_out()


# # 3.6 Sentiment Analysis
# 
# If you recall, our problem was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library), let’s check the sentiment of the first few tweets.
# 




Chunk 897:
Document ID: 0ce6232a-cf1c-460b-abbb-587c3948d679
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 10, 'num_chars': 52}
Text:  check the sentiment of the first few tweets.
# 

# 


Chunk 898:
Document ID: 806cbada-3227-4e66-b1bd-b6c79f1e5c19
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 62, 'num_chars': 397}
Text: 


train['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)


# Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model.

# 


Chunk 899:
Document ID: 888d9854-f722-44af-a7c0-24ed8fc7f6ed
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 81, 'num_chars': 512}
Text: 


train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )
train[['tweet','sentiment']].head()


# # 3.7 Word Embeddings
# 

# Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors.
# 
# Word2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki, etc.
# 
# Here, we will use pre-t


Chunk 900:
Document ID: 888d9854-f722-44af-a7c0-24ed8fc7f6ed
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 67, 'num_chars': 512}
Text: by Google, Wiki, etc.
# 
# Here, we will use pre-trained word vectors which can be downloaded from the [glove website](https://nlp.stanford.edu/projects/glove/). There are different dimensions (50,100, 200, 300) vectors trained on wiki data. For this example, I have downloaded the 100-dimensional version of the model.
# 
# You can refer an article [here](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/) to understand different form of word embeddings.
# 
# The first step here is


Chunk 901:
Document ID: 888d9854-f722-44af-a7c0-24ed8fc7f6ed
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 64, 'num_chars': 512}
Text: rm of word embeddings.
# 
# The first step here is to convert it into the word2vec format.

# In[ ]:


from gensim.scripts.glove2word2vec import glove2word2vec
glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)


# Now, we can load the above word2vec file as a model.

# In[ ]:


from gensim.models import KeyedVectors # load the Stanford GloVe model
filename = 'glove.6B.100d.txt.word2vec'
model = KeyedVectors.load_


Chunk 902:
Document ID: 888d9854-f722-44af-a7c0-24ed8fc7f6ed
Metadata: {'file_name': 'Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 5/Demo02_Data_Preprocessing_and_Cleaning (3) (1).ipynb', 'folder_name': 'Module 5', 'num_tokens': 32, 'num_chars': 265}
Text: e.6B.100d.txt.word2vec'
model = KeyedVectors.load_word2vec_format(filename, binary=False)
model['go']
model['away']
(model['go'] + model['away'])/2


# We have converted the entire string into a vector which can now be used as a feature in any modelling technique.



Chunk 903:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 105, 'num_chars': 512}
Text: CSEP 517 Natural Language Processing Word Embeddings Luke Zettlemoyer (Slides adapted from Danqi Chen, Greg Durrett, Chris Manning, Dan Jurafsky)
How to represent words? N-gram language models it is 76 F and P(w ∣ ) It is 76 F and ___. [0.0001, 0.1, 0, 0, 0.002, ..., 0.3, ..., 0] red sunny Text classification ⊺ P(y = 1 ∣ x) = σ(θ w + b) I like this movie. 👍 [0, 1, 0, 0, 0, ..., 1, ..., 1] (1) w [0, 1, 0, 1, 0, ..., 1, ..., 1] I don’t like this movie. 👎 w(2) don’t
Representing words as discrete symbols In tr


Chunk 904:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 101, 'num_chars': 512}
Text: don’t
Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols: hotel, conference, motel — a localist representation one 1, the rest 0’s Words can be represented by one-hot vectors: hotel = [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] motel = [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] Vector dimension = number of words in vocabulary (e.g., 500,000) Challenge: How to compute similarity of two words?
Representing words by their context Distributional hypothesis: words that occur in similar c


Chunk 905:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 93, 'num_chars': 512}
Text: butional hypothesis: words that occur in similar contexts tend to have similar meanings J.R.Firth 1957 • “You shall know a word by the company it keeps” • One of the most successful ideas of modern statistical NLP! These context words will represent banking.
Distributional hypothesis C1: A bottle of ___ is on the table. “tejuino” C2: Everybody likes ___. C3: Don’t have ___ before you drive. C4: We make ___ out of corn.
Distributional hypothesis C1 C2 C3 C4 tejuino 1 1 1 1 C1: A bottle of ___ is on the table


Chunk 906:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 111, 'num_chars': 512}
Text: ejuino 1 1 1 1 C1: A bottle of ___ is on the table. loud 0 0 0 0 C2: Everybody likes ___. motor-oil 1 0 0 0 C3: Don’t have ___ before you drive. tortillas 0 1 0 1 choices 0 1 0 0 C4: We make ___ out of corn. wine 1 1 1 0 “words that occur in similar contexts tend to have similar meanings”
Words as vectors • We’ll build a new model of meaning focusing on similarity • Each word is a vector • Similar words are “nearby in space” • A first solution: we can just use context vectors to represent the meaning of wor


Chunk 907:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 29, 'num_chars': 512}
Text: se context vectors to represent the meaning of words! • word-word co-occurrence matrix:
Words as vectors u v cos(u, v) = · u v k kk k <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""LLLLwwwwUUUUwwwwXXXX9999BBBBKKKK88880000YYYY7777mmmmxxxx5555uuuullllvvvvUUUUssssqqqqXXXXbbbb00008888AAAA0000===="""">>>>AAAAAAAAAAAACCCCSSSSHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqEEEEssss3333gggg0000WWWWooooIIIICCCCUUUURRRRQQQQTTTTddddCCCC0000YYYY3333L


Chunk 908:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: WooooIIIICCCCUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOssssmmmmEEEEmmmmUUUUmmmmhhhhppppPPPPkkkk8888NNNNyyyy7777dddd++++QQQQ1111uuuuXXXXCCCCjjjjiiiizzzzkkkkllllbbbbqqqqbbbbYYYYeeeeGGGGDDDDjjjj3333nnnnHHHHuuuu5555dddd44444444ffffMMMMyyyyqqqqVVVVZZZZbbbb0000YYYYhhhhZZZZXXXXVVVVttttffffWWWWNNNN4444mmmmZZZZppppaaaa3333ttttnnnndddd8888////ccccPPPP2222hhhhKKKKnnnngggghhhhMMMMGGGGppppggggzzzzLLLLttttoooo++++kkkkooooTTTTRRRRiiiiDDDDQQQQUUUUVVVVYYYYyyyy0000YYYY0000FFF


Chunk 909:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: TTTRRRRiiiiDDDDQQQQUUUUVVVVYYYYyyyy0000YYYY0000FFFFQQQQ6666DDDDPPPPSSSS8888ggggeeee3333uuuudddd8888aaaaEEEEiiiiEEEEppppjjjjxxxx7777UUUUKKKKCCCCZZZZuuuuiiiiHHHHooooRRRRDDDDSSSShhhhGGGGSSSSkkkkuuuueeee6666WWWWEEEEuuuuKKKK00006666IIIIVVVVNNNN8888PPPP0000iiiiQQQQ7777ggggzzzz99998888mmmmJJJJ3333CCCCaaaa++++ggggEEEEAAAAuuuuFFFF00007777kkkkMMMMHHHHdddd7777nnnn66661111ZZZZSSSSllllzzzznnnnhhhhuuuuOOOO2222MMMM4444LLLL4444eeee6666zzzzDDDDyyyyzzzzbbbbFFFFWWWWttttCCCCeeeeAAAAyyyyssssWWWWeeeekkkkDDDDGGGGaaaaooooeeee++++a


Chunk 910:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: eAAAAyyyyssssWWWWeeeekkkkDDDDGGGGaaaaooooeeee++++aaaazzzz0000++++UUUU4444CCCCUUUUmmmmkkkkMMMMEEEENNNNSSSSddddmmmmwwwwrrrrVVVVmmmm6666KKKKhhhhKKKKKKKKYYYYkkkkaaaazzzzkkkkJJJJJJJJLLLLEEEECCCCAAAA9999QQQQjjjj3333QQQQ0000jjjjVVVVBBBBIIIIppppJJJJttttOOOOggggssssjjjjggggiiiiVVVVaaaa6666MMMMOOOOBBBBCCCCvvvv0000jjjjBBBBiiiiffffpppp7777IIIIkkkkWWWWhhhhllllKKKKPPPPQQQQ1111555533335555kkkkXXXXLLLLRRRRyyyy8888XXXX////vvvvEEEE6666iiiiggggiiiissss3333ppppVVVVGGGGccccKKKKBBBBLLLLhhhh6666aaaaIIIIggggYYYYVVVVBBBBxxxxmmmmKKK


Chunk 911:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: BBBLLLLhhhh6666aaaaIIIIggggYYYYVVVVBBBBxxxxmmmmKKKKccccKKKKuuuu1111QQQQQQQQrrrrNNNNhhhhIIIIEEEE4444QQQQFFFF1111bbbbddddCCCC3333EEEEcccc6666OOOO6666WWWWzzzzLLLL++++kkkkQQQQ7777MMMMUUUUvvvvLLLL5555PPPPmmmmeeeeddddWWWW2222qqqqvvvvbbbb9999RRRRbbbbllll2222MMMM4444uuuujjjjCCCCIIII7777AAAAMMMMaaaaggggAAAAGGGG1111yyyyCCCCGGGGrrrrggggDDDDddddddddAAAAAAAAGGGGDDDDyyyyCCCCVVVV////AAAAOOOOPPPPoooowwwwnnnn4444888833334444NNNNLLLL6666mmmmrrrrQQQQVVVVjjjjNNNNnnnnMMMMIIII////qqqqBBBBQQQQ++++AAAAbbbbQQQQvvvvrrrrTTTTUUUU<<<</


Chunk 912:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 22, 'num_chars': 512}
Text: /qqqqBBBBQQQQ++++AAAAbbbbQQQQvvvvrrrrTTTTUUUU<<<<////llllaaaatttteeeexxxxiiiitttt>>>> V u v i i i=1 cos(u, v) = V V 2 2 Pu v i=1 i i=1 i <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""EEEExxxx11114444HHHHCCCCNNNNmmmmllllwwwweeeeaaaattttqqqqmmmm////BBBBBBBBddddVVVVuuuuIIIIIIIIyyyyGGGGZZZZIIII===="""">>>>AAAAAAAAAAAACCCCXXXX3333iiiiccccbbbbZZZZFFFFLLLLSSSSwwwwMMMMxxxxFFFFIIIIUUUUzzzz44447777ttttWWWWHHHHXXXXUUUUllllbbbbooooJJJJFFFFqqqqCCCCBBBBllllppppggggiiii6666EEEEUUUUQQ


Chunk 913:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: JJJJFFFFqqqqCCCCBBBBllllppppggggiiii6666EEEEUUUUQQQQ3333LLLLiiiivvvvYYYYKKKKnnnnTTTTaaaaIIIIZZZZNNNNmmmmaaaajjjjDDDDzzzzMMMMIIII++++BBBBEEEEvvvvIIIInnnn3333QQQQlllluuuu////CCCCddddmmmm2222ggggrrrraaaaeeeeiiiiFFFFwwwwOOOONNNN++++9999JJJJPPPPcccckkkkLLLLhhhhggggVVVV0000vvvvcccc////HHHHHHHHddddllllddddWWWW11119999YYYY3333OOOOrrrrttttllll3333ffff2222dddd3333zzzz9999gggg99996666IIIIllllcccccccckkkkyyyy7777OOOOWWWWcccc6666ffffYYYYyyyyQQQQIIIIooooxxxxnnnnppppSSSSiiiiooooZZZZeeeeSSSS44444444QQQQWWWWnnnnMMMMyyyyFFFF


Chunk 914:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: iiooooZZZZeeeeSSSS44444444QQQQWWWWnnnnMMMMyyyyFFFFPPPP8888eeeellllffffxxxxpppp5555JJJJwwwwQQQQffffPPPPssssUUUUUUUU4444KKKKMMMMkkkkjjjjRRRROOOOKKKKMMMMJJJJxxxxUUUUhhhhaaaaKKKK////JJJJKKKKnnnnIIIIttttmmmmmmmmCCCCLLLL5555EEEEiiiiddddaaaammmmXXXXPPPP4444oooo0000ttttzzzzBBBBqqqq9999hhhhmmmmHHHHCCCCEEEEddddSSSShhhhUUUUGGGGmmmmllll6666HHHHZZZZiiiihhhh7777hhhhmmmmttttIIIIggggrrrrLLLLiiiiBBBBppppjjjjwwwwRRRRuuuuXXXXSSSS3333jjjjYYYYrrrruuuuCCCC////rrrrJJJJwwwwxxxxEEEE3333kkkkNNNNvvvv++++VVVVPPPPCCCCyyyy6666LLLLYYYYCC


Chunk 915:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 3333kkkkNNNNvvvv++++VVVVPPPPCCCCyyyy6666LLLLYYYYCCCC4444aaaaYYYYFFFF6666ddddyyyyHHHHssssPPPPRRRRzzzzllllWWWWKKKKcccckkkkkkkkZZZZkkkkiiiiIIIIffffuuuuAAAAXXXXccccqqqqAAAARRRRllllxxxxQQQQzzzzYYYYmmmmqqqqhhhhEEEEqqqqRRRRAAAA++++BBBBWWWWNNNNSSSSdddd////KKKKDDDDKKKKVVVVEEEEDDDDPPPPQQQQ0000HHHHwwwwNNNNPPPPrrrrTTTTOOOOCCCCSSSScccc7777ttttyyyySSSSSSSSccccuuuurrrr8888nnnnNNNNEEEEqqqqFFFFmmmmKKKKSSSSxxxx7777aaaayyyy2222FFFF4444uuuussssMMMMvvvv9999jjjjffffSSSSWWWWTTTTqqqq4444GGGGmmmmWWWWaaaaEEEEkkkkyyyyffffDDDDssssoooo


Chunk 916:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: qq4444GGGGmmmmWWWWaaaaEEEEkkkkyyyyffffDDDDssssooookkkkQQQQxxxxKKKKHHHHNNNNYYYYhhhhQQQQ1111HHHHllllBBBBMMMMssss2222ccccQQQQKKKKhhhhDDDDmmmm1111bbbb4444XXXX4444BBBBddddllllIIIIppppffff2222SSSSmmmmgggg0000hhhhWWWWFFFFxxxx5555WWWWffffTTTTaaaarrrrccccBBBBvvvvBBBBQQQQ8888XXXXjjjjZZZZvvvvbbbbeeeeRRRRyyyybbbb4444BBBBiiiiccccggggCCCCYYYYIIIIwwwwCCCCWWWW4444AAAAffffeeeeggggAAAA7777ooooAAAAgggg0000////HHHHddddbbbbaaaadddduuuuvvvvPPPPllllbbbbrrrriiii7777rrrrjjjjddddrrrrddddZZZZ33335555zzzzCCCCHHHH4444UUUU++++7777RRRRNN


Chunk 917:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 74, 'num_chars': 512}
Text: ddddZZZZ33335555zzzzCCCCHHHH4444UUUU++++7777RRRRNNNNyyyyRRRRRRRRuuuuQQQQcccc====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> q q P P What is the range of ? cos( ⋅ )
Words as vectors Problem: not all counts are equal, words can randomly co-occur • Solution: re-weight by how likely it is for the two words to co-occur by simple chance • PPMI = Positive Pointwise Mutual Information
Sparse vs dense vectors • Still, the vectors we get from word-word occurrence matrix are sparse (most are 0’s) & long (vocabulary size)


Chunk 918:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 79, 'num_chars': 512}
Text: are sparse (most are 0’s) & long (vocabulary size) • Alternative: we want to represent words as short (50-300 dimensional) & dense (real-valued) vectors • The focus of this lecture • The basis of all the modern NLP systems
Dense vectors 0.286 0.792 0 1 0.177   B 0.107C B  C employees = B 10.109 C B C B 0.542C B  C B 0.349 C B C B 0.271 C B C B 0.487 C B C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""rrrrllllyyyyVVVV9999zzzz4444BBBBFFFFddddXXXXNNNN5555AAAATTTTSSSSBBBBw


Chunk 919:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 9zzzz4444BBBBFFFFddddXXXXNNNN5555AAAATTTTSSSSBBBBwwwwwwwwvvvveeee44448888tttt4444VVVVssss===="""">>>>AAAAAAAAAAAACCCCaaaa3333iiiiccccbbbbZZZZHHHHPPPPTTTT9999sssswwwwFFFFMMMMeeeeddddjjjjAAAA3333ooooNNNNiiiijjjjjjjjAAAAGGGGIIII7777WWWWKKKKuuuuQQQQddddllllmmmmVVVVhhhhEEEELLLLaaaaAAAAxxxxKKKKCCCCCCCC0000ccccmmmmrrrrYYYYDDDDUUUUVVVVJJJJXXXXjjjjvvvvhhhhYYYYLLLLxxxx4444nnnnssssFFFF0000QQQQVVVV9999bbbbIIII////ccccTTTTffff++++AAAAyyyy77778888DDDDzzzzhhhhNNNNQQQQAAAAPPPP2222JJJJEEEEssssffffffffdddd8888PPPPPPPP3333888


Chunk 920:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: PPP2222JJJJEEEEssssffffffffdddd8888PPPPPPPP33338888ddddZZZZ1111IIIIYYYY9999LLLLwwww7777xxxx3333222233339999PPPP7777DDDD8888ssssppppqqqq4444++++OOOOnnnnzzzz2222vvvvrrrrzzzzYYYY0000vvvv5555yyyybbbbNNNNNNNNYYYYcccc++++TTTT2222WWWWqqqqLLLL2222NNNNmmmmQQQQAAAAooooFFFFffffRRRRQQQQoooo4444TTTTLLLLTTTTwwwwJJJJJJJJYYYYwwwwkkkkVVVV8888ffffVVVVLLLLmmmmLLLL22225555AAAAGGGG5555GGGGqqqq3333zzzzjjjjLLLLYYYYJJJJiiiiwwwwqqqqRRRRIIIITTTTwwwwRRRRllllaaaaaaaaddddTTTT8888EEEEyyyyHHHHccccYYYYggggFFFFJJJJJJJJttttMMMMZZZZggggJJJJn


Chunk 921:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: yHHHHccccYYYYggggFFFFJJJJJJJJttttMMMMZZZZggggJJJJnnnnTTTTQQQQxxxxrrrrFFFFMMMMBBBBWWWWqqqqyyyyBBBBKKKKGGGGWWWWttttzzzzOOOOqqqqddddccccOOOOuuuuggggcccc0000iiiiiiiiyyyyEEEEvvvvaaaaCCCCEEEEnnnn11117777bbbbDDDD8888MMMMnnnn8888hhhhbbbbkkkkllll9999SSSSrrrrttttffff1111OOOOUUUUDDDDXXXXssssddddXXXXooooVVVVBBBBKKKKFFFFffffQQQQaaaaddddrrrryyyy0000GGGGNNNNnnnn++++eeeePPPPmmmmiiii2222vvvv7777SSSS2222CCCCvvvvggggWWWW////hhhhhhhhaaaapppp44442222zzzzUUUU////BBBBuuuuNNNNUUUU55554444nnnnooooJJJJBBBBLLLLZZZZsssszzzzAAAA9999zzz


Chunk 922:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 5554444nnnnooooJJJJBBBBLLLLZZZZsssszzzzAAAA9999zzzzIIIIccccFFFFkkkkyyyyjjjj4444BBBBLLLLmmmmjjjjSSSSgggg3333kkkkDDDDFFFF++++zzzzaaaaYYYYwwwwssssKKKKhhhhYYYYAAAAmmmmZZZZYYYYLLLLLLLLyyyyaaaa000011112222rrrrjjjjOOOOkkkkkkkk1111ffffYYYYooooppppAAAAvvvv1111333344446666CCCCJJJJccccbbbbMMMMkkkktttthhhhWWWW2222vvvv2222uuuuzzzzOOOOttttccccKKKKffff4444vvvvNNNN8888hhhhxxxx0000hhhh0000WWWWQQQQmmmmUUUU5555gggguuuuLLLLVVVVRRRRZZZZNNNNccccUUUUkkkkxxxxppppaaaaTTTTwwwwddddCCCCwwww0000cccc5555ccccwwwwCCCC44441111rrrrYYYYXXXXS


Chunk 923:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 8, 'num_chars': 512}
Text: Cwwww0000cccc5555ccccwwwwCCCC44441111rrrrYYYYXXXXSSSSmmmm////YYYYppppppppxxxxttttNNNN////TTTTssssCCCCbbbb4444rrrr5555////8888FFFFssss6666DDDDddddmmmmnnnnzzzzrrrr00007777rrrr6666LLLLiiii2222YYYY4444VVVV8888JJJJdddd////JJJJDDDD++++KKKKTTTTkkkkBBBByyyyRRRRUUUU3333JJJJGGGG++++ooooSSSSTTTTeeee2222ffffNNNN2222XXXXKKKK2222nnnnQQQQdddd3333000099991111xxxxvvvv1111WWWWllllrrrrllllPPPP3333bbbbJJJJIIIIXXXX4444eeee4444++++AAAAnnnniiii5555rrrr66664444====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> @ A
Why dense vectors? • S


Chunk 924:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 81, 'num_chars': 512}
Text: ttteeeexxxxiiiitttt>>>> @ A
Why dense vectors? • Short vectors are easier to use as features in ML systems • Dense vectors may generalize better than storing explicit counts • They do better at capturing synonymy • co-occurs with “car”, co-occurs with “automobile” w w 1 2 • Different methods for getting dense vectors: • Singular value decomposition (SVD) • word2vec and friends: “learn” the vectors!
Word2vec and friends (Mikolov et al, 2013): Distributed Representations of Words and Phrases and their Composi


Chunk 925:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 64, 'num_chars': 512}
Text: esentations of Words and Phrases and their Compositionality
Word2vec • Input: a large text corpora, V, d 0.124 0.224 • V: a pre-defined vocabulary     0.430 0.130 • d: dimension of word vectors (e.g. 300) v cat = 0 1 v dog = 0 0.2001 0.290     • B 0.329 C Text corpora: B 0.276 C B C B C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""7uuQAVVW8vvAMmmMqVVE6kkH3OOLLNNqMvvCToooM77EcZZr+ggtlSSE3wwmUrraeHH6NllWczziPjj/16670BB7h++71jjuaPPL/NNqYQQM===="""">>>>AAAAAAAAAAAACCCCOOO


Chunk 926:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ++71jjuaPPL/NNqYQQM===="""">>>>AAAAAAAAAAAACCCCOOOOXXXXiiiiccccbbbbVVVVBDDDNPPLSaaSxxxixNNNBBBBFGGFHJJKy111jtt2JbbfjaaMG11TTrr2Dfflz22Rdzzm6rrXz00bKccgVttqRggDhKK4FHHMyhhZyppQ9mmHKEEQy00UCCCVBttHiddAIBBiICC4uQQcXRRRgAAnRvvBNQQqskkJRSSAVTTOYFFoWrrbdJJrZLL6emmJnJJhr33Ze99WrkkVoggz0yyd9ZZVPnntUVV8P11X3mmQGvv93ggGE00/ZNNN5yyZm//v95555bbi8XXdVvvg9wwN4vvuCvvX3AACjlljQeeigPP1yCChJjj+eiiw8xx8wYYkfPPBS//8uggHyJJSuOOgSkk4qiinADDHUZZMN99PRMMddPPeUBB+r44J+77Mr33i1vvUKMMtC99Mq77v022bckkfcUUmfNN/4IIk7iixmYYOJ55zm++ed99/e


Chunk 927:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: tC99Mq77v022bckkfcUUmfNN/4IIk7iixmYYOJ55zm++ed99/ejjU/ccFX00n6HH0zWWlf995zssZbPPX8ddVNxxt377e755qDjjPx3338ff+X33dG992pggz8++QWCC39wwA166t1OOoe++iWzzVoUUakssmdjj5moojeCCLvddgcyyFHllJiZZTbvvWCrr0ohhUVFFaYttKGQQCOUUy6kk8RMMwIPPA4JJT2SSyFqqIh44FULLFWgg9SzzHJww1xLL8oFFcPFFikww/+lluMUUAfxxFYffjPLLZzvvahyyrBrrP6GGc1RRJRgghurrB9ccNm//+h00EUeeDY55LCwwf8XXtTEESQGGc6RRHM99RGrrSSOOrgZZ9rKKqyCC6Uoo6r55R/OOURGGhPQQwuXXilcc0X22WMrrceCCTEKKopEEoVGGyW669kzz9+SS+rffGOFFMuzzFtXXAMFF6E33i566JwUULqwwOEFFBxjjpVqq5ZqqWIss/Kggos44


Chunk 928:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: XXAMFF6E33i566JwUULqwwOEFFBxjjpVqq5ZqqWIss/Kggos447OGGrqnnB3ll4DTT0r++97yymweegZssYtGG+dbbqZYYyZ66+HNNtIII8eppt888Gu11xbuuE7yykG00Nx22x8YYiRIIbL44SLqqbcccu6XXPnYYQ0SSDlmm0rqqHr33HWzzbxvv4iwwFjIIeSddtRPPc8oobD33q/MMbcAAAYwwzKaa6brrl/MMQImmRJWWT2ooUHOOiMssNSkkTPvvtZCCHCWWrVNNVbSSfb//2brrG7mmc633iwaajo99wAppB2//jvKKU3ooJG77xRDDaZDDzz55smFFBsaaysSS7z77BQKKbkDDcNDDITQQNCKKSuxxKWaaC700jEdd9ChhM+KKLpzzeVAAQwuucpOOXKIIHSGGNGppBuVV9sBBBwQQxL++Vh11P0FFMWppEQoobheeL6BBcLiiYIyyXXss1Ycc79wwSNccLSFFaJTTfDzzE1DDt6GGJsxx8m


Chunk 929:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: cLiiYIyyXXss1Ycc79wwSNccLSFFaJTTfDzzE1DDt6GGJsxx8mccalLL99SSzd++TsvvS166sSbbfsFFopTT+GUUU+jjfTrrDWKKEPjj20XXmNssEsaaSo66uvVVU6LLmb993qNN3E115THHXmxx93zz7CNNIhpp3L55E/ll7Krr7ThhxfJJO7ttj899vg//29EED933bcvvibUUFiWW1S44l1nni633N3eeopooMMMMfFTTmmRRoriin177yKiiumSSKKuuKAiiRmgg3NRRVnttSHFFG4hhN099pKNNQDCCKWooAjVVanxxOIZZs1wwKruuFZaakKqqWmSS5yppXKNNKECCqmBB6PQQ4Pzz4BRRQH33JShhdKww2rkkb8ii4r33rkKKI0xxffUUhCTT8Cbb8krrlmhhd+AAy7VV3N77qCbbgXvvHESSrDggt0jj19vvT+nnVTrrmnxxsZOOeb++TzqqeU11ujmmo366k+NNA5pp2P99y366S899bZoo


Chunk 930:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 3, 'num_chars': 512}
Text: xxsZOOeb++TzqqeU11ujmmo366k+NNA5pp2P99y366S899bZooZ344JmJJQGyyPIvvZessJPkkksKK5MffyykkQfGGFIXXmEllkIOOTNQQQmnnfAJJ6bKKQdLLOqss/AllJNrrAH00HZiirBUU2w99/BII3pssrdgg3wHH3A8857ooDfVV1B88PeIIRX99mA++ed9983jjaA99WX55a3XXd077f944IGffDZ3334cc8CzzgGWWqz66QB449Pdd6811khmmA+nn<Ppp/MDDlX//aG44tHPPeG33xq++i4AAtA55>=WW=BB<qq///l00a==t<<e//xlliaattt>eexxiitt>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ZXXyS77U1JJh1OOktbbD+iilSHHYAYYwTNNUcXXUIwwEQbboYssQaII+aSS3JLLM4OOeVmmiZkkaujj


Chunk 931:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: YYwTNNUcXXUIwwEQbboYssQaII+aSS3JLLM4OOeVmmiZkkaujjCEXXkjbbTXSSTjssYzJJ50ww/YssM===="""">>>>AAAAAAAAAAAACCCCOOOOXXXXiiiiccccbbbbZZZZDBBBPNNNSSSShyyyxNNNBBBBEEEEMIIIZZZZr777j//1IFFPkhhV611j5dd3sHHN8eeWajjsjHHH3vvrpXX0oQQ0IrrBoCCsFxxHc44Ls22hvddpRCC4sTToQFFxlTTgS99UIAAFIQQwHffYjDDvxiiHuMMCIYYEKJJarRRFwIITsRRA6NNgyCC99TTnP66UTccpWSSsrGG7o33O0ttk966ZPhhuUuumN44v3aaEjMMMbQQMgzzzMzzf8GG8w//uxwwK533/mXX81rrCzzzZy554FEE8t77b4wwCEJJLLXXexjjP4wwUUooP8442ZllJoWWMX88ES228M55e+OOuuIIFirrhFLLpEooe/vvnaNNqPLLujwwi488qVVVNR


Chunk 932:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 8M55e+OOuuIIFirrhFLLpEooe/vvnaNNqPLLujwwi488qVVVNRUU0VVVqdXXV9vvtSVVMWGGj5qqYVppvoEET4XXc4GG1vbbPwrrfhyyNmRRjX00dsbbm2HH5+xx+fbbwjxxVMOO/3TT8vUU+x//WA77tu00pvzzunPPf3ccJ3ff7/PP5kyydNuuQjzzm6ccmewwROccHB22QyyyEwYYooyylrAAKshhzSkkHMhhnzUUEnYYLdooSm44ijiipRbboDkkYjFFUUJJSyDDFmQQZC22yHUUnFqqBGOOnkAAg8oocyNNKiccT2DDiKjjLNSSLNMMgFFF/4hhLmdd+JLLtzJJksTTV111GPggCX99sDPPTUwwf7VVYRiiLOZZDZ66FWHHDa33ofssxxppHj992nCCj2KKZUeeljUU4E//KyLLjarrQqhh9iQQ1kccKIHH8IWW6/ppq6XXbj66hfqqwZffjpttXvPPm4EEDwQQtJ44SzwwF699Pjyyt0RR+poo


Chunk 933:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 66hfqqwZffjpttXvPPm4EEDwQQtJ44SzwwF699Pjyyt0RR+pooGKttMqjjF2yyAwww6jwwjjhhyB66NkUUOTuuRJddplpp5nzzXgNNfqHHhyII/6ssWq88KwLL1K//evww3/666JqqZm11hq226/ggLWooNdNNaxQQsX55MH99VIVVKmggU9776E++K3ss/XNNbpCCIsXXzaaaqbYYjfEERINN0pUU/aWWBq11N3vv1/117CQQGG99+MCCh0ddWg99q444qeFFzB22GfZZRmYYqOllJNVVfN22T/UUTlDDAE00x7qqVLwwTmnnJJeeR7zzssNN13LLua225944Cunn3qmmuv44Jm//yAHHGFJJLA11QuXXK9mmB899S4XX3jrrtwssheNN2ZOOwLIIFFrrDIIIs0YY5JNNNLAAyVrriwFFFrrrghWWsv00IxGGPnLLMHMMwqVVslWWpszzFKgg5S11dkKK8xooAiaaGqDD1MwwnCwwN488Yexx/5CCBk


Chunk 934:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: pszzFKgg5S11dkKK8xooAiaaGqDD1MwwnCwwN488Yexx/5CCBkyydCssvdUULiJJRh775F00Q1HHXvTTdjWWcUccKj11RRjjH9ss+cKK4r11lJ88x5ccTRHHyXllM7BBd7ff0MzzfmvvcASSTDooOTddYP33+rEEtnuuHyKKcEeea2RRRoDD6Yuu38iiTn//7SEEXpzzdEmmj6PPPNrrt0eeR433JTHH+3kkV+ee2nttt300nO++23xxNZ33/Pbbtazz5z77FVUUK8SSnq//GdqqYY//ItWWWazz4LLL4jCC/R776K33m5ssaXqqKJllYCTT00jjDIMMJjEEGpLL2xYYp8YYMNffGCddB8TTK0NNqoFFhYMMM3aa1WFFwMllYbjj6KLLXAQQasjjlSDD4tQQoJhhIjUUbbffL4WWtSeeC044FyMM7uNNb/LLsKttQ5SSgLsssGUU8wxxnQNNfp11zIyyWPggnOCC9/99VQttrh33gRIIQEQQj9SS1Pff


Chunk 935:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 29, 'num_chars': 512}
Text: xxnQNNfp11zIyyWPggnOCC9/99VQttrh33gRIIQEQQj9SS1PffmfTT1m//Y455OOqq9WDDSsmmR3rrzIVVzhwwZ7IIIrWW2n66s055k/990gLL0JYYSUJJk8EEA3NNYDNN5CkkInggEyXXeCyykLiiSxyyVDyypBTTEOggkmKKBzyyvDRRyLbbQrbbPSJJ6hLLRA66/xqq9JRR6+BBtwBB9yLL+Vkkgcgg9wtte0++c3SS/weejJPP17HHigiilKXXvb33MopprO33N733Ka66PeDDst00hMNN7cWWeD00Q+eeWz88aDttIP5568llkp55G+88<Pkk/sHHlPeeaf88t3yyeeuuxqUUibTTtQqq>=yy=77<<<///lllaaattteeexxxiiittt>>> @ A • @ A Wikipedia + Gigaword 5: 6B • Twitter: 27B 0.234 0.290 0.266 0.441 • Common Crawl: 840B v = 0 1 v = 0  


Chunk 936:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 37, 'num_chars': 512}
Text: 0 0.266 0.441 • Common Crawl: 840B v = 0 1 v = 0  1 the language 0.239 0.762 B 0.199C B 0.982 C • d B C Output: f : V R B  C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""bhhQ4ZZLxMM8ciiA+77DOuurkSS7pxxt2ddRpZZ93HHsf//rvuukcZZ1/AAW1hhp+++qazzsodd+w//Q+WWUH88hYRRsTRRdG00qjZZRAMM4===="""">>>>AAAAAAAAAAAACCCCPPPPXXXXiiiiccccbbbbZZZZBBBBNNNLaSSSxxxysxxwxBBxEEEEIIIIZZZXn77T0jj63PPryFF14jjq2NN6yLL7Zqq21aaOmoou255nMccQvmmTIIIHinnAbhhQQxx3S66NxFFuennltEEhCDD8U11A0AAECQQKKAAgRi


Chunk 937:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: innAbhhQQxx3S66NxFFuennltEEhCDD8U11A0AAECQQKKAAgRiihhEEu6XXX6QQCaYYoWmm4QrrKkww0tss8g66OJwwQe99zIPPt3TTSRWWMyrrwmssXN33SH226R99SKAAatzzrdddFpNNoNcceksskQuu/sww5+//swyybfxx/6XX46PPMXII6/ffdovvGbHHxfnneeLLKcJJusYYHmeeVhIIrp665fNNoTGGHarr4avvO2bbhUssD7ii4hiiOHffFyppV9CCFIwwqH88khNN64bbcZVVKwXXWbTTmNVVRvmmsG++XmRRthKKvlWWYSmmnPTTLOssqfxxzwpp9Zvv/L66pzMMm5PPVZ11lXxx/VZZbtnnvcZZ7auufz33/8ll4M99XXYYKL//49LLtcSSK300pXXXTjFFXVttMfZZjvPPo+bbCmddF5ZZSvYYlLQQZQQQrS00zuRRmzKKFLYYtXyyQuccUT55k1llMNwwDhCCJU00SOppotqq4Daaz


Chunk 938:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: RmzKKFLYYtXyyQuccUT55k1llMNwwDhCCJU00SOppotqq4DaazHKKwZBBzJEEwkBBJcZZFbeeZT55wwAAFqZZlL448ImmeUCCDoiiO066pnSSn633Vftt2mddCnRRsW//TfaaP3IIUkPPJAxx9qssj1ppNTMMounnJT++bmEEymwwnahhZYzzVGjjcDllKTXXjESSsy005M77qGUUVinn0sBB6p00uyVV2FqqkvttWD22E511cvmmIH++2FVVFsEE4EccroIIqIAAXLSS8688xnVV6S11Uwtt5k++ZxBB4KddfMqqxcKKdGppC6DDT/PPuh00sgqqgmggSKKKjU33k6WWZUZZeqpplYxxzpyy4MNNLkHHaFFFjWQQuX++MdCCRc++pirrGj77/9jjwjEEY5aaJnRR6SvvPR88RKWWw+CCQ577C9ee7H11YOww2TRRajCCywzzPjYYYH332uaaa1ll7+PP5aYYECHHe9++jgvvOf77R6kk38ee+dgg7J2


Chunk 939:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: H332uaaa1ll7+PP5aYYECHHe9++jgvvOf77R6kk38ee+dgg7J22Umyymh//UG99BtrrG/dd43oorD66+ZCCho99HthhAHYYMf99VCCCT722L2EEWGzzUO77bIBBtb++yaRRFHII3X99VY//SlPPk6PPSMTTeriigAbbUDyySgeehcttuN22bnnnT8XXNkUUkozzG1kkbySSYWRRKGggbRkklqaaAShhKWuuBjLLajXXUXNNfjkk53OORlUUYBYYygllL099ipyy7YggdUFFelAAkJoo2jqqHHPPmSyyiaoodlssgw55W0FF8LzzXI00wc33+7HHp9ppKeNNu3hhO655aMqqdRnnDGYYubOO6pNNlByyxNffTbHHy+11M+FFdZNNut55pvzz8eTTnGppCbpppF335xMMYbuu2vKK00ee9zRRixjj1Ott+b11nu//23JJuY007lqqDKeefZWWaMjjw5ttPNMMzsEEt9991Rffo3ppz+99xvrr+3uu5Oyy2


Chunk 940:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: efZWWaMjjw5ttPNMMzsEEt9991Rffo3ppz+99xvrr+3uu5Oyy23rrqM225PssAajj6z88yH77xy11Gqaa01ssGi88H9DD3pOOUvXXzDllRexxXoKKFlnnlCRRAlcc6KIIiQWWpikkBMww1X++pH66Q4hhK1SSDLKKqzYYOSkk+hZZDnHHCsUUSyddLjKKcZ22rSNNFFCCRmBBfUQQcpDDcKRRIc11EewwuhYYcLaaNTTT+KbbF7llE8YYHroo4krr/mbb+brrSBhheCAAckFF1g77o8jjK8vvQ9QQBCggeHhhEEffxDnnq0//+9ww7+WWvDzzjLuuu1ttO2BBYFyyIPIISMLLtowwk/11luKKarVVyTxxTOQQkICCGCaayFaaRGIIfv22XAvvJWkkItCCju99kAkkidkkDxIICLddHAkkJLllDHRRHf++sgQQgM77ThOOe9SSfAEEZFNNuCIIvdssU/hhfgPPvC88xnppX7vvsB88d7JJt+bbUBff57e


Chunk 941:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 3, 'num_chars': 512}
Text: FNNuCIIvdssU/hhfgPPvC88xnppX7vvsB88d7JJt+bbUBff57ee4cLL4B++53++l8PPlCdd8f++k4ffXuddeWTT2p11zeqquCnnymvvz5OO6leesNZZduzz<K++/fQQlg//a3eet3YYe899x6PPiSttta11>zmm9uu<GG/ggl==a==t<<e//xlliaattt>eexxiitt>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""okkndCCKY++GG++HtZZR+ooyseeDyRRYjEEypaakattkXrroy77Hhvvfz55jB77gREE52ggUlIIdHooy033tqXXxerrCQJJi+SSavssVMssE===="""">>>>AAAAAAAAAAAACCCCOOOOHHHHiiiiccccbbbbZZZZBDDBBPPNSaaSxttytttNBAABFEEEMMMIfZZZfXX7WSS/tddHussbm448a66i1pput8


Chunk 942:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: tttNBAABFEEEMMMIfZZZfXX7WSS/tddHussbm448a66i1pput887j44ReyyS6bb7yEENH99BoLL8SAAGJ00K4FFYaXXUd22VjooEVkkDoOOCgzzsYggJUxxeBJJ9CGGq/DDbeIICtppRGbboBeeVU66MyUUCIDDDYss2wBBdOyyS355tljjKJVVkBeepmmm2dwwfnvvolWWr5aagm33k3EEJk77wrss/DjjwsYYsZCCL+DD/i224nDDM6nnbcyy8WaatPXXe0HHPVvvLvssipIIIxuuVYYY3MVV+ieeBXeePvkkX0ggEEppQnvvvURR12ZZ5C66oW77eP88HUooiP22rAppizXXq/XX5+6667wwwzcc03KKQmPPKvbbgX225+bb7cYY3Kmm1WSS5k99mpMMaCppnPDDp5DDm4oodceem6XX1/ff9mOOYX11d7vvJ9aae5jjWWxxV3001v99bn22XLSSK7rruzvvs/uub800lH22yGffZxPPOvXXNv++Y


Chunk 943:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 01v99bn22XLSSK7rruzvvs/uub800lH22yGffZxPPOvXXNv++YTyycpVVGx99jKww2a++W566sEJJrdkk0g44NS11mqhhQUwwAr66oNPPFWZZDcaaRwyyQtvvoKQQ4qmmTmZZrxAARRCCwZggKIUUJUddQnFFwmCClUjjUGhh4eKK+RttFIHHXrAAUPoorIll43DDaPCCg9ZZj6XXYvhhj699V6UUHQddxUQQwavvnKpp0166INCCp9NNYTiiTONN4MVVmMHHuOnn4wCCAkffyfQQtajj1N99amhh6XYYcgiiDpZZdOHHtzggZuDDgvKKDX00DD11CiKKD2LLP4++uRffQEDD5Yrry6IIdoAAuoYYECYYEGYYJWZZP5TTqaiiC4DDyfPPJxzzGT99Gi00oQggxuhhysLLlgFF2SQQvTWWtkRRraIIuOxx3S11TjGG49KKOoWWgruugG11I966O811DZVVE1jjvF++bUmmqwQQBeVVebBBxmAA4DooND7


Chunk 944:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: G11I966O811DZVVE1jjvF++bUmmqwQQBeVVebBBxmAA4DooND77baGG9aGGeFeedXrrwwOONLAAQGttnm11bG77fzVV+6bbdUzzqebbXoddqeAA1wNNb/TTy9wwJ3TT6f//Fp++fKggw0ffSAOO6gRRimVVSYvvUsaau8XXfhootnQQyM//lHDD3KXXQ3ccivNNXLRRkP66aittgAQQUHggIAqquUvvmbzzTf44F+vvNO223+oo0qPPulyywIbblkTTT9CCGQMMNkeeg1RRkDqqvcBBI2QQ3niiSY66AYZZ1ZMMkNTTDQ33Apff+uSSYC77DECC1pffoFMMWJYYlZ22Q+CCslSSAF88tvjjPMddKuIIJDDDojWWfnQQnAMMd2HHMw77s7NN61xxHDttdxCCqBzzN2qqtyFFXkgg0mEEKhpp65ppcd88Rstt91DDPT885k//G9ppx1GGyk++J+sshNMMxe66F5SSNrjjpYWWOx99uHii10mm/8kkfUSSf


Chunk 945:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: Gyk++J+sshNMMxe66F5SSNrjjpYWWOx99uHii10mm/8kkfUSSff//KDff4EvvV2ii5nYYnExxeSFF1uxxZ0sso6yyr3jjd300o9HH1AbbY+aamr//VVSSJXZZImmmiSssK711PVaa72YY6T//Uv66T3vvet11VTUUFihhGFyyN1ddal99phjjENOOgqhh7MkkQfhhg2RRNoBBHn88OyddbtVVbGHHAKooua11BtRRZSSS2ZjjVDGG81mmrpRR7UIITJhhDA00OaKKOODDN+RRmDzzvCllXS33hLAAucLLBrjj/EWWP0ttvNhhkuddruKKXCZZOC887XwwWtzzfeTTJ9jjvCaagCrrhJFFV+00cebb9/ggPBrrSx997O55j188WhkkCuPPChoobS115/qqCCqqfH++ZSTTJ2ffjMCC4HDD5pTTJqeeKrOOfBMMkFrrN/FFzgQQkKiinaLLDx88cDllLCrrJFUUDuiifzEElB++HAOOHRSSsxEEhBtt/C8


Chunk 946:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 8, 'num_chars': 512}
Text: aLLDx88cDllLCrrJFUUDuiifzEElB++HAOOHRSSsxEEhBtt/C885woo9T66a8005gSSdrYYx9dd6wwwdDccpTkk9fOOfe++WpkkKXHHafvvelyyc3zz+XffUpnnE3kk+0ffy9HHHYWWl5+++bOOAzzzUb99jzWWqGrrqfVVO6vvoTOO=dee</uu/8aalPQQaL//tjCCe6PPxqnniU55tQ22>=99=DD<FF/aalyyafft<<e//xlliaattt>eexxiitt>> @ A @ A ! <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""vvvv4444LLLLUUUU3333ffffRRRRnnnnmmmmQQQQUUUUVVVVAAAAppppJJJJPPPPSSSSDDDDxxxxJJJJssssttttTTTTKKKKuuuueeeeIIII===="""">>>>AAAAAAAAAAAACCCCBBBBnnnniiiiccccbbbb


Chunk 947:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: II===="""">>>>AAAAAAAAAAAACCCCBBBBnnnniiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqEEEEssssRRRRBBBBoooovvvvggggqqqqiiiiQQQQiiiiKKKKKKKK6666KKKKbbbbllllxxxxWWWWssssQQQQ9999ooooYYYYppppllllMMMMJJJJuuuu3333QQQQyyyyUUUUyyyyYYYYmmmmSSSSggggllllddddOOOOXXXXGGGGXXXX3333HHHHjjjjQQQQhhhhGGGG3333ffffooooMMMM7777////8888ZZZZJJJJmmmm4444WWWW2222HHHHrrrrhhhhwwwwOOOOOOOOddddeeee7777rrrr0000nnnnSSSSBBBBhhhhVVVV2222nnnnGGGG++++rrrrddddLLLLCCCC4444ttttLLLLyyyySSSSnnnnmmmm1111ssssrrrraaaa++++ss


Chunk 948:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 4444ttttLLLLyyyySSSSnnnnmmmm1111ssssrrrraaaa++++ssssbbbbllllllllbbbb++++++++0000llllEEEEggggllllJJJJkkkk0000ssssmmmmJJJJCCCCddddAAAACCCCnnnnCCCCKKKKCCCCddddNNNNTTTTTTTTUUUUjjjjnnnnUUUUQQQQSSSSFFFFAAAAeeeeMMMMttttIIIIPPPPhhhhZZZZeeee6666333377774444llllUUUUVVVVPPPPBBBBbbbbPPPPUUUUqqqqIIIIHHHH6666MMMM++++ppppxxxxHHHHFFFFSSSSBBBBuuuuppppZZZZ++++9999HHHH55557777AAAAFFFFPPPPUUUUnnnn7777AAAA44442222kkkkFFFFAAAA////QQQQiiii5555EEEEeeeeBBBBEEEEFFFF2222MMMM77774444LLLLeeee3333bbbbVVVVqqqqTTTTkkkkTTTTwwwwHHHHnnnniiii


Chunk 949:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: LLeeee3333bbbbVVVVqqqqTTTTkkkkTTTTwwwwHHHHnnnniiiiFFFFqqqqQQQQKKKKCCCCjjjjRRRR66669999ppppccccXXXXCCCCppppzzzzGGGGhhhhGGGGvvvvMMMMkkkkFFFFJJJJdddd11110000mmmm0000nnnnyyyyGGGGppppKKKKWWWWZZZZkkkkXXXXPPPPFFFFSSSSRRRRRRRRKKKKEEEEhhhh6666hhhhPPPPuuuuooooZZZZyyyyFFFFBBBBPPPPllllZZZZ5555MMMM3333xxxxvvvvDDDDQQQQKKKKCCCCGGGGMMMMhhhhDDDDTTTTFFFFNNNNZZZZyyyyoooovvvvyyyyccccyyyyFFFFCCCCssss1111iiiiggggPPPPTTTTmmmmdddd++++ooooZZZZrrrr1111cccc////MMMM////rrrrppppjjjjoooo66668888zzzzPPPPKKKKkkkk1111QQQQTTTTjjjjqqqqeeeeLL


Chunk 950:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 66668888zzzzPPPPKKKKkkkk1111QQQQTTTTjjjjqqqqeeeeLLLLooooppppRRRRBBBBLLLLWWWWCCCCeeeeCCCCQQQQyyyyppppJJJJFFFFiiiizzzzkkkkSSSSEEEEIIIISSSS2222ppppuuuuhhhhXXXXiiiiAAAAJJJJMMMMLLLLaaaaJJJJFFFFccccxxxxIIIIbbbbiiiizzzzLLLL8888++++TTTT1111nnnnHHHHNNNNddddWWWWrrrruuuu9999UUUUmmmm1111ffffllllHHHHEEEEUUUUQQQQZZZZ77774444AAAAAAAAccccAAAARRRReeeeccccggggjjjjqqqq4444AAAAgggg3333QQQQBBBBBBBBgggg8888ggggmmmmffffwwwwCCCCtttt6666ssssJJJJ++++vvvvFFFFeeeerrrrcccc++++ppppqqqq0000llllqqqq5555jjjjZZZZBBBBXXXX9999ggggffffffff4444


Chunk 951:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 73, 'num_chars': 512}
Text: 00llllqqqq5555jjjjZZZZBBBBXXXX9999ggggffffffff4444AAAAqqqqoooouuuuYYYYnnnngggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
Word2vec word = “sweden”
Word2vec Continuous Bag of Words (CBOW) Skip-grams
Skip-gram • The idea: we want to use words to predict their context words • Context: a fixed window of size 2m
Skip-gram
Skip-gram: objective function • For each position , predict context words within t = 1,2,...T context size m, given center word : w j all the parameters to be optimized T (✓) = P (w w ; ✓)


Chunk 952:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 29, 'num_chars': 512}
Text: the parameters to be optimized T (✓) = P (w w ; ✓) t+j t L | t=1 m j m,j=0 Y Y     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""eeeeVVVVYYYY3333kkkk2222hhhh9999ooooFFFFNNNNiiii4444RRRRVVVVzzzzOOOO++++rrrrddddBBBBhhhhuuuuffffttttGGGGssss===="""">>>>AAAAAAAAAAAACCCCTTTTXXXXiiiiccccbbbbZZZZFFFFNNNNaaaaxxxxssssxxxxEEEEIIIIaaaa1111zzzzrrrreeeeTTTTJJJJmmmm555566667777EEEEXXXXUUUUBBBBBBBByyyySSSSmmmmtttt1111SSSSaaaaCCCCEEEEEEEEQQQQnnnnrrrrppppooooQQQQccccXXXX4444iiiiTTTT


Chunk 953:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: EEEEEEQQQQnnnnrrrrppppooooQQQQccccXXXX4444iiiiTTTTggggddddRRRRaaaattttddddhhhhwwwwrrrrkkkkbbbbRRRRbbbbaaaaTTTTbbbbBBBBLLLLPPPPssssHHHHccccyyyynnnn00001111nnnn////RRRRSSSSwwww4444ttttJJJJUUUURRRRrrrr7777yyyyFFFFffffAAAA0000KKKKPPPP3333ppppnnnnRRRRxxxx6666ssss4444kkkk8888KKKKiiii7777////////2222GGGGnnnnPPPPzzzzCCCC4444ttttLLLLyyyyyyyyvvvvNNNN1111bbbbVVVVXXXX6666xxxxuuuutttt11115555vvvvHHHHNNNNssss0000NNNNhhhhzzzz5555PPPPZZZZWWWWppppOOOOYYYY2222ZZZZBBBBCCCCgggg11119999FFFFCCCCjjjjhhhhNNNNDDDDPPPPAAAAVVVVCCCCzz


Chunk 954:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 11119999FFFFCCCCjjjjhhhhNNNNDDDDPPPPAAAAVVVVCCCCzzzzhhhhJJJJLLLL77778888UUUUuuuuVVVVPPPPrrrrssssBBBBYYYYkkkkeeeeoooojjjjnnnnGGGGQQQQwwwwVVVVOOOOxxxxcccciiii5555HHHHggggDDDDJJJJ0000UUUUttttZZZZJJJJQQQQMMMMRRRRxxxxzzzzJJJJoooottttvvvvZZZZSSSSffffEEEEMMMMSSSSDDDDbbbbppppvvvvssss0000zzzzEEEEyyyyaaaaRRRRAAAAXXXXuuuuBBBB++++VVVVZZZZccccVVVVTTTTWWWWyyyy////eeeeKKKKhhhhhhhhJJJJ++++0000IIIIvvvvZZZZppppHHHHYYYYrrrr0000oooo77778888ssssuuuuhhhh1111rrrrllll33339999zzzzkkkkVVVVJJJJQQQQyyyyUUUUSSSSeeeehhhh3333hhhhHHHHqqqq


Chunk 955:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: kkVVVVJJJJQQQQyyyyUUUUSSSSeeeehhhh3333hhhhHHHHqqqq33333333KKKK6666NNNNWWWW2222++++////666600006666DDDDPPPPIIIIaaaaiiiihhhhTTTTeeeerrrrooooRRRRaaaa1111ffffYYYYZZZZLLLLyyyyXXXXIIIIFFFFGGGGLLLLppppmmmm1111gggg8888DDDDPPPPccccFFFFggggwwwwgggg4444JJJJLLLLKKKKJJJJtttthhhhbbbbiiiiFFFFjjjj////JJJJKKKKddddwwww8888CCCChhhhZZZZggggrrrrssssssssJJJJiiii6666UUUUddddIIIIttttppppyyyyRRRR0000llllBBBBoooo3333NNNNNNNNKKKKpppp++++rrrrCCCCjjjjYYYYMMMMrrrraaaaiiiiYYYYppppddddZZZZffffVVVV2222++++zzzzRRRRXXXXiiiiSSSS////llllBBBBjj


Chunk 956:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ffffVVVV2222++++zzzzRRRRXXXXiiiiSSSS////llllBBBBjjjjmmmmOOOOPPPPgggg8888LLLLoooobbbbMMMMccccQQQQffffPPPPZZZZQQQQaaaaNNNNccccUUUUkkkkxxxxppppZZZZSSSS1111NNNNhhhhAAAAGGGGOOOOccccuuuuKKKKAAAAccccSSSSPPPPccccXXXXSSSSkkkkffffMMMM8888MMMM4444uuuugggg9999ooooOOOOhhhhOOOOCCCCpppp00009999++++DDDDssssccccffffuuuuooooHHHHffffDDDDbbbb5555////bbbbBBBB8888cccc1111nnnnYYYYsssskkkk7777ffffkkkkHHHHeeeemmmmQQQQggggHHHHwwwwiiiiBBBB++++QQQQrrrr6666ZZZZEEEE++++4444eeeeSSSSGGGG////CCCCFFFF////yyyyTTTT////vvvvpppp3333ffffrrrr////


Chunk 957:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 46, 'num_chars': 512}
Text: //CCCCFFFF////yyyyTTTT////vvvvpppp3333ffffrrrr////ffffffffuuuuZZZZqqqqUUUUNNNNrrrr++++55555555QQQQxxxx5555FFFFYYYY++++kkkkeeeeuuuurrrraaaayyyy++++gggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • The objective function is the (average) negative log likelihood: J(θ) T 1 1 J(✓) = log (✓) = log P (w w ; ✓) t+j t   T L   T | t=1 m j m,j=0 X X     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKP


Chunk 958:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: wnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>AAAAAAAAAAAACCCCeeee3333iiiiccccddddVVVVFFFFddddaaaaxxxxQQQQxxxxFFFFMMMM2222MMMMWWWWuuuuuuuuqqqq7777aaaaqqqqPPPPggggggggQQQQXXXXccccaaaa3333ttttMMMMiiiiOOOOKKKKgggghhhhSSSSKKKKvvvvoooojjjj4444ssssEEEEKKKK3333LLLLeeeexxxxssssllll0000zzzz2222zzzzmmmm7777aaaaJJJJDDDDMMMMmmmmdddd5555QQQQllll5555EEEE////444400003333zzzzzzzznnnn////ggggiiiimmmmNNNNkkkkddddRRRRFFFFuuuu9999EEEEHHHHLLLLuuuuuuuuVVVV////JJJJuuuuXXXXkkkklll


Chunk 959:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 999EEEEHHHHLLLLuuuuuuuuVVVV////JJJJuuuuXXXXkkkkllllhhhhccccUUUUkkkk++++RRRR7777FFFFVVVV66665555eeee22227777iiii++++eeeeaaaaNNNNzzzz88889999bbbbttttrrrreeee3333uuuunnnnbbbbttttHHHHttttqqqqwwwwNNNNhhhhxxxxEEEEvvvvZZZZWWWWllllOOOOccccmmmmZZZZBBBBCCCCgggg0000jjjjFFFFCCCCjjjjhhhhppppDDDDLLLLAAAAVVVVCCCC7777hhhhOOOODDDD9999////22228888SSSSPPPPPPPP4444OOOOxxxxoooottttSSSSHHHHuuuuKKKKxxxxggggooootttthhhhcccciiii0000JJJJwwwwhhhhooooGGGGaaaaddddrrrr++++++++77772222eeee4444AAAAGGGGRRRRPPPP6666DDDD7777ddddyyyywwwwrrrrD


Chunk 960:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: e4444AAAAGGGGRRRRPPPP6666DDDD7777ddddyyyywwwwrrrrDDDDuuuuEEEEuuuu9999OOOO////SSSSZZZZLLLLOOOOcccc0000UUUUwwwwwwwwXXXXnnnnEEEEnnnn3333wwwwffff8888vvvvzzzzddddZZZZqqqq6666nnnnAAAA////9999aaaaeeee////vvvvTTTT1111FFFFMMMMwwwwmmmmffff6666NNNNnnnn6666UUUUrrrrssssNNNN0000ggggEEEEllll3333qqqq3333aaaaDDDDvvvvttttffffQQQQssss3333TTTTMMMMxxxx8888GGGGiiiiBBBBllllttttHHHHPPPP++++aaaattttggggPPPP8888ttttNNNNttttLLLLBBBBssssnnnnKKKK6666GGGGWWWWQQQQttttqqqqBBBBHHHHWWWWhhhhttttOOOOuuuu9999++++yyyyWWWWccccllllrrrrBBBBRRRRqqq


Chunk 961:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: tttOOOOuuuu9999++++yyyyWWWWccccllllrrrrBBBBRRRRqqqq5555ZZZZNNNNaaaaOOOO00006666TTTTCCCCiiiiWWWWMMMMGGGGBBBBZZZZffffggggOOOO1111llllttttooooWWWWLLLL8888nnnnMMMM1111hhhhHHHHKKKKBBBBmmmmCCCCuuuuzzzzEEEErrrrbbbbTTTTzzzz9999FFFFFFFFggggZZZZrrrrQQQQooooTTTTTTTTggggaaaa6666YYYYrrrr9999ssss8888IIIIxxxxZZZZeeee1111SSSS5555SSSSGGGGzzzzEEEEccccNNNNeeeejjjjDDDDXXXXkkkkvvvv2222LLLLjjjjGGGGoooottttXXXXEEEEyyyydddd0000VVVVSSSSNNNNoooovvvvhhhh5555UUUU1111JJJJJJJJiiiiSSSSZZZZttttFFFF0000JJJJkkkkwwwwwwwwFFFFEEEEuuuuAAAA2222D


Chunk 962:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ZttttFFFF0000JJJJkkkkwwwwwwwwFFFFEEEEuuuuAAAA2222DDDDcccciiiiPPPPBBBBWWWWyyyyhhhhccccssssCCCCIIIItttthhhhXXXXZZZZ0000ggggQQQQnnnnrrrrxxxxyyyy5555ffffBBBB0000bbbbNNNNBBBBmmmmggggzzzzSSSSjjjj888899997777BBBB22229999aaaaOOOOTTTTbbbbJJJJffffffffKKKKQQQQ9999EEEEllllKKKKXXXXppppIIIIDDDD8888oooo4444MMMMyyyyYYYYhhhhwwww8888iiiiNNNN6666EEEEDDDD2222OOOO++++ttttHHHHPPPPuuuuBBBBffffvvvvxxxxLLLLvvvvrrrr1111DDDDhhhhqqqqaaaa++++6666RRRRvvvvyyyyxxxx++++8888QQQQttttAAAA7777bbbb8888++++<<<<////llllaaaatttteeeexxxxiiiitttt>>>


Chunk 963:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 38, 'num_chars': 512}
Text: bbb8888++++<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
How to define ? P(w ∣ w ; θ) t+j t • We have two sets of vectors for each word in the vocabulary d u R : embedding for target word i i 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""QQQQssssggggoooo7777bbbbHHHHXXXXmmmmddddtttt////5555AAAAAAAAiiiioooowwwwyyyyqqqqkkkkJJJJ////9999EEEE++++0000===="""">>>>AAAAAAAAAAAACCCCBBBBnnnniiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ3333UUUUrrrr1111qqqq////oooohhhh5555FFFFWWWWCC


Chunk 964:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: JJJJ3333UUUUrrrr1111qqqq////oooohhhh5555FFFFWWWWCCCCyyyyCCCCpppp5555KKKKIIIIooooMMMMeeeeiiiiFFFF44449999VVVV7777AAAAeeee0000MMMMWWWWyyyy2222mmmm3333bbbbppppZZZZhhhhNNNN2222NNNN0000IIIIJJJJOOOOXXXXnnnnxxxxrrrr3333jjjjxxxxooooIIIIhhhhXXXXffff4444MMMM3333////44442222bbbbttttggggddddttttffffTTTTDDDDwwwweeeeGGGG++++GGGGmmmmXXXXllllBBBBwwwwppppnnnnSSSSjjjjvvvvNNNNttttllllZZZZaaaaWWWWVVVV1111bbbbXXXXyyyyuuuuuuuuVVVVjjjjcccc2222tttt7777RRRR11117777dddd6666++++llll4444llllQQQQSSSS2222iiiiQQQQxxxxjjjj2222UUUUnnnnwwww


Chunk 965:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 44llllQQQQSSSS2222iiiiQQQQxxxxjjjj2222UUUUnnnnwwwwIIIIppppyyyyJJJJmmmmhhhhTTTTMMMM88881111ppppJJJJ5555EEEEUUUURRRRwwwwGGGGnnnn7777WWWWBBBB0000VVVVffffjjjjttttBBBByyyyooooVVVViiii8888WWWWddddHHHHiiiiffffUUUUiiii////BBBBAAAAssssJJJJAAAARRRRrrrrIIII3333kkkk22224444eeee9999CCCCOOOOtttthhhhEEEEGGGGZZZZpppp7777jjjjPPPPUUUUYYYYwwwwJJJJNNNNhhhhSSSSCCCC7777zzzzeeee////7777vvvvllll11111111aaaassss4444EEEEaaaaJJJJGGGG4444MMMM1111KKKKFFFFGGGGRRRRqqqq++++////ddddXXXXrrrrxxxxyyyySSSSNNNNqqqqNNNNCCCCEEEEYYYY6666WWWW6666rr


Chunk 966:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: xxxxyyyySSSSNNNNqqqqNNNNCCCCEEEEYYYY6666WWWW6666rrrrppppNNNNooooLLLL8888NNNNSSSSMMMM8888JJJJppppXXXXuuuummmmlllliiiiiiiiaaaaYYYYjjjjPPPPCCCCAAAAddddgggg0000VVVVOOOOKKKKLLLLKKKKyyyyyyyyZZZZvvvv5555OOOOjjjjYYYYKKKKHHHH0000UUUUxxxxttttKKKKUUUU0000GGGGiiiiiiii////pppp7777IIIIccccKKKKTTTTUUUUOOOOAAAAppppMMMMZZZZ3333GGGGjjjjmmmmvvvvccccKKKK8888TTTT++++vvvvmmmm++++rrrrwwwwwwwwssssuuuuYYYYSSSSFFFFJJJJNNNNBBBBZZZZkkkkuuuuCCCCllllOOOOOOOOddddIIIIyyyyKKKKTTTTFFFFCCCCffffSSSSUUUUoooo0000HHHHxxxxuuuuCCCCiiiiWWWWTTTTmmmm


Chunk 967:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 10, 'num_chars': 512}
Text: ffSSSSUUUUoooo0000HHHHxxxxuuuuCCCCiiiiWWWWTTTTmmmmVVVVkkkkSSSSGGGGWWWWGGGGKKKKiiiiTTTTXXXXIIIIVVVVEEEE4444IIII7777////////IIIIiiiiaaaaZZZZ3333WWWWXXXXKKKKffffmmmm3333ppppxxxxVVVV66665555eeeezzzzOOOOMMMMppppwwwwAAAAEEEEddddwwwwAAAAiiii6666ccccQQQQxxxx2222uuuuooooQQQQFFFFNNNNIIIIPPPPAAAAIIIIzzzz////AAAAKKKKbbbb9999aaaaTTTT9999WWWWKKKK9999WWWWxxxx////TTTT1111ppppIIII1111mmmm9999mmmmHHHHPPPP7777AAAA++++ffffwwwwBBBB1111FFFFZZZZkkkkZZZZ<<<<////llllaaaatttteeeexxxxiiiitttt>>>> d v R : embedding for context word i’


Chunk 968:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 15, 'num_chars': 512}
Text: iiiitttt>>>> d v R : embedding for context word i’ i 0 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""jjjjllllnnnnCCCCkkkkKKKKyyyyjjjjEEEEggggmmmmmmmmzzzzyyyyrrrrWWWWVVVVffffCCCCHHHH8888VVVVFFFFvvvvPPPPBBBB4444===="""">>>>AAAAAAAAAAAACCCCCCCCXXXXiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ33334444WWWWeeeettttXXXX1111KKKKOOOOXXXXxxxxSSSSJJJJ6666KKKKooookkkkIIIIeeeeiiiixxxx66668888VVVVjjjjFFFFffffkkkkAAAAbbbbyyyy2222aaaa7777aaaaZZZZdddduuuuNNNNmmmmFFFF3333UUUUyyyygg


Chunk 969:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: aaaa7777aaaaZZZZdddduuuuNNNNmmmmFFFF3333UUUUyyyygggghhhhVVVVyyyy////++++FFFFSSSS8888eeeeFFFFPPPPHHHHqqqqPPPP////DDDDmmmmvvvv3333HHHHTTTT5555qqqqCCCCttttDDDDwwwwYYYYeeee777788880000wwwwMMMM8888++++PPPPOOOOVVVVPPPPaaaaccccbbbb6666ttttppppeeeeWWWWVVVV1111bbbbXXXX11110000kkkkZZZZ5555cccc2222tttt7777ZZZZ9999ffffeeee22222222++++qqqqKKKKJJJJGGGGEEEENNNNkkkkjjjjEEEEIIII9999nnnn2222ssssaaaaKKKKccccCCCCddddrrrrQQQQTTTTHHHHPPPPaaaajjjjiiiiXXXXFFFFoooocccc9999ppppyyyyxxxx9999dddd555533335555rrrrTTTTKKKKVVVViiiikkkkbbbb


Chunk 970:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: xx9999dddd555533335555rrrrTTTTKKKKVVVViiiikkkkbbbbjjjjXXXXkkkk5555hhhh6666IIIIRRRR4444IIIIFFFFjjjjCCCCCCCCttttZZZZFFFF6666NNNNuuuuqqqqGGGGWWWWAAAA////9999IIIIBBBB1111nnnnvvvvZZZZSSSSddddZZZZKKKKjjjjLLLLRRRRKKKKHHHH55556666VVVV333322220000OOOO////ZZZZFFFFaaaaffffqqqqTTTTIIIIEEEEWWWWiiiiVVVVuuuuQQQQCCCChhhhSSSSoooo9999++++yyyyvvvvbbbbjjjj8888iiiiSSSSUUUUiiiiFFFFJJJJhhhhwwwwrrrr1111XXXXGGGGddddWWWWHHHHssssppppllllppppooooRRRRTTTTrrrrNNNNyyyyNNNN1111EEEE0000xxxxmmmmSSSSEEEEBBBB7777RRRRjjjjqqqqMMMMAAAAhhhhVVVVVV


Chunk 971:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: mmmmSSSSEEEEBBBB7777RRRRjjjjqqqqMMMMAAAAhhhhVVVVVVVV44446666////SSSSRRRRDDDDxxxx0000bbbbppppooooyyyyCCCCSSSSppppooooRRRRGGGGUUUU////XXXX3333RRRRIIIIppppDDDDppppSSSSaaaahhhhbbbbzzzzrrrrzzzzGGGG9999WWWW8888llll4444vvvv////eeeeZZZZ1111EEEEBBBB5555ddddeeeeyyyykkkkSSSSccccaaaaCCCCrrrrIIIIbbbbFFFFGGGGQQQQccccKKKKQQQQjjjjllllMMMMeeeeCCCC++++kkkkxxxxSSSSoooovvvvnnnnEEEEEEEEEEEEwwwwkkkkMMMM7777cccciiiiMMMMssssQQQQSSSSEEEE22223333CCCCKKKK5555ssssQQQQ3333PPPPmmmmXXXXFFFF0000nnnnzzzzrrrrOOOOoooo6666VVVVffffffff2222vvvv


Chunk 972:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 19, 'num_chars': 512}
Text: FF0000nnnnzzzzrrrrOOOOoooo6666VVVVffffffff2222vvvvFFFFKKKK7777KKKKuuuuIIIIoooowwwwSSSSEEEEccccwwwwSSSSmmmm4444ccccAAAAEEEE1111uuuuIIIIEEEE6666NNNNIIIIDDDDAAAAIIIIzzzzzzzzDDDDKKKK7777xxxxZZZZTTTT9999aaaaLLLL9999WWWW55559999zzzzFFFFqqqqXXXXrrrrGGGGLLLLmmmmAAAAPPPP7777AAAA++++vvvvwwwwBBBBvvvvMMMMiiiiaaaaVVVVwwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • u v Use inner product to measure how likely word i i i · 0 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""RRRRzzz


Chunk 973:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: aaa1111____bbbbaaaasssseeee66664444====""""RRRRzzzzTTTTZZZZ0000bbbbVVVVGGGG1111ttttXXXX3333mmmm7777GGGGXXXXeeeessssooooGGGGaaaabbbb////HHHHjjjjRRRRIIII===="""">>>>AAAAAAAAAAAACCCCCCCC3333iiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEENNNN3333UUUUrrrr1111qqqq////oooohhhh66669999LLLLCCCC2222iiiipppp5555KKKKIIIIooooMMMMeeeeiiiiFFFF44448888VVVVbbbbCCCCuuuu0000IIIIWWWWwwww2222mmmm3333bbbbppppZZZZjjjjffffssssbbbbggggoooollll5555OOOO7777FFFFvvvv++++LLLLFFFFggggyyyyJJJJeeee////QQQQPPPPeeee////DDDDdddduuuu2222ggggjjjja


Chunk 974:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: Jeeee////QQQQPPPPeeee////DDDDdddduuuu2222ggggjjjjaaaa++++mmmmDDDDgggg8888dddd4444MMMMMMMM////OOOOCCCChhhhFFFFGGGGllllHHHHeeeeffffLLLLqqqqqqqqyyyyssssrrrrqqqq1111vvvvVVVVDDDDddddrrrrWWWW9999ssss7777uuuu3333vvvv2222////kkkkFFFFXXXXiiiiVVVVRRRRiiii0000ssssGGGGCCCCCCCCXXXXkkkkffffIIIIEEEEUUUUYYYY5555aaaaSSSSjjjjqqqqWWWWbbbbkkkkPPPPppppEEEEEEEExxxxQQQQEEEEjjjjvvvvWWWWBBBB8888XXXXffffiiii9999CCCCZZZZGGGGKKKKCCCCnnnn6666nnnnppppwwwwnnnnxxxxYYYYjjjjTTTTkkkkNNNNKKKKIIIIYYYYaaaaSSSSPPPP5555ddddnnnn0000QQQQIIIIzzzz000


Chunk 975:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: IIIYYYYaaaaSSSSPPPP5555ddddnnnn0000QQQQIIIIzzzz0000KKKKooooiiiizzzzNNNNffffQQQQooooHHHHOOOOBBBBQQQQaaaa////kkkkiiiiTTTT3333MMMM////ooooSSSSeeee7777bbbbDDDDaaaaffffppppzzzzAAAACCCCXXXXiiiiVVVVuuuuSSSSBBBBiiiijjjjRRRR9999uuuu3333PPPPQQQQSSSShhhhwwwwGGGGhhhhOOOOuuuuMMMMUUUUNNNNKKKK9999VVVV0000nnnn0000VVVV6666GGGGppppKKKKaaaaYYYYkkkkbbbbwwww2222SSSSBBBBVVVVJJJJEEEEBBBB6666jjjjIIIIeeeekkkkbbbbyyyyllllFFFFMMMMllllJJJJffffNNNNffffssssnnnnhhhhssssVVVVFFFFCCCCGGGGAAAAllllppppiiiimmmmssss4444UUUU33339999PPPPZZZZCCCCh


Chunk 976:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: Allllppppiiiimmmmssss4444UUUU33339999PPPPZZZZCCCChhhhWWWWaaaahhhhooooHHHHpppprrrrMMMM4444UUUUyyyy11116666hhhhffffiiiiffff1111000099991111ddddOOOOllllllllllllCCCCeeeeppppJJJJhhhhzzzzPPPPFFFF0000UUUUppppgggg1111rrrrAAAAIIIIhhhhggggYYYYUUUUkkkkmmmmwwwwZZZZllllNNNNDDDDEEEEJJJJbbbbUUUU3333AAAArrrrxxxxCCCCEEEEmmmmEEEEttttYYYYmmmmvvvvZZZZkkkkJJJJwwwwFFFF11119999eeeeJJJJtttt2222zzzzppppuuuussss00003333ddddvvvvzzzzRRRRuuuuuuuuqqqqjjjjKKKKMMMMKKKKjjjjkkkkAAAAddddnnnnAAAAIIIIXXXXXXXXIIIIAAAAWWWWuuuuAAAAFFFFtttt0000AAA


Chunk 977:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 41, 'num_chars': 512}
Text: AAAIIIIXXXXXXXXIIIIAAAAWWWWuuuuAAAAFFFFtttt0000AAAAEEEEYYYYPPPPIIIIAAAAnnnn8888AAAAJJJJeeeerrrrUUUUffffrrrr2222XXXXqqqqzzzz3333uuuueeeettttFFFFaaaauuuuccccOOOOQQQQRRRR////YYYYHHHH11118888AAAA6666ZZZZPPPPmmmm2222ssss====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> appears with context word i’, the larger the better “softmax” we learned last time! exp(u v ) w w P (w w ) = t · t+j t+j t | exp(u v ) w k k V t · <<<<<<<<<<<<<lllllllllllllaaaaaaaaaaaaattttttttttttteeeeeeeeeeeeexxxxxxxxxxxxxiiiiiiiiiiiiittttttttttttt 


Chunk 978:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 2, 'num_chars': 512}
Text: eeeeeeeeeexxxxxxxxxxxxxiiiiiiiiiiiiittttttttttttt ssssssssssssshhhhhhhhhhhhhaaaaaaaaaaaaa1111111111111_____________bbbbbbbbbbbbbaaaaaaaaaaaaassssssssssssseeeeeeeeeeeee66666666666664444444444444============="""""""""""""YYYheeJYYYYYYxxxP++wxxxxxxUUU+zzMUUUUUU1116++v111111xxxL99/xxxxxx444rffs444444JJJUOOOJJJJJJ555frrf555555AAA2oozAAAAAAllldxxallllllDDD3ssSDDDDDDTTTt44RTTTTTT333Z330333333JJJaUUcJJJJJJ///lnnv//////DDDdXXFDDDDDDpppqSSGpppppp+++attL++++++pppQWWspppppp555QJJc555555333v//r333333QQQESSaQQQQQQpppKbbh


Chunk 979:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: pppp555QJJc555555333v//r333333QQQESSaQQQQQQpppKbbhppppppgggM11sggggggiiiXgg/iiiiii+++y88c++++++UUUwoo0UUUUUU=============""""""""""""">>>>>>>>>>>>>AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACCCBCCCCCCCCCccc2ZZcccccccnnnX33nnnnnnniiiiiiiiiiiiicccccccccccccjjjbjjjjjjjjjVVVZVVVVVVVVVFFFDFFHFFFFFFdddNddRddddddaaaSSSTaaaaaa999gxxt999999sssMwwssssssswwwxxxwwwwwwwFFFFFFFFFFFFFJJJIMMHJJJJJJXXXX11XXXXXXXdddvMMCddddddbbb1rrxbbbbbbcccLddlcccccc3338XXg333333SSS6VV3SSSSSSfffV22offffffaaaqrrGaaaaaaQQQ1XXPQQQQQQdddrggiddddddf


Chunk 980:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: V22offffffaaaqrrGaaaaaaQQQ1XXPQQQQQQdddrggiddddddfffNiihffffffddd8wwUddddddlllEUUmllllllgggibbbgggggg000uuut000000xxxCxxwxxxxxxYYYoRRqYYYYYYGGGz22pGGGGGGCCCbEEFCCCCCCRRRnZZVRRRRRRvvvQaaOvvvvvvBBBpZZVBBBBBBHHHuvv7HHHHHHoooHttAooooooPPPFQQVPPPPPPuuuZXXeuuuuuupppwQQJppppppRRRbffiRRRRRRCCCZDDHCCCCCC666CFF2666666lllOxxsllllllzzz5yyszzzzzz111R11d111111mmmM00Ommmmmmsss5VVWssssssKKKk22gKKKKKKSSS4FFtSSSSSSFFFbnnSFFFFFFOOOmGGUOOOOOOBBBsTT0BBBBBBhhhkKKWhhhhhhZZZMZZOZZZZZZlllyjj4lllllllllRMM4llllllsss2ZZCssssssttthNNpt


Chunk 981:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ZZZlllyjj4lllllllllRMM4llllllsss2ZZCssssssttthNNpttttttkkkDMM7kkkkkkmmmHkkUmmmmmmyyy0NNTyyyyyykkkByy2kkkkkk666FRRD666666777211V777777ZZZ511BZZZZZZBBBECCZBBBBBB666fff/666666AAACoogAAAAAAfff9BBBffffff0003//+000000777vzzj777777///ozzz//////WWW3ddeWWWWWWtttp//+ttttttvvvzhhgvvvvvv6660SSh666666IIIJ//cIIIIIIvvvb+++vvvvvv+++DAAY++++++wwwwLLGwwwwwwFFFQPP7FFFFFFVVV+jjIVVVVVVPPPzCCpPPPPPPAAAkvvAAAAAAA+++nXX3++++++666Ijj2666666dddvoosddddddgggSQQCgggggg888cccN888888777uCCZ777777IIIlhhOIIIIIIDDDL33jDDDDDDiiiQNNriiiiiicc


Chunk 982:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: CCZ777777IIIlhhOIIIIIIDDDL33jDDDDDDiiiQNNriiiiiicccUuuncccccccccB773cccccc+++Nrru++++++///900v//////VVVe55rVVVVVV111byy4111111VVVWSS6VVVVVVFFFdrrwFFFFFFWWW3BBSWWWWWWCCCbDD3CCCCCCWWW/UUEWWWWWW444+TTA444444ggggRRUggggggiiifXX3iiiiiiqqquRRQqqqqqq666gBBb666666CCCf++hCCCCCCcccNWWyccccccOOOfPPrOOOOOOPPPzqqPPPPPPPBBBj44nBBBBBBwwwk//qwwwwww0009GGy000000eeeNll/eeeeeebbbmllWbbbbbbnnnottXnnnnnnccc2bbnccccccffffWWZffffffdddz22eddddddrrr0//vrrrrrrSSSgnnVSSSSSSdddjnn7ddddddPPPsjjfPPPPPPnnniSS2nnnnnnzzzl33Ozzzzzz333ztty33


Chunk 983:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ddPPPsjjfPPPPPPnnniSS2nnnnnnzzzl33Ozzzzzz333ztty333333vvvlzz+vvvvvvbbb5ff2bbbbbbeeejWWZeeeeee///nTTq//////MMMmKKYMMMMMMTTTFWWsTTTTTTFFFplltFFFFFFlllXMMallllllrrrU22Urrrrrryyy2ppjyyyyyyqqqCKKWqqqqqqaaaVUUkaaaaaa000Joop000000FFFC99SFFFFFFKKKpWWnKKKKKKUUU8ll2UUUUUU+++LGGS++++++yyygDDEyyyyyyooozBBcoooooohhhyNNMhhhhhhhhhLccEhhhhhhgggFssVggggggiiifSS2iiiiiisssbFFwssssss222jwwM222222BBB6EEHBBBBBBQQQfOOAQQQQQQ6660yyQ666666CCCi007CCCCCCHHH700qHHHHHHVVV7ooTVVVVVVWWW+zzQWWWWWWaaagIIjaaaaaaEEEsTTMEEEEEEZZZTLLhZZZZZZkkk


Chunk 984:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: zQWWWWWWaaagIIjaaaaaaEEEsTTMEEEEEEZZZTLLhZZZZZZkkkLCCPkkkkkkJJJXTTsJJJJJJdddTbbOddddddpppzHHJppppppgggQootggggggtttr00/ttttttvvvM99Wvvvvvv666M00/666666777r88r7777779994uuH999999wwwWmmFwwwwww111MTT0111111OOOtaawOOOOOOmmmU88bmmmmmmDDDCVVXDDDDDDSSSkHHqSSSSSS///799q//////VVVOhhfVVVVVVDDD6VVsDDDDDD111orrK111111hhhyGGjhhhhhhVVVaJJYVVVVVVbbbrJJTbbbbbbCCCaGGJCCCCCCHHHAeeJHHHHHHJJJdKKTJJJJJJsssLFFxsssssseeeM55QeeeeeeIIIWwwtIIIIIIFFF2SSOFFFFFFpppI88CppppppwwwVFFXwwwwwwSSSxLLgSSSSSS888Daap888888lllCvv7llllllPPP9hhVPPP


Chunk 985:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: wSSSxLLgSSSSSS888Daap888888lllCvv7llllllPPP9hhVPPPPPPYYYYll6YYYYYYuuua00PuuuuuuJJJNrr+JJJJJJooob11pooooooOOO+IIeOOOOOOzzzGLLpzzzzzz111Seeh111111MMMSxxbMMMMMMKKK7cc1KKKKKKHHHKOOzHHHHHHnnnDJJhnnnnnnwwwD55xwwwwww444ULLP444444nnnJnnJnnnnnnkkkx++ckkkkkkuuuaCC3uuuuuufff0qqyffffff444dFFZ444444LLLhHHwLLLLLLIIIEjjgIIIIIIUUUF77BUUUUUUhhhBAA/hhhhhh333USSx333333ssscaaUssssssdddUEEmddddddJJJNJJhJJJJJJoooSttCooooooQQQaQQbQQQQQQmmmFmmUmmmmmm111w77J111111CCC7rruCCCCCCTTTgrr6TTTTTTuuu9qqruuuuuuvvvLJJ6vvvvvvBBBiJJiBBBBBBooow


Chunk 986:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 6TTTTTTuuu9qqruuuuuuvvvLJJ6vvvvvvBBBiJJiBBBBBBooowHHSooooookkkUCCRkkkkkkkkkXeewkkkkkkcccUFFlccccccJJJzbbhJJJJJJIII7ZZWIIIIIIVVVG222VVVVVVtttgqqdttttttnnnUffqnnnnnnaaaPWWnaaaaaapppNuu1pppppp999Mrrr999999aaa766qaaaaaa666RNN+666666vvvR55jvvvvvvoootCCeoooooo333RffQ333333kkkxhhnkkkkkkJJJzZZ4JJJJJJ+++zvvt++++++IIIimm3IIIIII9996zzz999999888dkkR888888222kppy222222sss7ooMssssssjjjAPP0jjjjjjNNN0rrENNNNNNBBBNOONBBBBBBDDD+ZZ3DDDDDDddd5uuAdddddd000oYY2000000NNNYWWMNNNNNNjjjkqqbjjjjjjGGGvZZVGGGGGG111322M111111TTT9ii7TTTT


Chunk 987:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: jjjkqqbjjjjjjGGGvZZVGGGGGG111322M111111TTT9ii7TTTTTTOOO4hhROOOOOO000uOOw000000SSSKuunSSSSSSJJJZ88XJJJJJJ1119MMO111111zzzbjjGzzzzzzhhhO99Jhhhhhhmmmsbb+mmmmmmffft449ffffffvvvjdd8vvvvvvfffdppJffffffCCCzzz8CCCCCCcccD774ccccccuuuhmmFuuuuuuhhhN00zhhhhhhccc733acccccc222GYYb222222mmmanncmmmmmmvvv266XvvvvvvHHHMUUDHHHHHH444PQQa444444222/PPM222222iiiL88GiiiiiiBBBBll+BBBBBBvvvissCvvvvvvgggWQQmgggggg+++lLLJ++++++iiit00WiiiiiiVVV1kk9VVVVVVvvvEEEJvvvvvvSSSlLLDSSSSSSRRRdDDLRRRRRRyyyVNNUyyyyyy000ELLZ000000mmmS22pmmmmmmaaaab


Chunk 988:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: RRRRRRyyyVNNUyyyyyy000ELLZ000000mmmS22pmmmmmmaaaabb9aaaaaaeeerZZyeeeeee888HKKb888888yyy6XXJyyyyyyyyyKttSyyyyyyUUUCJJ1UUUUUUttt0ZZpttttttaaaVMMLaaaaaaSSSoAApSSSSSS6665RRo666666aaawXXAaaaaaaAAAtEEKAAAAAACCCdmmYCCCCCCmmmmHHsmmmmmmLLLaEEwLLLLLLMMMJcc0MMMMMMPPPNVVjPPPPPPIIICTTiIIIIII444hCCq444444qqqIxxYqqqqqqWWWzRRWWWWWWWFFFRAAaFFFFFFiiixOOKiiiiiiiiiwnnBiiiiiigggYggUggggggVVVarr8VVVVVVPPPSllFPPPPPPBBBbWWcBBBBBBXXXlUUJXXXXXXDDDYhh6DDDDDDeeekttkeeeeeepppJWWNppppppDDDNEEqDDDDDDaaa1TTwaaaaaasssyooissssssIIIQlldIIIII


Chunk 989:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: DDNEEqDDDDDDaaa1TTwaaaaaasssyooissssssIIIQlldIIIIIIXXXaZZkXXXXXXZZZ8221ZZZZZZJJJZzzMJJJJJJjjj3ss2jjjjjjNNNHqq9NNNNNNvvvYSSVvvvvvvdddSKKQddddddUUUbSSRUUUUUUEEEGmmyEEEEEEccc2YYcccccccnnn9llznnnnnnMMMDttMMMMMMMwww7IINwwwwwwjjj7nnpjjjjjjaaaoPPEaaaaaaRRRd445RRRRRROOOBppvOOOOOOfffu11Offffffzzz3dduzzzzzzeeewyyVeeeeeeKKKMXXHKKKKKKzzzYJJBzzzzzzkkkATTekkkkkkuuu6aaluuuuuuSSSnHH9SSSSSSuuuMwwkuuuuuu222MWWc222222PPPF44BPPPPPPAAAXUUbAAAAAAtttEfftttttttyyyE++QyyyyyyoooI99/ooooootttNYYOttttttzzz3YYyzzzzzzsssAkkysssssssssH00


Chunk 990:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: oooootttNYYOttttttzzz3YYyzzzzzzsssAkkysssssssssH00RsssssskkkDZZxkkkkkkccc9iiiccccccaaaCYYxaaaaaasssBzzkssssssZZZLXX5ZZZZZZOOOgzziOOOOOOYYYhlluYYYYYYrrrIffXrrrrrr111400a111111222B775222222uuuXzzruuuuuuaaae22Haaaaaauuuv553uuuuuu999Yuul999999555nJJL555555aaa3778aaaaaa///533l//////JJJnrrzJJJJJJccc2iiecccccc333sGGt333333rrruYYorrrrrr666qnnT666666HHHp99iHHHHHH4445iiY444444sss6uuWssssssrrr9aaarrrrrrBBBLpp6BBBBBBcccOqqqccccccVVV4YYGVVVVVVTTTIIIpTTTTTTUUU+ooiUUUUUUwww8++iwwwwwwRRRzDDDRRRRRRXXXxSSxXXXXXX9998ppc999999


Chunk 991:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: w8++iwwwwwwRRRzDDDRRRRRRXXXxSSxXXXXXX9998ppc999999fff4qqVffffffVVVxggtVVVVVVNNNIaacNNNNNNQQQoHHBQQQQQQCCC4EEQCCCCCCQQQ<884QQQQQQ444///m444444nnnlxxXnnnnnnXXXaxx+XXXXXX+++tzzO++++++eeeejjOeeeeeeOOOxWWeOOOOOOcccijjaccccccaaatIIUaaaaaa000>GGR000000ZZZaaAZZZZZZBBBeeLBBBBBBrrrEETrrrrrrDDDKKwDDDDDDwwwqqjwwwwwwhhh55VhhhhhhVVV333VVVVVVHHHxxOHHHHHHOOOXX+OOOOOO///TTK//////KKKcc6KKKKKK666++R666666YYYLLnYYYYYYnnnTTxnnnnnnxxxBBaxxxxxxKKKvvYKKKKKKccc99PccccccNNNLL/NNNNNN///LLp//////pppRRYppppppeee994eeeeee666CCP666666PPP


Chunk 992:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: //LLp//////pppRRYppppppeee994eeeeee666CCP666666PPP//IPPPPPPoooPPXooooooTTTrr7TTTTTT444JJ8444444777bb5777777ppp88KppppppPPPnndPPPPPPvvvookvvvvvvkkkVV8kkkkkk999zzm999999mmm++UmmmmmmnnnOOYnnnnnnUUU++RUUUUUURRRvv8RRRRRRyyyGGPyyyyyyNNNff4NNNNNN444CCR444444uuuKK9uuuuuu+++22Q++++++fffgg7ffffff+++bbP+++++++++++G++++++OOOggrOOOOOODDDHHjDDDDDDNNN66WNNNNNNoooqqEoooooo444IIP444444OOOYYvOOOOOOeee//0eeeeeeoooUUCooooooXXXaafXXXXXXeeeHHUeeeeeeooo66RooooooQQQBBzQQQQQQGGGggHGGGGGGKKKNNaKKKKKK00000R000000RRRBB4RRRRRR444BBf444


Chunk 993:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: GGGGGKKKNNaKKKKKK00000R000000RRRBB4RRRRRR444BBf444444aaaRRoaaaaaaoooddOoooooo222BBx22222299999q999999ooosshoooooogggBBMggggggqqqTTaqqqqqqaaavvLaaaaaaIIIBBoIIIIIIooo99Loooooouuu++tuuuuuutttAAgttttttggghhOggggggNNN333NNNNNN333AAg333333ggg55cggggggddd33fddddddvvvnngvvvvvvAAA++vAAAAAAlllIItllllll+++KKw++++++hhhggJhhhhhhSSS00/SSSSSS///VVw//////DDDuuYDDDDDDtttXXttttttt22299t222222GGGEEmGGGGGGbbbLLFbbbbbbXXXhhQXXXXXXRRRJJdRRRRRRiii11viiiiii000HHz000000PPPMMFPPPPPPSSSppvSSSSSS///aa2//////QQQ//FQQQQQQXXXBB8XXXXXXwww


Chunk 994:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 23, 'num_chars': 512}
Text: SSppvSSSSSS///aa2//////QQQ//FQQQQQQXXXBB8XXXXXXwwwwwPwwwwwwggg==Mgggggg///==v//////333<<f333333ggg//zggggggCCCllCCCCCCCAAAaa/AAAAAAccctt4ccccccLLLeeQLLLLLL///xx=//////lllii=llllll<<<tt<<<<<<<///>>///////llllllllllaaaaaaaaaatttttttttteeeeeeeeeexxxxxxxxxxiiiiiiiiiitttttttttt>>>>>>>>>> 2 P ✓ = u , v are all the parameters in this model! k k {{ } { }} <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuuEEEE6666wwwwEEEEgggg++++ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD222277776


Chunk 995:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: +ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD222277776666YYYYVVVV5555++++NNNN9999kkkk===="""">>>>AAAAAAAAAAAACCCCGGGGHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqkkkkssss3333gggg0000VVVVwwwwIIIITTTTUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOnnnnkkkkwwwwcccc1111MMMMooooIIIIZZZZ////hhhhxxxxllll9999xxxx44440000IIIIRRRRtttt999933335555NNNN00007777TTTTggggNNNNpppp6666hhhhooooHHHHDDDDOOOOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC000


Chunk 996:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: OOOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC0000////wwwwyyyySSSSiiiiuuuurrrraaaa++++ssssbbbb5555cccc3333KKKK1111vvvvbbbbOOOO7777llll55551111////6666CCCCttttooookkkkRRRRSSSS1111qqqqKKKKRRRRiiiiGGGGTTTTXXXXIIII4444ooooJJJJHHHHrrrrIIIIWWWWccccBBBBCCCCssssGGGG0000ttttGGGGAAAAkkkk++++wwwwjjjjjjjjeeee6666mmmm////mmmmddddMMMMZZZZOOOOKKKKRRRR++++EEEEjjjjTTTTGGGGLLLLmmmmBBBBGGGGQQQQQQQQccccpppp9999TTTTAAAAllllppppyyyyqqqq++++cccc2222DDDDBBBBkkkkQQQQffffIIIIPPPPttttVVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzR


Chunk 997:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: tVVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzRRRR3333ZZZZ2222hhhhnnnn++++UUUUccccaaaa7777YYYYmmmmVVVVuuuuttttmmmmXXXXUUUUzzzzBBBB11114444mmmmVVVVkkkkFFFFqqqqqqqqEEEEDDDDTTTTrrrrUUUU7777ttttffffkkkkSSSSTTTTggggIIIIVVVVAAAABBBBVVVVGGGGqqqqZZZZ5555kkkkxxxxOOOOCCCCmmmmRRRRwwwwKKKKllllggggWWWWccccVVVVOOOOFFFFIIIIssssJJJJHHHHZZZZEEEEBBBB66662222kkkkaaaakkkkooooAAAAppppJJJJ88880000PPPPyyyy////CCCCJJJJVVVVvvvvrrrrYYYYjjjj6666TTTT++++IIIIeeeeBBBBcccc////dddd2222RRRRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeee


Chunk 998:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: RRRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeeeeaaaattttGGGGbbbbiiiiffff99995555vvvvQQQQTTTT8888aaaayyyyffffllllYYYYZZZZwwwwAAAACCCC++++llll8888kkkkJJJJ8888IIIIDDDDBBBBGGGGeeeeppppYYYYTTTT7777XXXXDDDDIIIIKKKKYYYYqqqqIIIIJJJJooooZZZZLLLLrrrrXXXXTTTTEEEEddddEEEEkkkkkkkkoooo6666CCCCwwwwrrrrOOOOggggRRRRrrrr8888eeeeRRRRllll0000rrrr6666ooooWWWW2222bbbbddddeeeerrrriiiissssNNNNWWWW6666LLLLOOOOMMMMrrrrooooCCCCBBBB2222jjjjUUUU2222SSSShhhhKKKK9999RRRRAAAA99996666iiiiJJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC7


Chunk 999:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 32, 'num_chars': 512}
Text: iJJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC77778888WWWWyyyy8888GGGGhhhh////GGGG55557777yyyy0000ZZZZBBBBQQQQ9999hhhh++++ggggPPPPjjjjOOOOkkkk3333uuuueeee6666gggg1111wwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Q: Why two sets of vectors? Any issues?
How to train the model Calculating all the gradients together! ✓ = u , v k k {{ } { }} <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuuEEEE6666wwwwEEEEgggg++++ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD2222777766


Chunk 1000:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD222277776666YYYYVVVV5555++++NNNN9999kkkk===="""">>>>AAAAAAAAAAAACCCCGGGGHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqkkkkssss3333gggg0000VVVVwwwwIIIITTTTUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOnnnnkkkkwwwwcccc1111MMMMooooIIIIZZZZ////hhhhxxxxllll9999xxxx44440000IIIIRRRRtttt999933335555NNNN00007777TTTTggggNNNNpppp6666hhhhooooHHHHDDDDOOOOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC0000


Chunk 1001:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: OOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC0000////wwwwyyyySSSSiiiiuuuurrrraaaa++++ssssbbbb5555cccc3333KKKK1111vvvvbbbbOOOO7777llll55551111////6666CCCCttttooookkkkRRRRSSSS1111qqqqKKKKRRRRiiiiGGGGTTTTXXXXIIII4444ooooJJJJHHHHrrrrIIIIWWWWccccBBBBCCCCssssGGGG0000ttttGGGGAAAAkkkk++++wwwwjjjjjjjjeeee6666mmmm////mmmmddddMMMMZZZZOOOOKKKKRRRR++++EEEEjjjjTTTTGGGGLLLLmmmmBBBBGGGGQQQQQQQQccccpppp9999TTTTAAAAllllppppyyyyqqqq++++cccc2222DDDDBBBBkkkkQQQQffffIIIIPPPPttttVVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzRR


Chunk 1002:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: VVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzRRRR3333ZZZZ2222hhhhnnnn++++UUUUccccaaaa7777YYYYmmmmVVVVuuuuttttmmmmXXXXUUUUzzzzBBBB11114444mmmmVVVVkkkkFFFFqqqqqqqqEEEEDDDDTTTTrrrrUUUU7777ttttffffkkkkSSSSTTTTggggIIIIVVVVAAAABBBBVVVVGGGGqqqqZZZZ5555kkkkxxxxOOOOCCCCmmmmRRRRwwwwKKKKllllggggWWWWccccVVVVOOOOFFFFIIIIssssJJJJHHHHZZZZEEEEBBBB66662222kkkkaaaakkkkooooAAAAppppJJJJ88880000PPPPyyyy////CCCCJJJJVVVVvvvvrrrrYYYYjjjj6666TTTT++++IIIIeeeeBBBBcccc////dddd2222RRRRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeeee


Chunk 1003:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: RRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeeeeaaaattttGGGGbbbbiiiiffff99995555vvvvQQQQTTTT8888aaaayyyyffffllllYYYYZZZZwwwwAAAACCCC++++llll8888kkkkJJJJ8888IIIIDDDDBBBBGGGGeeeeppppYYYYTTTT7777XXXXDDDDIIIIKKKKYYYYqqqqIIIIJJJJooooZZZZLLLLrrrrXXXXTTTTEEEEddddEEEEkkkkkkkkoooo6666CCCCwwwwrrrrOOOOggggRRRRrrrr8888eeeeRRRRllll0000rrrr6666ooooWWWW2222bbbbddddeeeerrrriiiissssNNNNWWWW6666LLLLOOOOMMMMrrrrooooCCCCBBBB2222jjjjUUUU2222SSSShhhhKKKK9999RRRRAAAA99996666iiiiJJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC77


Chunk 1004:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 38, 'num_chars': 512}
Text: JJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC77778888WWWWyyyy8888GGGGhhhh////GGGG55557777yyyy0000ZZZZBBBBQQQQ9999hhhh++++ggggPPPPjjjjOOOOkkkk3333uuuueeee6666gggg1111wwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> T T 1 11 1 J(✓) = logJ((✓✓)) == log (✓) = log P (w w l;o✓g)P (w J(✓w) ;=✓)? t+j t t+j t ✓   T L   TT L   T | | r <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ooooFFFFttttCCCCjjjj5555NNNNEEEE4444VVVVIIIIaaaa6666vvvvccccNNNNKKKKQQQQllllllllyyyy3333hhhhb


Chunk 1005:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: Iaaaa6666vvvvccccNNNNKKKKQQQQllllllllyyyy3333hhhhbbbbvvvvttttMMMM===="""">>>>AAAAAAAAAAAACCCCAAAA3333iiiiccccbbbbZZZZDDDDLLLLSSSSssssNNNNAAAAFFFFIIIIYYYYnnnnXXXXmmmmuuuu9999VVVVdddd3333ppppJJJJlllliiiiEEEEuuuuiiiimmmmJJJJCCCCLLLLooooRRRRiiii22227777EEEEVVVVQQQQVVVV7777ggggSSSSaaaaUUUUkkkk++++mmmmkkkkHHHHTTTTqqqqZZZZhhhhJJJJkkkkTTTTooooYYYYSSSSCCCCGGGG1111////FFFFjjjjQQQQttttFFFF3333PPPPooooSSSS7777nnnnwwwwbbbbpppp5555eeeeFFFFttttvvvv4444wwww8888PPPPGGGGffffcccczzzzhhhhzzzz////iiiiAAAARRRRXXXXKKKKPPPPjjjjfff


Chunk 1006:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ccczzzzhhhhzzzz////iiiiAAAARRRRXXXXKKKKPPPPjjjjffffFFFFssssLLLLiiii0000vvvvLLLLKKKK6666uuuu5555ttttffffzzzz6666xxxxuuuubbbbWWWWddddmmmmFFFFnnnntttt66667777jjjjVVVVFFFFFFFFWWWWoooo7777GGGGIIIIVVVVTTTTMMMMAAAAzzzzQQQQSSSSXXXXrrrrIIIIYYYYccccBBBBWWWWssssmmmmiiiikkkkEEEEUUUUCCCCNNNNYYYYIIII++++tttteeeejjjjeeeeuuuuOOOOBBBBKKKKcccc1111jjjjeeeeYYYY++++DDDDhhhhPPPPkkkkRRRRddddCCCCUUUUPPPPOOOOQQQQUUUU0000VVVVrrrruuuuwwww77770000kkkkIIIIBBBBLLLLQQQQzzzzDDDD3333ssssMMMMYYYYXXXXhhhhbbbbmmmmssssDDDDxxxxxxxxWWWWWWWW7777U


Chunk 1007:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: MYYYYXXXXhhhhbbbbmmmmssssDDDDxxxxxxxxWWWWWWWW7777UUUUHHHHTTTTKKKKzzzzlllljjjj2222PPPPLLLLhhhhTTTTKKKKJJJJKKKKppppqqqquuuu3333CCCCllll9999eeeeJJJJaaaaRRRRooooxxxxiiiiVVVVSSSSAAAA1111iiii3333XXXXSSSSddddDDDDPPPPQQQQCCCCGGGGnnnngggggggg3333zzzzXXXXqqqqppppZZZZAAAArrrrQQQQPPPPXXXXddddYYYYyyyyKKKKCCCCFFFFiiii2222ssss////GGGGNNNNwwwwzzzzttttIIII++++NNNN00007777DDDDBBBBWWWW5555kkkkmmmm0000xxxx++++7777vvvviiiiQQQQwwwwiiiirrrrQQQQddddRRRRYYYYDDDDoooojjjjwwwwJJJJ6666eeeerrrrYYYY3333MMMM////2222qqqqttttFFFFMMMMNNNNzzz


Chunk 1008:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: eeerrrrYYYY3333MMMM////2222qqqqttttFFFFMMMMNNNNzzzzPPPP++++MMMMyyyySSSSZZZZFFFFJJJJOOOOllllkkkkUUUUppppssssLLLLGGGG2222BBBB4444FFFFYYYYnnnneeee4444YYYYhhhhTTTTFFFFwwwwAAAABBBBQQQQxxxxcccc1111ffffbbbbddddooooDDDDBBBBRRRRRRRRNNNNbbbbHHHHkkkkTTTTggggjjjjtttt77778888jjjjzzzzUUUUTTTT8888qqqquuuuUUUU3333bbbbvvvvTTTToooouuuuVVVVqqqq2222kkkkccccOOOOXXXXJJJJAAAADDDDkkkkmmmmJJJJuuuuOOOOSSSSMMMMVVVVMMMMggggNNNNqqqqZZZZIIIIaaaaooooeeeeSSSSRRRRPPPPJJJJNNNNXXXX8888mmmmYYYY9999WWWWSSSS////WWWWuuuu////UUUUxxxxaaaaVVVV2222w


Chunk 1009:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 22, 'num_chars': 512}
Text: Y9999WWWWSSSS////WWWWuuuu////UUUUxxxxaaaaVVVV2222wwwwppppjjjjNNNN77775555IIII++++sssszzzzxxxx9999vvvvaaaappppddddbbbb<<<<////llllaaaatttteeeexxxxiiiitttt>>>> t=1 m j m,j=0t=1 m j m,j=0 X X X X     6     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>AAAAAAAAAAAACCCCeeee3333iiiiccccddddVVVVFFFFddddaaaaxxxxQQQQxxxxFFFFMMMM2222MMMMWWWWuuuuuuuuqqqq77


Chunk 1010:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: aaaaxxxxQQQQxxxxFFFFMMMM2222MMMMWWWWuuuuuuuuqqqq7777aaaaqqqqPPPPggggggggQQQQXXXXccccaaaa3333ttttMMMMiiiiOOOOKKKKgggghhhhSSSSKKKKvvvvoooojjjj4444ssssEEEEKKKK3333LLLLeeeexxxxssssllll0000zzzz2222zzzzmmmm7777aaaaJJJJDDDDMMMMmmmmdddd5555QQQQllll5555EEEE////444400003333zzzzzzzznnnn////ggggiiiimmmmNNNNkkkkddddRRRRFFFFuuuu9999EEEEHHHHLLLLuuuuuuuuVVVV////JJJJuuuuXXXXkkkkllllhhhhccccUUUUkkkk++++RRRR7777FFFFVVVV66665555eeee22227777iiii++++eeeeaaaaNNNNzzzz88889999bbbbttttrrrreeee3333uuuunnnnbbbbttttHHHHttttqqqqwwwwNNNN


Chunk 1011:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ttrrrreeee3333uuuunnnnbbbbttttHHHHttttqqqqwwwwNNNNhhhhxxxxEEEEvvvvZZZZWWWWllllOOOOccccmmmmZZZZBBBBCCCCgggg0000jjjjFFFFCCCCjjjjhhhhppppDDDDLLLLAAAAVVVVCCCC7777hhhhOOOODDDD9999////22228888SSSSPPPPPPPP4444OOOOxxxxoooottttSSSSHHHHuuuuKKKKxxxxggggooootttthhhhcccciiii0000JJJJwwwwhhhhooooGGGGaaaaddddrrrr++++++++77772222eeee4444AAAAGGGGRRRRPPPP6666DDDD7777ddddyyyywwwwrrrrDDDDuuuuEEEEuuuu9999OOOO////SSSSZZZZLLLLOOOOcccc0000UUUUwwwwwwwwXXXXnnnnEEEEnnnn3333wwwwffff8888vvvvzzzzddddZZZZqqqq6666nnnnAAAA////9999aaaaeeee//


Chunk 1012:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: vvvvzzzzddddZZZZqqqq6666nnnnAAAA////9999aaaaeeee////vvvvTTTT1111FFFFMMMMwwwwmmmmffff6666NNNNnnnn6666UUUUrrrrssssNNNN0000ggggEEEEllll3333qqqq3333aaaaDDDDvvvvttttffffQQQQssss3333TTTTMMMMxxxx8888GGGGiiiiBBBBllllttttHHHHPPPP++++aaaattttggggPPPP8888ttttNNNNttttLLLLBBBBssssnnnnKKKK6666GGGGWWWWQQQQttttqqqqBBBBHHHHWWWWhhhhttttOOOOuuuu9999++++yyyyWWWWccccllllrrrrBBBBRRRRqqqq5555ZZZZNNNNaaaaOOOO00006666TTTTCCCCiiiiWWWWMMMMGGGGBBBBZZZZffffggggOOOO1111llllttttooooWWWWLLLL8888nnnnMMMM1111hhhhHHHHKKKKBBBBmmmmCCCCuuuuzzzz


Chunk 1013:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: LL8888nnnnMMMM1111hhhhHHHHKKKKBBBBmmmmCCCCuuuuzzzzEEEErrrrbbbbTTTTzzzz9999FFFFFFFFggggZZZZrrrrQQQQooooTTTTTTTTggggaaaa6666YYYYrrrr9999ssss8888IIIIxxxxZZZZeeee1111SSSS5555SSSSGGGGzzzzEEEEccccNNNNeeeejjjjDDDDXXXXkkkkvvvv2222LLLLjjjjGGGGoooottttXXXXEEEEyyyydddd0000VVVVSSSSNNNNoooovvvvhhhh5555UUUU1111JJJJJJJJiiiiSSSSZZZZttttFFFF0000JJJJkkkkwwwwwwwwFFFFEEEEuuuuAAAA2222DDDDcccciiiiPPPPBBBBWWWWyyyyhhhhccccssssCCCCIIIItttthhhhXXXXZZZZ0000ggggQQQQnnnnrrrrxxxxyyyy5555ffffBBBB0000bbbbNNNNBBBBmmmmggggzzzzSSSSjjjj888899


Chunk 1014:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 3, 'num_chars': 512}
Text: ffffBBBB0000bbbbNNNNBBBBmmmmggggzzzzSSSSjjjj888899997777BBBB22229999aaaaOOOOTTTTbbbbJJJJffffffffKKKKQQQQ9999EEEEllllKKKKXXXXppppIIIIDDDD8888oooo4444MMMMyyyyYYYYhhhhwwww8888iiiiNNNN6666EEEEDDDD2222OOOO++++ttttHHHHPPPPuuuuBBBBffffvvvvxxxxLLLLvvvvrrrr1111DDDDhhhhqqqqaaaa++++6666RRRRvvvvyyyyxxxx++++8888QQQQttttAAAA7777bbbb8888++++<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrpp


Chunk 1015:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>AAAAAAAAAAAACCCCeeee3333iiiiccccddddVVVVFFFFddddaaaaxxxxQQQQxxxxFFFFMMMM2222MMMMWWWWuuuuuuuuqqqq7777aaaaqqqqPPPPggggggggQQQQXXXXccccaaaa3333ttttMMMMiiiiOOOOKKKKgggghhhhSSSSKKKKvvvvoooojjjj4444ssssEEEEKKKK3333LLLLeeeexxxxssssllll0000zzzz2222zzzzmmmm7777aaaaJJJJDDDDMMMMmmmmdddd5555QQQQllll5555EEEE////444400003333zzzzzzzznnnn////ggggiiiimmmmNNNNkkkkddddRRRRFFFFuuuu9999EEEEHHHHLLLLuuuuuuuuVVVV////JJJJ


Chunk 1016:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ddRRRRFFFFuuuu9999EEEEHHHHLLLLuuuuuuuuVVVV////JJJJuuuuXXXXkkkkllllhhhhccccUUUUkkkk++++RRRR7777FFFFVVVV66665555eeee22227777iiii++++eeeeaaaaNNNNzzzz88889999bbbbttttrrrreeee3333uuuunnnnbbbbttttHHHHttttqqqqwwwwNNNNhhhhxxxxEEEEvvvvZZZZWWWWllllOOOOccccmmmmZZZZBBBBCCCCgggg0000jjjjFFFFCCCCjjjjhhhhppppDDDDLLLLAAAAVVVVCCCC7777hhhhOOOODDDD9999////22228888SSSSPPPPPPPP4444OOOOxxxxoooottttSSSSHHHHuuuuKKKKxxxxggggooootttthhhhcccciiii0000JJJJwwwwhhhhooooGGGGaaaaddddrrrr++++++++77772222eeee4444AAAAGGGGRRRRPPPP6666DDDD7777dd


Chunk 1017:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ++++77772222eeee4444AAAAGGGGRRRRPPPP6666DDDD7777ddddyyyywwwwrrrrDDDDuuuuEEEEuuuu9999OOOO////SSSSZZZZLLLLOOOOcccc0000UUUUwwwwwwwwXXXXnnnnEEEEnnnn3333wwwwffff8888vvvvzzzzddddZZZZqqqq6666nnnnAAAA////9999aaaaeeee////vvvvTTTT1111FFFFMMMMwwwwmmmmffff6666NNNNnnnn6666UUUUrrrrssssNNNN0000ggggEEEEllll3333qqqq3333aaaaDDDDvvvvttttffffQQQQssss3333TTTTMMMMxxxx8888GGGGiiiiBBBBllllttttHHHHPPPP++++aaaattttggggPPPP8888ttttNNNNttttLLLLBBBBssssnnnnKKKK6666GGGGWWWWQQQQttttqqqqBBBBHHHHWWWWhhhhttttOOOOuuuu9999++++yyyyWWWWccccllll


Chunk 1018:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: BBHHHHWWWWhhhhttttOOOOuuuu9999++++yyyyWWWWccccllllrrrrBBBBRRRRqqqq5555ZZZZNNNNaaaaOOOO00006666TTTTCCCCiiiiWWWWMMMMGGGGBBBBZZZZffffggggOOOO1111llllttttooooWWWWLLLL8888nnnnMMMM1111hhhhHHHHKKKKBBBBmmmmCCCCuuuuzzzzEEEErrrrbbbbTTTTzzzz9999FFFFFFFFggggZZZZrrrrQQQQooooTTTTTTTTggggaaaa6666YYYYrrrr9999ssss8888IIIIxxxxZZZZeeee1111SSSS5555SSSSGGGGzzzzEEEEccccNNNNeeeejjjjDDDDXXXXkkkkvvvv2222LLLLjjjjGGGGoooottttXXXXEEEEyyyydddd0000VVVVSSSSNNNNoooovvvvhhhh5555UUUU1111JJJJJJJJiiiiSSSSZZZZttttFFFF0000JJJJkkkkwwwwwwwwFFFFEE


Chunk 1019:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: JJJJiiiiSSSSZZZZttttFFFF0000JJJJkkkkwwwwwwwwFFFFEEEEuuuuAAAA2222DDDDcccciiiiPPPPBBBBWWWWyyyyhhhhccccssssCCCCIIIItttthhhhXXXXZZZZ0000ggggQQQQnnnnrrrrxxxxyyyy5555ffffBBBB0000bbbbNNNNBBBBmmmmggggzzzzSSSSjjjj888899997777BBBB22229999aaaaOOOOTTTTbbbbJJJJffffffffKKKKQQQQ9999EEEEllllKKKKXXXXppppIIIIDDDD8888oooo4444MMMMyyyyYYYYhhhhwwww8888iiiiNNNN6666EEEEDDDD2222OOOO++++ttttHHHHPPPPuuuuBBBBffffvvvvxxxxLLLLvvvvrrrr1111DDDDhhhhqqqqaaaa++++6666RRRRvvvvyyyyxxxx++++8888QQQQttttAAAA7777bbbb8888++++<<<<////llllaaaatttteeee


Chunk 1020:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 28, 'num_chars': 512}
Text: QQttttAAAA7777bbbb8888++++<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Q: How many parameters are in total? We can apply stochastic gradient descent (SGD)! (t+1) (t) ✓ = ✓ ⌘ J (✓) ✓   r <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""2222xxxxbbbbrrrrEEEEJJJJRRRR++++XXXXVVVVhhhhUUUUccccyyyyssssjjjjVVVVyyyyGGGGPPPPSSSSHHHHiiiicccc0000HHHHYYYY===="""">>>>AAAAAAAAAAAACCCCJJJJnnnniiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEJJJJzzzz1111GGGGeeeeMMMMrrrr6666ttttHHHHLLLLYYYYBBBBAAAAi


Chunk 1021:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: z1111GGGGeeeeMMMMrrrr6666ttttHHHHLLLLYYYYBBBBAAAAiiiiYYYYttttggggVVVVQQQQSSSS++++CCCC6666EEEEUUUU8888RRRRTTTTBBBBRRRRyyyyKKKK5555LLLL77772222RRRRiiiihhhhsssszzzzOOOOLLLLjjjjOOOO9999QQQQlllljjjjyyyyNNNNVVVV77778888FFFFSSSS8888eeeeIIIIiiiiLLLLeeee////BBBBQQQQnnnnDDDD////BBBBZZZZMMMMFFFFBBBBdddd1111UUUU1111PPPPVVVV5555RRRRKKKKYYYYddddBBBB1111333355552222pppp6666ZZZZnnnnZZZZuuuuffffnnnnCCCCQQQQnnnnFFFFxxxxaaaaXXXXllllllllttttbbbbSSSS22223333jjjjBBBBJJJJpppphhhhmmmmvvvvssss0000QQQQmmmm++++iiiiYYYYCCCCwwww6666VVV


Chunk 1022:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: mmmvvvvssss0000QQQQmmmm++++iiiiYYYYCCCCwwww6666VVVVQQQQvvvvIIII4444CCCCJJJJbbbb9999JJJJNNNNYYYYcccc4444kkkkvvvvwwww66666666pppp4444NNNN////eeeetttt7777rrrroooo1111IIII1111BBBBXXXX2222UUUUhhhh7777EEEEccccKKKKddddEEEEWWWWzzzzBBBBAAAAKKKK4444WWWWllllYYYYxxxx88887777HHHHOOOOEEEE2222rrrr++++CCCCuuuutttt9999OOOOnnnnxxxx////RRRRLLLLssssOOOOUUUUeeee9999WWWW1111BBBBffffQQQQWWWWRRRRhhhhDDDDAAAAffffeeee////2222LLLLyyyyppppjjjjsssshhhhKKKKWWWWyyyyWWWW3333VVVVHHHHooooHHHH++++JJJJNNNNyyyyFFFFllllMMMMkkkkEEEEttttLLLLAAAA3


Chunk 1023:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: H++++JJJJNNNNyyyyFFFFllllMMMMkkkkEEEEttttLLLLAAAA33338888VVVVssssKKKKyyyymmmmCCCCttttkkkkEEEEooooxxxxppppeeeemmmm6666KKKKQQQQQQQQ4444aaaaBBBBZZZZOOOO8888XXXX////QQQQzzzzwwww1111NNNNggggXXXXbbbbjjjjjjjjTTTTUUUUssssVVVVxxxxNNNNwwwwEEEE++++eeeejjjjMMMMPPPPtttt22222222SSSSoooouuuu2222EEEE22222222ffffQQQQjjjjppppSSSSvvvv0000////kkkkEEEEBBBBvvvvTTTTiiiiyyyyPPPPbbbbGGGGQQQQNNNN2222zzzzGGGG9999vvvvKKKKPPPP7777nnnnNNNNTTTTNNNNssssHHHHwwwwWWWW5555UUUUGGGGmmmmGGGGXXXXLLLLHHHHxxxxoooonnnnYYYYmmmmKKKKSSSSZZZZ0000mmmmBBB


Chunk 1024:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: LLLHHHHxxxxoooonnnnYYYYmmmmKKKKSSSSZZZZ0000mmmmBBBBllllttttCCCCcccc0000ZZZZyyyypppp4444llllwwwwLLLLSSSSwwwwffff6666WWWWssssAAAAxxxxooooYYYY2222mmmmSSSSLLLLNNNNggggTTTTvvvv99998888llll////SSSSWWWWOOOO////6666rrrrllllVVVV7777////KKKKggggffffHHHHIIII6666iiiiaaaaNNNNAAAANNNNsssskkkkWWWWqqqqRRRRCCCCPPPPHHHHJJJJIIIITTTTcccckkkk5555qqqqppppEEEE4444YYYYeeeeSSSSBBBBPPPPZZZZEEEEBBBBeeeennnnEEEEffffnnnn2222XXXXllll11113333ssssaaaattttUUUU88885555kkkkZZZZooooPPPP8888ggggPPPPPPPPxxxxCCCCRRRRNNNNyyyyppppFFFFMMMM====<<<</


Chunk 1025:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 3, 'num_chars': 512}
Text: gPPPPPPPPxxxxCCCCRRRRNNNNyyyyppppFFFFMMMM====<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
<latexit sha1_base64="5JOEV/IhfbnbaD9GxpGyWmwbF9o=">AAACM3icbVDLSsNAFJ34rPVVdelmsAgVtCRS0E2hqAtpNxWsCk0Nk+nEDp1JwsyNUEL+yY0/4kIQF4q49R+c1ix8HRg4nHMud+7xY8E12PaTNTU9Mzs3X1goLi4tr6yW1tYvdJQoyjo0EpG68olmgoesAxwEu4oVI9IX7NIfHo/9y1umNI/CcxjFrCfJTcgDTgkYySs1mxUXBgzIDq7jPdyunNQd7Erex7BLd4ziBorQ1MnSVoZdnUgv5XUnu25NonYe9bgJe6WyXbUnwH+Jk5MyytH2Sg9uP6KJZCFQQbTuOnYMvZQo4FSwrOgmmsWEDskN6xoaEsl0L53cnOFto/RxECnzQsAT9ftESqTWI+mbpCQw0L+9sf


Chunk 1026:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 29, 'num_chars': 512}
Text: 6xoaEsl0L53cnOFto/RxECnzQsAT9ftESqTWI+mbpCQw0L+9sfif100gOOylPIwTYCH9WhQkAkOExwXiPleMghgZQqji5q+YDogpCUzNRVOC8/vkv+Riv+rUqrWzWrlxlNdRQJtoC1WQgw5QA52iNuogiu7QI3pBr9a99Wy9We9f0Skrn9lAP2B9fAKhIqYl</latexit> Skip-gram with negative sampling (SGNS) Idea: recast problem as binary classification! P (D = 1 t, c) =  (u v ) t c | · • <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""++++eeeeQQQQ6666DDDDddddAAAAqqqqXXXXMMMMFFFFHHHHXXXX0000OOOOwwwwYYYYllllYYYYQQQQ5555TTTTwwww9999TTTT22


Chunk 1027:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 0000OOOOwwwwYYYYllllYYYYQQQQ5555TTTTwwww9999TTTT22224444===="""">>>>AAAAAAAAAAAACCCCJJJJXXXXiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ33334444ttttrrrr6666iiiiLLLLtttt0000MMMMFFFFqqqqGGGGCCCCllllEEEEQQQQEEEEXXXXSSSSiiiiIIIIuuuunnnnBBBBZZZZwwwwVVVVaaaahhhhCCCCWWWWEEEEyyyymmmmbbbbRRRRDDDDZZZZ5555IIIIwwwwcccc1111MMMMooooooooTTTT////jjjjxxxxllll9999xxxx44448888IIIIiiiiggggiiiitttt////xxxxWWWWmmmmbbbbhhhhVVVVooooPPPPzzzzHHHHDDDDmmmmnnnnHHHHuuuuZZZZeeee0000++++YYYYCCCCaaaa7777BBBBccccTTTT6666ttttuuuuffffmmmm


Chunk 1028:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ++YYYYCCCCaaaa7777BBBBccccTTTT6666ttttuuuuffffmmmmFFFFxxxxaaaaXXXXllllllllddddXXXXKKKK2222vvvvrrrrGGGG5555ppppaaaa9999vvvvddddPPPPSSSSaaaaaaaa4444ooooaaaa9999JJJJUUUUppppOOOOooooxxxxJJJJJJJJooooJJJJnnnnrrrrAAAAmmmmccccBBBBDDDDssssMMMMVVVVOOOOMMMMyyyyFFFFCCCCwwwwhhhh7777BBBB3333PPPPffffYYYYffff++++kkkkxxxxppppnnnniiiibbbb3333MMMMMMMMiiiiYYYYLLLL0000kkkknnnn4444TTTTGGGGnnnnBBBBIIIIwwwwUUUU2222OOOOeeeeNNNN2222gggg2222++++wwwwCCCC77772222JJJJIIII8888wwwwHHHHGGGGFFFF6666aaaaJJJJ6666eeee5555hhhh1111JJJJaaaapppp44


Chunk 1029:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: FFFF6666aaaaJJJJ6666eeee5555hhhh1111JJJJaaaapppp4444kkkk0000AAAA3333jjjjIIIIhhhh8888GGGGggggDDDD0000aaaappppeeeeYYYYuuuuppppffff4444wwwwooooIIIIeeeeBBBBXXXXXXXXXXXXqqqqzzzzggggRRRR4444llllrrrrggggllllqqqqaaaaIIIISSSSjjjjccccAAAAeeeeeeeeVVVVFFFFKKKKcccc8888kkkkSSSSooooIIIIJJJJoooo3333XXXXaaaaddddDDDDPPPPyyyyCCCCKKKKOOOOBBBBUUUUssssGGGGHHHHFFFFyyyyzzzzXXXXLLLLCCCCOOOO2222RRRRDDDDmmmmssssbbbbmmmmhhhhDDDDJJJJttttFFFF9999MMMMtttthhhhzzzziiiiAAAA6666NNNNEEEEOOOOEEEE6666VVVVOOOOQQQQnnnnggggiiiiffffqqqqzzzzooooyyyy


Chunk 1030:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: EE6666VVVVOOOOQQQQnnnnggggiiiiffffqqqqzzzzooooyyyyBBBBSSSS66664444EEEEMMMMTTTTeeeeVVVV4444SSSSPPPP3333XXXXGGGG4444vvvv////eeeeeeee0000cccc4444jjjjOOOO////4444EEEEmmmmWWWWAAAA0000vvvvoooo9999KKKKMMMM4444FFFFxxxxhhhhSSSSPPPPIIII4444MMMMRRRR1111wwwwxxxxCCCCmmmmJJJJggggCCCCKKKKGGGGKKKKmmmm1111kkkkxxxx7777RRRRJJJJFFFFKKKKJJJJhhhhggggKKKKyyyyYYYYEEEE9999++++////KKKKssss6666RRRR1111XXXXHHHHeeeedddduuuunnnntttt3333UUUUrrrr22228888KKKKuuuuNNNNYYYYQQQQXXXXttttooooHHHH9999WWWWQQQQiiii00007777RRRRJJJJbbbbppppFFFFDDDDdd


Chunk 1031:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 24, 'num_chars': 512}
Text: 9999WWWWQQQQiiii00007777RRRRJJJJbbbbppppFFFFDDDDddddRRRREEEEFFFFDDDD2222hhhhFFFF////SSSSGGGGRRRRttttaaaazzzz9999WWWWqqqq9999WWWWxxxx////TTTT0000jjjjmmmmrrrr7777NNNNllllFFFFvvvv2222BBBB9999ffffQQQQPPPPKKKKiiiiqqqqMMMMcccc<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Target word is positive example 1 • All words not in context are negative  (x) = 1 + exp( x)   <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""QQQQvvvv4444DDDDTTTTdddd6666PPPP1111PPPPmmmmvvvvwwww3333zzzzCCCC7777YYY


Chunk 1032:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ddd6666PPPP1111PPPPmmmmvvvvwwww3333zzzzCCCC7777YYYY////ccccLLLLIIIIeeeekkkkIIIIGGGGAAAA===="""">>>>AAAAAAAAAAAACCCCCCCC3333iiiiccccbbbbVVVVDDDDLLLLSSSSggggMMMMxxxxFFFFMMMM3333UUUUVVVV66662222vvvvUUUUZZZZdddduuuuQQQQoooovvvvQQQQIIIIppppaaaaJJJJCCCCLLLLooooRRRRiiiimmmm5555ccccVVVVrrrrAAAAPPPP6666AAAAwwwwllllkkkk2222bbbbaaaa0000GGGGRRRRmmmmSSSSDDDDLLLLSSSSMMMMnnnnTTTTvvvvxxxxllll9999xxxx44440000IIIIRRRRtttt////6666AAAAOOOO////////GGGGttttJJJJ2222FFFFtttthhhh66664444ccccDDDDjjjjnnnnXXXXuuuu66669999xxxx444488885


Chunk 1033:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 64444ccccDDDDjjjjnnnnXXXXuuuu66669999xxxx444488885555UUUU9999ppppxxxxvvvvqqqq3333ccccyyyyuuuurrrraaaa++++kkkkZZZZ++++ssss7777CCCC1111vvvvbbbbOOOO7777ZZZZ++++8888ffffNNNNFFFFWWWWUUUUSSSSEEEEIIIIbbbbJJJJOOOOKKKKRRRRbbbbPPPPttttYYYYUUUUcccc5555CCCC2222ttttBBBBMMMMcccc9999qqqqOOOOJJJJccccXXXXCCCC55557777TTTTllllDDDD2222++++mmmmffffuuuuuuuuBBBBSSSSssssWWWWiiii8888FFFF6666PPPPYYYY++++ooooJJJJ3333AAAA9999ZZZZwwwwAAAAjjjjWWWWRRRRuuuurrrraaaaRRRRVVVVeeeexxxxvvvvssssDDDDllllUUUUQQQQVVVVeeeeQQQQTTTTeeeeQQQQmmmmKKKKRRR


Chunk 1034:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: DDDllllUUUUQQQQVVVVeeeeQQQQTTTTeeeeQQQQmmmmKKKKRRRRooookkkkiiiiJJJJ4444AAAAllll00006666iiiissssuuuunnnnoooo8888qqqqkkkkaaaa5555eeeeccccqqqqjjjjMMMMDDDDXXXXCCCCYYYYooooIIIIyyyyWWWWQQQQoooodddd66661111vvvv9999xxxxeeeeRRRRBBBBJJJJBBBBQQQQ000000004444VVVVqqqqqqqqDDDDnnnnFFFFhhhh7777KKKKZZZZaaaaaaaaEEEEUUUU4444nnnnBBBBTTTTddddRRRRNNNNMMMMZZZZkkkkiiiiPPPPuuuu0000YYYY2222iiiiIIIIBBBBVVVVVVVVeeeeOOOOvvvvttttllllAAAAoooo++++NNNN0000ooooNNNNBBBBJJJJEEEE2222FFFFGGGGssss7777UUUU3333xxxxMMMMppppFFFFkkkkqqqqNNNNhhhhWWWW8


Chunk 1035:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: s7777UUUU3333xxxxMMMMppppFFFFkkkkqqqqNNNNhhhhWWWW88886666BBBBddddYYYYDDDDtttteeeehhhhNNNNxxxxffff++++8888TTTTqqqqKKKKDDDDSSSSyyyy9999llllYYYYZZZZxxxxooooGGGGppppLLLL5555ooooiiiiDDDDhhhhUUUUEEEEddddwwwwGGGGggggzzzzssssMMMMUUUUmmmmJJJJ5555mmmmNNNNDDDDMMMMJJJJHHHHMMMM3333AAAArrrrJJJJAAAAJJJJttttAAAAttttIIIImmmmvvvvYYYYEEEEJJJJAAAAiiiiyyyy8888vvvvkkkk++++ZZZZZZZZFFFFTTTTllllVVVVddddHHHHddddeeeeqqqqllll1111nnnncccceeeeTTTTBBBBEEEESSSSiiiiCCCCMMMMkkkkDDDDggggAAAAttttTTTTAAAALLLLaaaaiiiiDDDDBBBBiiiiDDDDggggEEEETTT


Chunk 1036:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 67, 'num_chars': 512}
Text: tttTTTTAAAALLLLaaaaiiiiDDDDBBBBiiiiDDDDggggEEEETTTTyyyyDDDDVVVV////BBBBmmmmPPPPVVVVkkkkvvvv1111rrrrvvvv1111MMMMWWWW////NNNNWWWWddddnnnnMMMMIIIIffffggggDDDD6666////MMMMHHHH4444ccccOOOOZZZZBBBBgggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> To compute loss, pick K random words as negative examples: K 1 J(✓) = P (D = 1 t, c) P (D = 0 t , c) i   |   K | i=1 X
Continuous Bag of Words (CBOW) T L(✓) = P (w w , m j m, j = 0) t t+j | { }     6 t=1 Y <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaa


Chunk 1037:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 2, 'num_chars': 512}
Text: aatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""3333++++llll6666AAAAbbbbcccc66663333xxxxGGGGDDDDhhhhSSSSVVVVFFFFppppwwwwKKKKTTTTAAAAllllCCCCKKKK8888ffffUUUU===="""">>>>AAAAAAAAAAAACCCCRRRRHHHHiiiiccccbbbbZZZZDDDDLLLLTTTThhhhssssxxxxFFFFIIIIYYYY9999llllFFFFKKKKaaaacccckkkknnnnbbbbZZZZTTTTddddWWWWoooo0000ppppBBBBhhhhWWWWggggGGGGVVVVWWWWoooo3333SSSSKKKKjjjjddddddddNNNNFFFFFFFFkkkkAAAAggggggggxxxxWWWWHHHHkkkk8888ZZZZxxxxJJJJDDDDLLLLZZZZnnnnaaaapppp8888ppppiiiikkkkbbbbzzzzccccGGGGxxxx4


Chunk 1038:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: Znnnnaaaapppp8888ppppiiiikkkkbbbbzzzzccccGGGGxxxx4444AAAAHHHHYYYY8888QQQQTTTTddddddddttttEEEEJJJJssssUUUUZZZZ3333LLLLggggttttuuuuRRRRLLLLHHHH////6666////3333NNNN00007777DDDD8888ppppllllHHHHQQQQYYYYhhhhllllffffBBBBwwwwrrrrPPPPFFFF55550000ssssvvvvllllllll88882222XXXXqqqq2222ssssrrrrqqqq00003333XXXX777788885555ccccHHHHllllppppBBBBffffRRRREEEErrrrnnnnJJJJ7777llllHHHHAAAAHHHHSSSShhhhrrrrooooooooUUUUQQQQFFFFRRRR4444UUUUFFFFrrrrhhhhMMMMFFFFhhhh8888nnnnpppptttt4444llll////++++AAAAuuuusssskkkk7777nnnnZZZZxxxx3333EEE


Chunk 1039:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: lll////++++AAAAuuuusssskkkk7777nnnnZZZZxxxx3333EEEEBBBBAAAA88882222HHHHRRRRmmmmZZZZSSSSccccPPPPRRRRSSSS3333OOOOzzzz////aaaaDDDDMMMMccccAAAAffffIIIINNNNuuuukkkkNNNNZZZZYYYYffffMMMM0000rrrrnnnnAAAAnnnnqqqqoooo++++rrrr////bbbbrrrrqqqqMMMMggggUUUUZZZZttttssss9999iiiippppEEEEzzzzLLLLllllLLLLLLLLqqqqzzzzLLLLssssffffTTTT2222ppppWWWWbbbb9999IIIIttttTTTTbbbb33339999kkkk55557777MMMMLLLLrrrr00005555IIIIeeeeMMMMppppZZZZFFFFYYYYOOOORRRR7777hhhhRRRRxxxx88881111WWWW2222AAAAmmmmnnnnRRRRRRRR9999DDDDNNNNIIIIccccWWWWmmmmVVVVc


Chunk 1040:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: AmmmmnnnnRRRRRRRR9999DDDDNNNNIIIIccccWWWWmmmmVVVVcccc3333bbbbllll6666yyyyNNNNBBBBeeeellllBBBBooooNNNNCCCCcccceeeeffff6666UUUUVVVVjjjjggggooooOOOOIIIIWWWWppppVVVVBBBBQQQQNNNN1111jjjjppppooooOOOODDDDiiiillllAAAA++++hhhh77779999FFFFwwwwDDDDWWWW5555QQQQTTTTUUUUOOOOoooo6666QQQQeeeevvvvppppDDDDTTTTLLLLrrrrTTTT8888GGGG6666VVVVSSSS9999OOOO1111FFFFxxxx7777ddddxxxxYYYYJJJJ77775555TTTTccccxxxxyyyy5555hhhh99995555EEEEffffMMMMrrrrrrrrllll5555hhhh9999GGGGVVVVTTTTSSSSFFFFCCCCWWWWCCCCEEEEbbbbNNNNFFFFWWWWaaaakkkkoooo5555nnn


Chunk 1041:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: CCCWWWWCCCCEEEEbbbbNNNNFFFFWWWWaaaakkkkoooo5555nnnnSSSSSSSSKKKKEEEE2222llllBBBBYYYYFFFFqqqq7777IIIIEEEELLLLKKKK////1111bbbbqqqqRRRRhhhhxxxxyyyywwwwXXXX66663333BBBBssss++++hhhhOOOOjjjjhhhhllllxxxx////DDDDwwwwXXXXYYYYnnnnCCCCjjjjvvvvRRRR3333qqqqffffWWWW7777ttttdddd5555HHHHMMMMvvvvkkkkHHHHXXXXllllPPPP2222iiiiQQQQiiiinnnn8888kkkkuuuu++++UUUU66666666ppppEEEEccccEEEEOOOOSSSSeeee////yyyyVVVV////yyyyLLLL7777ggggIIII////ggggTTTTXXXXwwwwcccc2222ssssddddSSSSGGGGYYYYzzzz7777wwwwllll9999yyyyqqqq4444////QQQQ8888llllTTTTK


Chunk 1042:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 19, 'num_chars': 512}
Text: z7777wwwwllll9999yyyyqqqq4444////QQQQ8888llllTTTTKKKK++++5555<<<<////llllaaaatttteeeexxxxiiiitttt>>>> 1 v ̄ = v t t+j 2m m j m,j=0 X     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuu3333qqqqEEEE2222VVVVmmmmppppSSSSooooWWWWttttPPPPssssbbbbLLLLZZZZccccSSSSmmmm88888888TTTTLLLLffffQQQQ4444===="""">>>>AAAAAAAAAAAACCCCPPPP3333iiiiccccbbbbZZZZBBBBNNNNSSSSwwwwMMMMxxxxEEEEIIIIaaaazzzzffffttttbbbb6666VVVVffffXXXXooooJJJJVVVVggggEEEEQQQQSSSS22227777IIIIuuuuhhhhFFFFKKKKHH


Chunk 1043:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: VVVVggggEEEEQQQQSSSS22227777IIIIuuuuhhhhFFFFKKKKHHHHrrrrxxxxqqqqGGGGCCCCrrrr0000CCCC1111LLLLNNNNssss1111qqqqNNNNMMMMmmmmuuuuyyyyaaaaxxxxQQQQwwwwvvvv4444zzzzLLLL////4444FFFFbbbb111166669999eeeeFFFFDDDDEEEEqqqqzzzzeeeezzzzbbbbQQQQ9999++++DDDDYYYYQQQQ8888vvvvDDDDOOOOTTTTzzzzLLLLxxxxxxxxJJJJrrrrggggBBBB33333333////yyyyxxxxssssYYYYnnnnJJJJqqqqeeeemmmmKKKKzzzzPPPPVVVV2222bbbbnnnn5555hhhhccccXXXXaaaa0000nnnnLLLLbbbbppppLLLLmmmmmmmmrrrrEEEEVVVVTTTTkkkkeeeeqqqqLLLLmmmmBBBBggggmmmmuuuuGGGGIIIItttt4444CCCCDDDDYYYYRRRR


Chunk 1044:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: mmBBBBggggmmmmuuuuGGGGIIIItttt4444CCCCDDDDYYYYRRRRaaaaYYYYZZZZkkkkbbbbFFFFgggg5555////HHHHNNNNUUUUZZZZkkkk////vvvv2222PPPPaaaa8888FFFFSSSSddddQQQQTTTT9999jjjjXXXXUUUUkkkkuuuuFFFFUUUU88884444JJJJeeeeCCCCkkkkqqqqNNNNYYYYOOOOYYYY6666JJJJttttKKKKAAAAllllccccxxxxYYYYmmmm9999KKKK4444ooooIIII8888AAAAEEEEOOOOEEEE00002222ooooDDDDQQQQqqqq7777IIIIwwwwssssbbbbmmmmllllxxxxGGGGddddllllvvvviiiiUUUULLLLBBBBbbbbffffDDDD222288885555FFFFZZZZJJJJyyyyppppFFFFffffffffGGGGuuuuPPPPLLLLGGGGxxxxeeeeFFFF++++6666VVVVWWWWtttt1111vvvv++


Chunk 1045:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: LLLLGGGGxxxxeeeeFFFF++++6666VVVVWWWWtttt1111vvvv++++IIIIPPPPAAAAffffyyyyEEEEYYYYQQQQRRRR2222NNNN4444iiiiSSSSqqqqPPPPYYYYaaaa9999llllOOOOaaaaSSSSKKKKaaaaCCCCCCCCGGGGNNNNMMMMJJJJ////AAAAyyyy6666llllmmmmjjjjggggVVVVLLLLCCCCiiiiGGGGuuuuaaaaGGGGZZZZYYYYTTTTeeeekkkkEEEEvvvvWWWWccccaaaaiiiiIIIIZZZZKKKKZZZZrrrrBBBB////ssssXXXXeeeeNNNN0000ppppPPPPZZZZyyyykkkk2222hhhh0000FFFFeeeeKKKKBBBB++++77777777BBBBEEEEGGGGttttOOOOXXXXssssaaaassssssssJJJJzzzzWWWW////cccc6666XXXX4444XXXX66666666TTTTQQQQ7777LLLLffffttttVVVVxxxxllll


Chunk 1046:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 44XXXX66666666TTTTQQQQ7777LLLLffffttttVVVVxxxxllllOOOOTTTTBBBBFFFFhhhhxxxx8888lllluuuuccccCCCCQQQQ4444ttttJJJJMMMM3333OOOOOOOOaaaaUUUURRRRBBBB9999BBBB4444RRRRqqqq7777mmmmbbbbFFFF9999IIIIoooo444477778888BBBBZZZZXXXXnnnnUUUUmmmmBBBBLLLL9999XXXX////ggggvvvvttttnnnnUUUUbbbbggggNNNN4444LLLLTTTT3333XXXXrrrrzzzzccccGGGGRRRRHHHHBBBBaaaa2222iiiiNNNNbbbbSSSSBBBBAAAArrrrSSSSHHHHmmmmuuuuggggYYYYnnnnaaaaAAAAWWWWoooouuuuggggeeeePPPPaaaaNNNNXXXX9999OOOOYYYY9999eeeeCCCC////eeeeuuuu////ccccxxxxLLLLBBBB3333zzzzRRRRjjjj0000rr


Chunk 1047:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 57, 'num_chars': 512}
Text: eeeeuuuu////ccccxxxxLLLLBBBB3333zzzzRRRRjjjj0000rrrr6666EEEEdddd4444nnnn11119999vvvv4444rrrrBBBBbbbb<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
GloVe: Global Vectors • Let’s take the global co-occurrence statistics: X i,j • Training faster • Scalable to very large corpora (Pennington et al, 2014): GloVe: Global Vectors for Word Representation
GloVe: Global Vectors (Pennington et al, 2014): GloVe: Global Vectors for Word Representation
FastText: Sub-Word Embeddings • Similar as Skip-gram, but break words into n


Chunk 1048:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 50, 'num_chars': 512}
Text: ngs • Similar as Skip-gram, but break words into n-grams with n = 3 to 6 3-grams: <wh, whe, her, ere, re> where: 4-grams: <whe, wher, here, ere> 5-grams: <wher, where, here> 6-grams: <where, where> • u v u v Replace by g j i j · · <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ooooXXXX8888MMMM9999OOOO0000FFFFffff2222eeeekkkkffffvvvvBBBBmmmmSSSSkkkkooooLLLLppppIIII7777yyyy8888XXXXYYYY===="""">>>>AAAAAAAAAAAACCCCCCCCHHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEENNNN3333


Chunk 1049:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: iiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEENNNN3333UUUUrrrr1111qqqq////oooohhhh44449999uuuuFFFFggggEEEETTTTyyyyUUUURRRRQQQQYYYY9999FFFFLLLLxxxx4444rrrr2222FFFFZZZZooooQQQQ9999hhhhssssNNNNuuuu3333aaaazzzzWWWW7777YYYY3333RRRRRRRRKKKKyyyyNNNNGGGGLLLLffff8888WWWWLLLLBBBB0000WWWW8888++++hhhhOOOO8888++++WWWW////ccccttttBBBBGGGG00009999ccccHHHHAAAA444477770000ZZZZZZZZuuuuYYYYFFFFCCCCaaaaNNNNKKKKOOOO88886666XXXXVVVVVVVVllllaaaaXXXXllllllllddddqqqq66667777XXXXNNNNjjjjaaaa3333ttttnnnnffffssss3333bbbb2222OOOOEEEEqqqqnn


Chunk 1050:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: aaaa3333ttttnnnnffffssss3333bbbb2222OOOOEEEEqqqqnnnnEEEEppppIIII0000FFFFEEEE////IIIIuuuuQQQQIIIIoooowwwwyyyykkkkllllbbbbUUUU88883333IIIIXXXXSSSSIIIIJJJJiiiiggggNNNNGGGGuuuussssHHHHooooqqqqvvvvCCCC7777YYYYyyyyIIIIVVVVFFFFffffxxxxWWWWTTTTxxxxLLLLiiiixxxxWWWWjjjjAAAAaaaaUUUUQQQQxxxx0000kkkkbbbbyyyy7777ccccNNNN++++jjjjPPPPQQQQwwwwiiiiLLLLIIII00009999yyyynnnnssss44441111BBBBoooo++++CCCCOOOONNNNcccc////////eeeetttt++++ttttOOOOwwww5555kkkkCCCCLLLLhhhhKKKK3333JJJJHHHHVVVVQQQQoooouuuuXXXXbbbbnnnn////1111QQQQ4444DDDD


Chunk 1051:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: HHVVVVQQQQoooouuuuXXXXbbbbnnnn////1111QQQQ4444DDDDQQQQmmmmXXXXGGGGOOOOGGGGllllOOOOqqqq5555TTTTqqqqKKKK9999DDDDEEEEllllNNNNMMMMSSSSNNNN5555rrrrZZZZ8888qqqqkkkkiiiiAAAA8888QQQQggggPPPPSSSSMMMM5555SSSSjjjjmmmmCCCCggggvvvvmmmmzzzz6666SSSSwwww2222OOOOjjjjhhhhDDDDAAAASSSS0000hhhhTTTTXXXXccccKKKKrrrr++++nnnnsssshhhhQQQQrrrrNNNNQQQQkkkkDDDDkkkkxxxxnnnnccccaaaaOOOOaaaa9999wwwwrrrrxxxxPPPP6666++++XXXX6666uuuujjjjCCCCyyyyyyyyhhhhPPPPUUUUkkkk00004444nnnniiii2222KKKKUUUUggggaaaa1111ggggEEEEUUUUqqqqMMMMKKKKSSSSSSSSYYYYMM


Chunk 1052:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: ggggaaaa1111ggggEEEEUUUUqqqqMMMMKKKKSSSSSSSSYYYYMMMM0000mmmmhhhhiiiiAAAAssssqqqqbbbbkkkkVVVV4444iiiiGGGGSSSSCCCCGGGGuuuuTTTTXXXXcccc2222EEEE4444MMMM6666////vvvvEEEEgggg6666ppppwwww3333XXXXaaaabbbbgggg3333ZZZZ////XXXXmmmmZZZZRRRRllllHHHHFFFFRRRRyyyyAAAAIIII3333AAAACCCCXXXXHHHHAAAAOOOOmmmmuuuuAAAAaaaattttEEEEAAAAbbbbYYYYPPPPAAAAAAAAnnnnssssAAAALLLLeeeeLLLLUUUUeeeerrrrWWWWffffrrrrzzzzXXXXqqqqffffttttVVVVaaaassssccccmmmmYYYYffff////IIIIHHHH11118888QQQQ1111eeee4444JJJJoooovvvv<<<<////llllaaaatttteeeexxxxiiiitttt


Chunk 1053:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 9, 'num_chars': 512}
Text: 44JJJJoooovvvv<<<<////llllaaaatttteeeexxxxiiiitttt>>>> g n-grams(w ) X i 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""vvvvjjjjRRRRrrrr++++MMMMXXXXnnnnddddBBBBSSSS33339999DDDD++++OOOOssss22222222ZZZZGGGGPPPPUUUURRRRTTTTaaaaYYYY===="""">>>>AAAAAAAAAAAACCCCKKKKHHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAFFFFNNNNzzzzUUUUrrrr1111qqqq////qqqqhhhh66669999LLLLBBBBaaaahhhhHHHHiiiiyyyyJJJJCCCCHHHHqqqqzzzz6666MMMMWWWWjjjjggggqqqq1111CCCCUUUU8888JJJJmmmmuuuu2222nnnnXXXXbbbbjjjj


Chunk 1054:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: qq1111CCCCUUUU8888JJJJmmmmuuuu2222nnnnXXXXbbbbjjjjZZZZhhhh99996666VVVVaaaaQQQQnnnn6666OOOOFFFF////++++KKKKFFFFxxxxFFFFFFFFvvvvPPPPppppLLLL3333LLLLYYYYRRRRttttHHHHVVVVggggYYYYZZZZiiiiZZZZxxxx777744443333ffffiiiiyyyy4444BBBBttttvvvv++++ttttAAAAppppzzzz8888wwwwuuuuLLLLSSSS8888XXXXllll0000ssssrrrrqqqq2222vvvvppppGGGGeeeeXXXXOOOOrrrrqqqqaaaaNNNNEEEEUUUUddddaaaaggggkkkkYYYYjjjjUUUUrrrrUUUU88880000EEEE1111yyyyyyyyBBBBnnnnAAAAQQQQ7777DDDDZZZZWWWWjjjjIIIISSSS++++YYYYDDDDdddd++++////3333zzzzkkkk3333wwwwyyyyYYYY0000jj


Chunk 1055:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: DDDDdddd++++////3333zzzzkkkk3333wwwwyyyyYYYY0000jjjjyyyySSSS1111zzzzCCCCMMMMWWWWTTTTsssskkkkXXXXcccckkkkDDDDTTTTggggkkkkYYYYyyyySSSSuuuuffffuuuujjjjooooJJJJvvvvbbbbSSSSLLLLXXXXSSSS6666xxxxddddIIIIEEEE9999QQQQHHHHrrrrQQQQVVVVSSSSTTTTUUUUWWWWffffXXXXeeee4444////uuuuZZZZGGGGxxxxLLLLoooo++++UUUUGGGGaaaaZZZZJJJJ4444JJJJ0000UUUU4444EEEE++++EEEEccccaaaaZZZZNNNN6666ddddVVVV66667777YYYYNNNNXXXXssssMMMMPPPPEEEEuuuuccccnnnnFFFFRRRRQQQQjjjjkkkkuuuuvvvv////OOOOpppp2222IIIIppppqqqqEEEETTTTAAAAIIIIVVVVRRRROOOOuuuuWWWWYYYY


Chunk 1056:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: IIppppqqqqEEEETTTTAAAAIIIIVVVVRRRROOOOuuuuWWWWYYYY8888ffffQQQQTTTTooookkkkCCCCTTTTggggXXXXLLLLSSSSmmmm6666iiiiWWWWUUUUxxxxoooonnnn3333RRRRZZZZyyyy1111BBBBJJJJQQQQqqqqbbbbbbbb6666ffffjjjjQQQQDDDDOOOO8888ZZZZppppYYYYOOOODDDDSSSSJJJJkkkknnnnAAAAYYYY////VVVV3333xxxxOOOOpppp2222VVVVssssPPPPQQQQ99998888kkkkRRRRzzzzvvvvqqqqaaaaWWWW8888kkkk////uuuueeee1111EEEEgggghhhhOOOO2222iiiimmmmXXXXccccQQQQJJJJMMMM0000ssssllllHHHHQQQQSSSSIIIIwwwwRRRRHHHHjjjjUUUUGGGGuuuu5555wwwwxxxxSSSSiiiiIIIIooooSSSSGGGGEEEEKKKKmmmm55552222xx


Chunk 1057:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 8, 'num_chars': 512}
Text: xxxxSSSSiiiiIIIIooooSSSSGGGGEEEEKKKKmmmm55552222xxxxbbbbRRRRHHHHFFFFKKKKFFFFgggguuuuiiii2222ZZZZEEEEppppzzzzppppkkkk2222ddddJJJJ88887777DDDDmmmm2222DDDDXXXXnnnn6666qqqqhhhhSSSSPPPP8888vvvvrrrrKKKKKKKKIIIIddddttttIIIIuuuuqqqqyyyyEEEEHHHHHHHHqqqqIIII4444uuuu0000CCCCVVVVqqqqIIIIIIIIooooeeee0000TTTTNNNN6666QQQQ++++////WWWWkkkk////VVVViiiiffffVVVViiiiffffkkkk2222jjjjBBBByyyymmmmeeee22220000RRRR9999YYYYXXXX9999////UUUUuuuuKKKKeeeetttt<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • More to come! Contextualized word emb


Chunk 1058:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 60, 'num_chars': 512}
Text: iitttt>>>> • More to come! Contextualized word embeddings (Bojanowski et al, 2017): Enriching Word Vectors with Subword Information
Trained word embeddings available • word2vec: https://code.google.com/archive/p/word2vec/ • GloVe: https://nlp.stanford.edu/projects/glove/ • FastText: https://fasttext.cc/ Differ in algorithms, text corpora, dimensions, cased/uncased...
Evaluating Word Embeddings
Extrinsic vs intrinsic evaluation 👎 Extrinsic evaluation • Let’s plug these word embeddings ML model into a real NL


Chunk 1059:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 82, 'num_chars': 512}
Text: plug these word embeddings ML model into a real NLP system and see whether this improves performance 0.31 0.01 1.87 −3.17 1.23 • (−0.28) (−0.91) (0.03) (−0.18) (1.59) Could take a long time but still the most important evaluation metric I don’t like this movie Intrinsic evaluation • Evaluate on a specific/intermediate subtask • Fast to compute • Not clear if it really helps the downstream task
Intrinsic evaluation Word similarity Example dataset: wordsim-353 353 pairs of words with human judgement http://ww


Chunk 1060:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 44, 'num_chars': 512}
Text:  353 pairs of words with human judgement http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Cosine similarity: Metric: Spearman rank correlation
Intrinsic evaluation Word Similarity
Intrinsic evaluation Word analogy man: woman king: ? ≈ arg max (cos(u , u u + u )) i b a c   i <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJrrrrppppggggXXXXOOOOIIIIkkkk2222wwwwxxxxyyyy6666PPPPeeeeooooggggrrrrggggqqqqRRRRjjjj0000rrrrjjjj22224444===="""">>>>AAAAAAAAAAAACCCCQQQ


Chunk 1061:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: 000rrrrjjjj22224444===="""">>>>AAAAAAAAAAAACCCCQQQQHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqkkkkssss3333gggg0000VVVVooooUUUUUUUUssssiiiiggggiiii6666LLLLbbbbllllxxxxWWWWssssAAAA9999ooooQQQQpppphhhhMMMMJJJJ++++3333gggg5555MMMMHHHHMMMMjjjjVVVVhhhhCCCCPPPP88882222NNNNnnnn++++DDDDOOOOttttRRRRssssXXXXiiiirrrrhhhh11115555aaaaTTTTttttIIIIrrrrYYYYeeeeGGGGDDDDjjjjnnnn3333HHHHuuuu5555dddd44444444XXXXCCCC66667777AAAANNNNFFFF++++NNNNwwwwttttLLLLyyyyyyyyuuuuppppaaaaccccbbbb22220000s


Chunk 1062:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: NwwwwttttLLLLyyyyyyyyuuuuppppaaaaccccbbbb22220000ssssbbbbmmmm1111vvvvVVVVPPPPeeee3333WWWWuuuurrrrKKKKJJJJGGGGUUUUttttWWWWggggkkkkIIIIttttnnnn1111iiiiGGGGKKKKCCCChhhh6666wwwwFFFFHHHHAAAATTTTrrrrxxxxppppKKKKRRRRwwwwBBBBOOOOssss444499991111ffffZZZZ////XXXXOOOOAAAA5555OOOOKKKKRRRR++++EEEEddddjjjjGGGGLLLLmmmmBBBBGGGGQQQQQQQQccccpppp9999TTTTAAAAttttppppyyyyyyyyxxxx2222bbbbyyyyIIIIEEEEddddkkkkEEEEcccc33335555WWWWNNNNbbbbMMMMBBBB++++qqqqNNNNoooo1111UUUUVVVVVVVVsssswwww9999PPPPwwww0000GGGGbbbbvvvv8888BBBBOOOOeeeeUUU


Chunk 1063:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: www9999PPPPwwww0000GGGGbbbbvvvv8888BBBBOOOOeeeeUUUUhhhh0000////zzzzkkkkuuuuDDDDjjjjvvvvKKKKQQQQ1111WWWW////LLLLBBBBEEEEGGGGppppuuuuuuuuWWWWLLLLWWWWzzzzQQQQnnnnwwwwIIIIrrrrFFFFmmmmppppIIIIJJJJmmmmaaaaLLLLrrrrllllFFFF7777ssssffff0000SSSSRRRRggggIIIIVVVVBBBBBBBBllllOOOOppppZZZZZZZZggggxxxxOOOOSSSSiiiiRRRRwwwwKKKKttttiiii4444ZZZZCCCCeeeeKKKKxxxxYYYYTTTTeeeekkkkwwwwHHHHrrrraaaaRRRRqqqqSSSSggggCCCCkkkknnnnnnnnQQQQQQQQwwwwxxxxkkkkffffaaaa6666WWWWMMMM////kkkkvvvvqqqqFFFFggggCCCCdddduuuuffffiiiiIIIIllllggggVVVVKKKKj


Chunk 1064:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 1, 'num_chars': 512}
Text: qFFFFggggCCCCdddduuuuffffiiiiIIIIllllggggVVVVKKKKjjjjwwwwNNNNOOOOdddd2222aaaa1111qqqqvvvvppppaaaaZZZZ////9999VVVV6666CCCCffffiiiiXXXXTTTTssssrrrrDDDDOOOOAAAAEEEEWWWW0000uuuukkkkiiiiPPPPxxxxEEEEYYYYIIIIppppyyyylllliiiiffffttttccccMMMMggggppppiiiippppAAAAmmmmhhhhkkkkuuuuttttbbbbMMMMRRRR0000SSSSSSSSSSSSjjjjoooozzzzEEEEssss6666BBBBGGGGvvvv++++yyyy4444uuuukkkkffffVVVVaaaa3333zzzzLLLLpppp1111eeee11115555ppppXXXXMMMM3333iiiiKKKKKKKKIIIIDDDDddddIIIIiiiiqqqqyyyyEEEEIIIIXXXXqqqqIIIIFFFFuuuuUUUUBBBBOOOO1111EEEEEEEEVVV


Chunk 1065:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 35, 'num_chars': 512}
Text: IIIXXXXqqqqIIIIFFFFuuuuUUUUBBBBOOOO1111EEEEEEEEVVVVPPPP6666AAAA11119999ooooEEEE////jjjj2222XXXXgggg3333vvvvoooozzzzvvvvaaaaWWWWvvvvBBBBmmmmMMMM3333ssssoooozzzz8888wwwwffffnnnn4444BBBBllllssssGGGGwwww6666AAAA========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> semantic syntactic Chicago:Illinois Philadelphia: ? bad:worst cool: ? ≈ ≈ More examples at http://download.tensorflow.org/data/questions-words.txt
What can go wrong with word embeddings? • What’s wrong with learning a word’s “meaning” from its usage? • Wha


Chunk 1066:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 95, 'num_chars': 512}
Text:  learning a word’s “meaning” from its usage? • What data are we learning from? • What are we going to learn from this data?
What do we mean by bias? • Identify she - he axis in word vector space, project words onto this axis Bolukbasi et al. (2016) • Nearest neighbor of (b - a + c) Manzini et al. (2019)
Debiasing • Identify gender subspace with gendered words homemaker • Project words onto this she subspace homemaker’ • Subtract those woman projections from the he original word man Bolukbasi et al. (2016)
H


Chunk 1067:
Document ID: dfc2b412-6fd2-4c4b-ada2-4dd9bcc56718
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/Lesson six-Word Embeddings.pdf', 'folder_name': 'Module 6', 'num_tokens': 46, 'num_chars': 266}
Text: the he original word man Bolukbasi et al. (2016)
Hardness of Debiasing • Not that effective...and the male and female words are still clustered together • Bias pervades the word embedding space and isn’t just a local property of a few words Gonen and Goldberg (2019)


Chunk 1068:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 88, 'num_chars': 512}
Text: Contextual Token Representations ULMfit, OpenAI GPT, ELMo, BERT, XLM Noe Casas
Background: Language Modeling T T ... </s> 1 2 • Data: Monolingual Corpus softmax softmax ... softmax project. project. ... project. • Task: predict next token given previous tokens (causal): embed 1 embed 2 embed 3 Model P(T | T ...T ) i 1 i−1 • Usual models: LSTM, Transformer. <s> T ... T 1 N
Contextual embeddings: intuition • Same word can have different meaning depending on the context. Example: - Please, type everything in l


Chunk 1069:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 73, 'num_chars': 512}
Text: e context. Example: - Please, type everything in lowercase. - What type of flowers do you like most? • Classic word embeddings offer the same vector representation regardless of the context. • Solution: create word representations that depend on the context.
Articles Model Alias Org. Article Reference Universal Language Model Fine-tuning for Text Classification ULMfit fast.ai Howard and Ruder Deep contextualized word representations ELMo AllenNLP Peters et al. Improving Language Understanding by Generative 


Chunk 1070:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 73, 'num_chars': 512}
Text: l. Improving Language Understanding by Generative Pre-Training OpenAI GPT OpenAI Radford et al. BERT: Pre-training of Deep Bidirectional Transformers for BERT Google Language Understanding Devlin et al. Cross-lingual Language Model Pretraining XLM Facebook Lample and Conneau
Overview • Train model in one of multiple tasks that lead to word representations. • Release pre-trained models. • Use pre-trained models, options: A. Fine-tune model on final task. B. Directly encode token representations with model.
O


Chunk 1071:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 67, 'num_chars': 512}
Text: irectly encode token representations with model.
Overview (graphical) Phase 1: Phase 2: semi-supervised training downstream task fine-tuning *LM task Downstream task LM task head Downstream task (projection + softmax) head contextual transfer learning representations Language Language Modeling Modeling Architecture Architecture small learning rate or directly freeze monolingual task-specific weights corpus data
Differences Alias Model Token Tasks Language ULMfit LSTM word Causal LM English ELMo LSTM word Bi


Chunk 1072:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 90, 'num_chars': 512}
Text: Mfit LSTM word Causal LM English ELMo LSTM word Bidirectional LM English Causal LM OpenAI GPT Transformer subword English + Classification Masked LM BERT Transformer subword + Next sentence Multilingual prediction Causal LM XLM Transformer subword +Masked LM Multilingual + Translation LM
ULMFiT • Task: causal LM T T </s> 1 2 ... softmax softmax ... softmax • Model: 3-layer LSTM project. project. ... project. • Tokens: words LSTM LSTM ... LSTM LSTM LSTM ... LSTM LSTM LSTM ... LSTM </s> E ... E 1 N
ELMO • T 1


Chunk 1073:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 95, 'num_chars': 512}
Text: STM LSTM LSTM ... LSTM </s> E ... E 1 N
ELMO • T 1 T 2 ... T N Task: bidirectional LM softmax softmax ... softmax • project. project. ... project. Model: 2-layer biLSTM • Tokens: words LSTM LSTM ... LSTM LSTM ... LSTM LSTM LSTM LSTM ... LSTM LSTM ... LSTM LSTM charCNN charCNN charCNN charCNN ... <s> C C </s> 1 N
OpenAI GPT • Task: causal LM Output tokens he will be late </s> • Model: self-attention layers softmax softmax softmax softmax softmax project. project. project. project. project. • Tokens: subwords


Chunk 1074:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 80, 'num_chars': 512}
Text: ect. project. project. project. • Tokens: subwords Self-attention layers Token </s>] he will be late embeddings + + + + + Positional 0 1 2 3 4 embeddings
BERT he will br late [SEP] you should leave now [SEP] Output tokens This output is used for classification tasks softmax softmax softmax softmax softmax softmax softmax softmax softmax softmax project. project. project. project. project. project. project. project. project. project. Self-attention Layers Token [CLS] he [MASK] be late [SEP] you [MASK] leave 


Chunk 1075:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 123, 'num_chars': 512}
Text: en [CLS] he [MASK] be late [SEP] you [MASK] leave now [SEP] embeddings + + + + + + + + + + + Positional 0 1 2 3 4 5 6 7 8 9 10 embeddings + + + + + + + + + + + Segment A A A A A A B B B B B embeddings 15% of tokens get masked • Tasks: masked LM + next sentence prediction • Model: self-attention layers • Tokens: subwords
Masked Language take [/s] drink now Modeling (MLM) Transformer Token [/s] [MASK] a seat X[MASK] LhaMve a [MASK] [/s] [MASK] relax and embeddings + + + + + + + + + + + + Position 0 1 2 3 4 5 


Chunk 1076:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 138, 'num_chars': 512}
Text: ings + + + + + + + + + + + + Position 0 1 2 3 4 5 6 7 8 9 10 11 embeddings + + + + + + + + + + + + Language en en en en en en en en en en en en embeddings Translation Language curtains were les bleus Modeling (TLM) Transformer Token [/s] the [MASK] [MASK] blue [/s] [/s] [MASK] rideaux étaient [MASK] [/s] embeddings + + + + + + + + + + + + Position 0 1 2 3 4 5 0 1 2 3 4 5 embeddings + + + + + + + + + + + + Language en en en en en en fr fr fr fr fr fr embeddings • Tasks: LM + masked LM + Translation LM Maske


Chunk 1077:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 72, 'num_chars': 512}
Text: ngs • Tasks: LM + masked LM + Translation LM Masked LM with • Model: self-attention layers parallel sentences • Tokens: subwords Projection and softmax are omitted *figure from “Cross-lingual Language Model Pretraining”
Downstream Tasks • Natural Language Inference (NLI) or Cross-lingual NLI. • Text classification (e.g. sentiment analysis). • Next sentence prediction. • Supervised and Unsupervised Neural Machine Translation (NMT). • Question Answering (QA). • Named Entity Recognition (NER).
Further reading 


Chunk 1078:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 69, 'num_chars': 512}
Text: • Named Entity Recognition (NER).
Further reading • “Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling”, Bowman et al., 2018 • “What do you learn from context? Probing for sentence structure in contextualized word representations”, Tenney et al., 2018. • “Assessing BERT’s Syntactic Abilities”, Goldberg, 2018 • “Learning and Evaluating General Linguistic Intelligence”, Yogatama et al., 2019.
Differences with other representations Note the differences of contextual token represen


Chunk 1079:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 57, 'num_chars': 512}
Text:  Note the differences of contextual token representations with: • Non-word representations like in (CoVe): Learned in Translation: Contextualized Word Vectors by McCann et al. 2017 [salesforce]. • Fixed-size sentence representations like in Massively Multilingual Sentence Embeddings for Zero-Shot Cross- Lingual Transfer and Beyond by Artetxe and Schewnk, 2018 [facebook].
Other resources • https://nlp.stanford.edu/seminar/details/jdevlin.pdf • http://jalammar.github.io/illustrated-bert/ • https://medium.com/


Chunk 1080:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 57, 'num_chars': 512}
Text: .github.io/illustrated-bert/ • https://medium.com/dissecting-bert/dissecting-bert- part2-335ff2ed9c73 • https://github.com/huggingface/pytorch-pretrained-BERT
Summary Phase 1: Phase 2: semi-supervised training downstream task fine-tuning *LM task Downstream task task-specific data LM task head Downstream task monolingual (projection + softmax) transfer learning head corpus model model Alias Model Token Tasks Language ULMfit LSTM word Causal LM English ELMo LSTM word Bidirectional LM English Causal LM OpenAI


Chunk 1081:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 76, 'num_chars': 512}
Text: STM word Bidirectional LM English Causal LM OpenAI GPT Transformer subword English + Classification Masked LM BERT Transformer subword Multilingual + Next sentence prediction Causal LM XLM Transformer subword +Masked LM Multilingual + Translation LM
Bonus slides
Are these really token representations? • They are a linear projection away from he will be late token space. softmax softmax softmax softmax project. project. project. project. • Word-level nearest neighbours in Model corpus finds same word with sa


Chunk 1082:
Document ID: 4aa396b3-032d-4c66-86fc-bc056d44b9fe
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 6/contextual_representations.pdf', 'folder_name': 'Module 6', 'num_tokens': 14, 'num_chars': 75}
Text: neighbours in Model corpus finds same word with same usage. he will be late


Chunk 1083:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text: Information Extraction: Methodologies and Applications Jie Tang, Mingcai Hong, Duo Zhang, Bangyong Liang, and Juanzi Li Jie Tang (corresponding author) Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: jietang@tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Mingcai Hong Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax numb


Chunk 1084:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 56, 'num_chars': 512}
Text: a University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: hmc@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Duo Zhang Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: zhangduo@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Bangyong Liang Affiliation: NEC Labs China Telephone:


Chunk 1085:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text: gyong Liang Affiliation: NEC Labs China Telephone: +86013601002822 Fax number: +8610-62789831 E-mail: liangbangyong@research.nec.com.cn Post mail address: 11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084, China Juanzi Li Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62781461 Fax number: +8610-62789831 E-mail: ljz@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. 1
Keyword List Computer Sc


Chunk 1086:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 64, 'num_chars': 512}
Text: Beijing, 100084. China. 1
Keyword List Computer Science Data Resource Management Data Extraction Data Management Data Mining Knowledge Discovery Software Natural Language Processors Information Systems Information Theory Information Processing 2
Information Extraction: Methodologies and Applications Jie Tang1, Mingcai Hong1, Duo Zhang1, Bangyong Liang2, and Juanzi Li1 1Department of Computer Science, Tsinghua University 10-201, East Main Building, Tsinghua University, Beijing, 100084. China 2NEC Labs China 


Chunk 1087:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: niversity, Beijing, 100084. China 2NEC Labs China 11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084. China liangbangyong@research.nec.com.cn ABSTRACT This chapter is concerned with the methodologies and applications of information extraction. Information is hidden in the large volume of web pages and thus it is necessary to extract useful information from the web content, called Information Extraction. In information extraction, given a sequence of instances, we identify and pull out a su


Chunk 1088:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: quence of instances, we identify and pull out a sub-sequence of the input that represents information we are interested in. In the past years, there was a rapid expansion of activities in the information extraction area. Many methods have been proposed for automating the process of extraction. However, due to the heterogeneity and the lack of structure of Web data, automated discovery of targeted or unexpected knowledge information still presents many challenging research problems. In this chapter, we will 


Chunk 1089:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: nging research problems. In this chapter, we will investigate the problems of information extraction and survey existing methodologies for solving these problems. Several real-world applications of information extraction will be introduced. Emerging challenges will be discussed. INTRODUCTION Information Extraction (IE), identifying and pulling out a sub-sequence from a given sequence of instances that represents information we are interested in, is an important task with many practical applications. Informa


Chunk 1090:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: ant task with many practical applications. Information extraction benefits many text/web applications, for example, integration of product information from various websites, question answering, contact information search, finding the proteins mentioned in a biomedical journal article, and removal of the noisy data. Our focus will be on methodologies of automatic information extraction from various types of documents (including plain texts, web pages, and emails, etc.). Specifically, we will discuss three of


Chunk 1091:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: ils, etc.). Specifically, we will discuss three of the most popular methods: rule learning based 1
method, classification model based method, and sequential labeling based method. All these methods can be viewed as supervised machine learning approaches. They all consist of two stages: extraction and training. In extraction, the sub-sequence that we are interested in are identified and extracted from given data using learned model(s) by different methods. Then the extracted data are annotated as specified i


Chunk 1092:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: en the extracted data are annotated as specified information on the basis of the predefined metadata. In training, the model(s) are constructed to detect the sub-sequence. In the models, the input data is viewed as a sequence of instances, for example, a document can be viewed as either a sequence of words or a sequence of text lines (it depends on the specific application). All these methodologies have immediate real-life applications. Information extraction has been applied, for instance, to part-of-speec


Chunk 1093:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: n has been applied, for instance, to part-of-speech tagging (Ratnaparkhi, 1998), named entity recognition (Zhang, 2004), shallow parsing (Sha, 2003), table extraction (Ng, 1999; Pinto, 2003; Wang, 2002), and contact information extraction (Kristjansson, 2004). In the rest of the chapter, we will describe the three types of the state-of-the-art methods for information extraction. This is followed by presenting several applications to better understand how the methods can be utilized to help businesses. The c


Chunk 1094:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text:  methods can be utilized to help businesses. The chapter will have a mix of research and industry flavor, addressing research concepts and looking at the technologies from an industry perspective. After that, we will discuss the challenges the information extraction community faced. Finally, we will give the concluding remark. METHODOLOGIES Information extraction is an important research area, and many research efforts have been made so far. Among these research work, rule learning based method, classificat


Chunk 1095:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: arch work, rule learning based method, classification based method, and sequential labeling based method are the three state-of-the-art methods. Rule Learning based Extraction Methods In this section, we review the rule based algorithms for information extraction. Numerous information systems have been developed based on the method, including: AutoSlog (Riloff, 1993), Crystal (Soderland, 1995), (LP)2 (Ciravegna, 2001), iASA (Tang, 2005b), Whisk (Soderland, 1999), Rapier (Califf, 1998), SRV (Freitag, 1998), 


Chunk 1096:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: 999), Rapier (Califf, 1998), SRV (Freitag, 1998), WIEN (Kushmerick, 1997), Stalker (Muslea, 1998; Muslea, 1999a), BWI (Freitag, 2000), etc. See (Muslea, 1999b; Siefkes, 2005; Peng, 2001) for an overview. In general, the methods can be grouped into three categories: dictionary based method, rule based method, and wrapper induction. Dictionary based method Traditional information extraction systems first construct a pattern (template) 2
dictionary, and then use the dictionary to extract needed information fro


Chunk 1097:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: e the dictionary to extract needed information from the new untagged text. These extraction systems are called as dictionary based systems (also called pattern based systems) including: AutoSlog (Riloff, 1993), AutoSlog-TS (Riloff, 1996), and CRYSTAL (Soderland, 1995). The key point in the systems is how to learn the dictionary of patterns that can be used to identify the relevant information from a text. AutoSlog (Riloff, 1993) was the first system to learn text extraction dictionary from training examples


Chunk 1098:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text:  text extraction dictionary from training examples. AutoSlog builds a dictionary of extraction patterns that are called concept nodes. Each AutoSlog concept node has a conceptual anchor that activates it and a linguistic pattern, which, together with a set of enabling conditions, guarantees its applicability. The conceptual anchor is a triggering word, while the enabling conditions represent constraints on the components of the linguistic pattern. For instance, in order to extract the target of the terroris


Chunk 1099:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: ce, in order to extract the target of the terrorist attack from the sentence The Parliament was bombed by the guerrillas. One can use a concept that consists of the triggering word bombed together with the linguistic pattern <subject> passive-verb. Applying such an extraction pattern is straightforward: first, the concept is activated because the sentence contains the triggering word bombed; then the linguistic pattern is matched against the sentence and the subject is extracted as the target of the terrori


Chunk 1100:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text:  subject is extracted as the target of the terrorist attack. AutoSlog uses a predefined set of 13 linguistic patterns; the information to be extracted can be one of the following syntactic categories: subject, direct object, or noun phrase. In general, the triggering word is a verb, but if the information to be extracted is a noun phrase, the triggering word may also be a noun. In Figure 1, we show a sample concept node. The slot “Name” is a concise, human readable description of the concept. The slot “Trig


Chunk 1101:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 73, 'num_chars': 512}
Text: eadable description of the concept. The slot “Trigger” defines the conceptual anchor, while the slot “Variable Slots” represents that the information to be extracted is the subject of the sentence. Finally, the subject must be a physical target (see “Constraints:”), and the enabling conditions require the verb to be used in its passive form. Concept Node: Name: target-subject-passive-verb-bombed Trigger: bombed Variable Slots: (target (*S *1)) Constraints: (class phys-target *S*) Constant Slots: (type bombi


Chunk 1102:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: class phys-target *S*) Constant Slots: (type bombing) Enabling Conditions: ((passive)) Figure 1. Example of AutoSlog concept node AutoSlog needs to parse the natural language sentence using a linguistic parser. The parser is used to generate syntax elements of a sentence (such as subject, verb, preposition phrase). Then the output syntax elements are matched against the linguistic pattern and fire the best matched pattern as the result pattern to construct a pattern dictionary. 3
AutoSlog need tag the text 


Chunk 1103:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 86, 'num_chars': 512}
Text:  pattern dictionary. 3
AutoSlog need tag the text before extracting patterns. This disadvantage has been improved by AutoSlog-TS (Riloff, 1996). In AutoSlog-TS, one does not need to make a full tag for the input data and only needs to tag the data whether it is relevant to the domain or not. The procedure of AutoSlog is divided into two stages. In the first stage, the sentence analyzer produces a syntactic analysis for each sentence and identifies the noun phrases using heuristic rules. In the second stage,


Chunk 1104:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: hrases using heuristic rules. In the second stage, the pre-classified text is inputted to the sentence analyzer again with the pattern dictionary generated in the first stage. The sentence analyzer activates all the patterns that are applicable in each sentence. The system then computes relevance statistics for each pattern and uses a rank function to rank the patterns. In the end, only the top patterns are kept in the dictionary. Riloff et al (1999) propose using bootstrapping to generate the dictionary wi


Chunk 1105:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text:  using bootstrapping to generate the dictionary with a few tagged texts (called seed words). The basic idea is to use a mutual bootstrapping technique to learn extraction patterns using the seed words and then exploit the learned extraction patterns to identify more seed words that belong to the same category. In this way, the pattern dictionary can be learned incrementally as the process continues. See also Crystal (Soderland, 1995). Rule based method Different from the dictionary based method, the rule ba


Chunk 1106:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: rent from the dictionary based method, the rule based method use several general rules instead of dictionary to extract information from text. The rule based systems have been mostly used in information extraction from semi-structured web page. A usual method is to learn syntactic/semantic constraints with delimiters that bound the text to be extracted, that is to learn rules for boundaries of the target text. Two main rule learning algorithms of these systems are: bottom-up method which learns rules from s


Chunk 1107:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: ms are: bottom-up method which learns rules from special cases to general ones, and top-down method which learns rules from general cases to special ones. There are proposed many algorithms, such as (LP)2 (Ciravegna, 2001), iASA (Tang, 2005b), Whisk (Soderland, 1999), Rapier (Califf, 1998), and SRV (Freitag, 1998). Here we will take (LP)2 and iASA as examples in our explanation. (LP)2 (LP)2 (Ciravegna, 2001) is one of the typical bottom-up methods. It learns two types of rules that respectively identify the


Chunk 1108:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text:  two types of rules that respectively identify the start boundary and the end boundary of the text to be extracted. The learning is performed from examples in a user-defined corpus (training data set). Training is performed in two steps: initially a set of tagging rules is learned; then additional rules are induced to correct mistakes and imprecision in extraction. Three types of rules are defined in (LP)2: tagging rules, contextual rules, and correction rules. A tagging rule is composed of a pattern of con


Chunk 1109:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: es. A tagging rule is composed of a pattern of conditions on a connected sequence of words and an action of determining whether or not the current position is a boundary of an instance. Table 1 shows an example of the tagging rule. The first column represents a sequence of words. The second to the fifth columns represent 4
Part-Of-Speech, Word type, Lookup in a dictionary, and Name Entity Recognition results of the word sequence respectively. The last column represents the action. In the example of Table 1,


Chunk 1110:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text:  represents the action. In the example of Table 1, the action “<Speaker>” indicates that if the text match the pattern, the word “Patrick” will be identified as the start boundary of a speaker. Table 1. Example of initial tagging rule Pattern Action Word POS Kind Lookup Name Entity ; : Punctuation Patrick NNP Word Person’s first name <Speaker> Person Stroh NNP Word , , Punctuation assistant NN Word Job title professor NN Word , , Punctuation SDS NNP Word The tagging rules are induced as follows: (1) First, 


Chunk 1111:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text:  tagging rules are induced as follows: (1) First, a tag in the training corpus is selected, and a window of w words to the left and w words to the right is extracted as constraints in the initial rule pattern. (2) Then all the initial rules are generalized. The generalization algorithm could be various. For example, based on NLP knowledge, the two rules (at 4 pm) and (at 5 pm) can be generalized to be (at DIGIT pm). Each generalized rule is tested on the training corpus and an error score E=wrong/matched is


Chunk 1112:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: ining corpus and an error score E=wrong/matched is calculated. (3) Finally, the k best generalizations for each initial rule are kept in a so called best rule pool. This induction algorithm is also used for the other two types of rules discussed below. Table 2 indicates a generalized tagging rule for the start boundary identification of the Speaker. Table 2. Example of generalized tagging rule Pattern Action Word POS Kind Lookup Name Entity ; : Punctuation Word Person’s first name <Speaker> Person Word Punc


Chunk 1113:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text: ord Person’s first name <Speaker> Person Word Punctuation assistant NN Word Jobtitle professor NN Word Another type of rules, contextual rules, is applied to improve the effectiveness of the system. The basic idea is that <tag > might be used as an indicator of the x occurrence of <tag >. For example, consider a rule recognizing an end boundary y 5
between a capitalized word and a lowercase word. This rule does not belong to the best rule pool as its low precision on the corpus, but it is reliable if used o


Chunk 1114:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: cision on the corpus, but it is reliable if used only when closing to a tag <speaker>. Consequencely, some non-best rules are recovered, and the ones which result in acceptable error rate will be preserved as the contextual rules. The correction rules are used to reduce the imprecision of the tagging rules. For example, a correction rule shown in Table 3 is used to correct the tagging mistake “at <time> 4 </time> pm” since “pm” should have been part of the time expression. So, correction rules are actions t


Chunk 1115:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text: ime expression. So, correction rules are actions that shift misplaced tags rather than adding new tags. Table 3. Example of correction rule Pattern Action Word Wrong tag Move tag to At 4 </stime> pm </stime> After all types of rules are induced, information extraction is carried out in the following steps: (cid:122) The learned tagging rules are used to tag the texts. (cid:122) Contextual rules are applied in the context of introduced tags in the first step. (cid:122) Correction rules are used to correct mi


Chunk 1116:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text:  (cid:122) Correction rules are used to correct mistaken extractions. (cid:122) All the identified boundaries are to be validated, e.g. a start tag (e.g. <time>) without its corresponding close tag will be removed, and vice versa. See also Rapier (Califf, 1998; Califf, 2003) for another IE system which adopts the bottom-up learning strategy. iASA Tang et al (2005b) propose an algorithm for learning rules for information extraction. The key idea of iASA is that it tries to induce the ‘similar’ rules first. I


Chunk 1117:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: at it tries to induce the ‘similar’ rules first. In iASA, each rule consists of three patterns: body pattern, left pattern, and right pattern, respectively representing the text fragment to be extracted (called target instance), the w words previous to the target instance, and w words next to the target instance. Thus, the rule learning tries to find patterns not only in the context of a target instance, but also in the target instance itself. Tang et al define similarity between tokens (it can be word, pun


Chunk 1118:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: ine similarity between tokens (it can be word, punctuation, and name entity), similarity between patterns, and similarity between rules. In learning, iASA creates an initial rule set from the training data set. Then it searches for the most similar rules from the rule set and generalizes a new rule using the two rules. The new rule is evaluated on the training corpus and a score of the rule is calculated. If its score exceeds a threshold, it would be put back to the rule set. The processing continues until 


Chunk 1119:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: k to the rule set. The processing continues until no new rules can be generalized. The other type of strategy for learning extraction rules is the top-down fashion. The method starts with the most generalized patterns and then gradually adds constraints into the patterns in the learning processing. See SRV (Freitag, 1998) and Whisk (Soderland, 1999) as examples. 6
Wrapper induction Wrapper induction is another type of rule based method which is aimed at structured and semi-structured documents such as web p


Chunk 1120:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: ctured and semi-structured documents such as web pages. A wrapper is an extraction procedure, which consists of a set extraction rules and also program codes required to apply these rules. Wrapper induction is a technique for automatically learning the wrappers. Given a training data set, the induction algorithm learns a wrapper for extracting the target information. Several research works have been studied. The typical wrapper systems include WIEN (Kushmerick, 1997), Stalker (Muslea, 1998), and BWI (Freita


Chunk 1121:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: ck, 1997), Stalker (Muslea, 1998), and BWI (Freitag, 2000). Here, we use WIEN and BWI as examples in explaining the principle of wrapper induction. WIEN WIEN (Kushmerick, 1997) is the first wrapper induction system. An example of the wrapper defined in WIEN is shown in Figure 2, which aims to extract “Country” and “Area Code” from the two HTML pages: D1 and D2. D1: <B>Congo</B> <I>242</I><BR> D2: <B>Egypt</B> <I>20</I><BR> Rule: *‘<B>’(*)‘</B>’*‘<I>’(*)‘</I>’ Output: Country_Code {Country@1}{AreaCode@2} Fig


Chunk 1122:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 86, 'num_chars': 512}
Text: ’ Output: Country_Code {Country@1}{AreaCode@2} Figure 2. Example of wrapper induction The rule in Figure 2 has the following meaning: ignore all characters until you find the first occurrence of ‘<B>’ and extract the country name as the string that ends at the first ‘</B>’. Then ignore all characters until ‘<I>’ is found and extract the string that ends at ‘</I>’. In order to extract the information about the other country names and area codes, the rule is applied repeatedly until it fails to match. In the 


Chunk 1123:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 103, 'num_chars': 512}
Text: pplied repeatedly until it fails to match. In the example of Figure 2, we can see that the WIEN rule can be successfully to be applied to both documents D1 and D2. The rule defined above is an instance of the so called LR class. A LR wrapper is defined as a vector <l , r , ..., l , r > of 2K delimiters, with each pair <l, r> 1 1 k k i i corresponding to one type of information. The LR wrapper requires that resources format their pages in a very simple manner. Specifically, there must exist delimiters that r


Chunk 1124:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: . Specifically, there must exist delimiters that reliably indicate the left- and right-hand boundaries of the fragments to be extracted. The classes HLRT, OCLR, and HOCLRT are extensions of LR that use document head and tail delimiters, tuple delimiters, and both of them, respectively. The algorithm for learning LR wrappers (i.e. learn ) is shown in Figure 3. LR In Figure 3, E represents the example set; notation cands(k, E) represent the l candidates for delimiter l given the example set E. The candidates 


Chunk 1125:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 101, 'num_chars': 512}
Text: limiter l given the example set E. The candidates are generated by k enumerating the suffixes of the shortest string occurring to the left of each instance of attribute k in each example; valid(u, k, E) refers to the constraints to validate a l candidate u for delimiter l . k 7
procedure learn (examples E) LR { for each 1≤k≤K for each u∈cands(k, E) l if valid(u, k, E) then l ←u and terminate this loop l k for each 1≤k≤K for each u∈cands(k, E) r if valid(u, k, E) then r ←u and terminate this loop r k return 


Chunk 1126:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text: , E) then r ←u and terminate this loop r k return LR wrapper <l , r, ..., l, r> 1 1 k k } Figure 3. The learn algorithm LR LR wrapper class is the simplest wrapper class. See (Kushmerick, 2000) for variant wrapper classes. Stalker (Muslea, 1998; Muslea, 1999a) is another wrapper induction system that performs hierarchical information extraction. It can be used to extract data from such documents with multiple levels. In Stalker, rules are induced by a covering algorithm which tries to generate rules until a


Chunk 1127:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: ng algorithm which tries to generate rules until all instances of an item are covered and returns a disjunction of the found rules. A Co-Testing approach has been also proposed to support active learning in Stalker. See (Muslea, 2003) for details. BWI The Boosted Wrapper Induction (BWI) system (Freitag, 2000) targets at making wrapper induction techniques suitable for free text, which uses boosting to generate and combine the predictions from numerous extraction patterns. In BWI, a document is treated as a 


Chunk 1128:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 109, 'num_chars': 512}
Text: tion patterns. In BWI, a document is treated as a sequence of tokens, and the IE task is to identify the boundaries of different type of information. Let indices i and j denote the boundaries, we can use <i, j> to represent an instance. A wrapper W = <F, A, H> learned by BWI consists of two sets of patterns that are used respectively to detect the start and the end boundaries of an instance. Here F = {F , F , ..., F } identifies the start boundaries and A = {A , A , ..., A } identifies the 1 2 T 1 2 T end b


Chunk 1129:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 113, 'num_chars': 512}
Text: {A , A , ..., A } identifies the 1 2 T 1 2 T end boundaries; and a length function H(k) that estimates the maximum-likelihood probability that the field has length k. To perform extraction using the wrapper W, every boundary i in a document is first given a “start” score F(i)=∑ C F (i) and an “end” score A(i)=∑ C A (i). Here, k Fk k k Ak k C is the weight for F , and F (i) = 1 if i matches F , otherwise F (i) = 0. For A(i), Fk k k k k the definition is similar. W then classifies text fragment <i, j> as foll


Chunk 1130:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text: ar. W then classifies text fragment <i, j> as follows: ⎧1 if F(i)A(j)H(j−i)>τ (1) W(i, j)=⎨ ⎩0 otherwise where τ is a numeric threshold. Learning a wrapper W involves determining F, A, and H. The function H reflects the prior probability of various field lengths. BWI estimates these probabilities by constructing a frequency histogram H(k) recording the number 8
of fields of length k occurring in the training set. To learn F and A, BWI boosts LearnDetector, an algorithm for learning a single detector. Figure


Chunk 1131:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text: n algorithm for learning a single detector. Figure 4 shows the learning algorithm in BWI. procedure BWI (example sets S and E) { F ←AdaBoost(LearnDetector, S) A←AdaBoost(LearnDetector, E) H ←field length histogram from S and E return wrapper W = <F, A, H> } Figure 4. The BWI algorithm In BWI, AdaBoost algorithm runs in iterations. In each iteration, it outputs a weak learner (called hypotheses) from the training data and also a weight for the learner representing the percentage of the correctly classified i


Chunk 1132:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: nting the percentage of the correctly classified instances by applying the weak learner to the training data. AdaBoost simply repeats this learn-update cycle T times, and then returns a list of the learned weak hypotheses with their weights. BWI invokes LearnDetector (indirectly through AdaBoost) to learn the “fore” detectors F, and then T more times to learn the “aft” detectors A. LearnDetector iteratively builds from a empty detector. At each step, LearnDetector searches for the best extension of length L


Chunk 1133:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: tector searches for the best extension of length L (a lookahead parameter) or less to the prefix and suffix of the current detector. The procedure returns when no extension yields a better score than the current detector. More detailed experiments and results analysis about BWI is discussed in (Kauchak, 2004). Classification based Extraction Methods In this section, we introduce another principled approach to information extraction using supervised machine learning. The basic idea is to cast information ext


Chunk 1134:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 91, 'num_chars': 512}
Text: earning. The basic idea is to cast information extraction problem as that of classification. In this section, we will describe the method in detail. We will also introduce several improving efforts to the approach. Classification model Let us first consider a two class classification problem. Let {(x , y ), ... , (x , y )} 1 1 n n be a training data set, in which x denotes an instance (a feature vector) and i y ∈{−1,+1} denotes a classification label. A classification model usually consists of i two stages:


Chunk 1135:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: sification model usually consists of i two stages: learning and prediction. In learning, one attempts to find a model from the labeled data that can separate the training data, while in prediction the learned model is used to identify whether an unlabeled instance should be classified as +1 or -1. (In some cases, the prediction results may be numeric values, e.g. ranging from 0 to 1. Then an instance can be classified using some rules, e.g. classified as +1 when the prediction value is larger than 0.5.) 9
S


Chunk 1136:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: when the prediction value is larger than 0.5.) 9
Support Vector Machines (SVMs) is one of the most popular methods for classification. Now, we use SVM as example to introduce the classification model (Vapnik, 1998). Support vector machines (SVMs) are linear functions of the form f(x) = wTx + b, where wTx is the inner product between the weight vector w and the input vector x. The main idea of SVM is to find an optimal separating hyper-plane that maximally separates the two classes of training instances (mor


Chunk 1137:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: parates the two classes of training instances (more precisely, maximizes the margin between the two classes of instances). The hyper-plane then corresponds to a classifier (linear SVM). The problem of finding the hyper-plane can be stated as the following optimization problem: 1 (2) Minimize: wTw 2 s.t.:y (wTx +b)≥1,i=1,2,...,n i i To deal with cases where there may be no separating hyper-plan due to noisy labels of both positive and negative training instances, the soft margin SVM is proposed, which is for


Chunk 1138:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 83, 'num_chars': 512}
Text: ces, the soft margin SVM is proposed, which is formulated as: 1 n (3) Minimize: wTw+C∑ξ 2 i i=1 s.t.:y (wTx +b)≥1−ξ,i=1,2,...,n i i i where C≥0 is the cost parameter that controls the amount of training errors allowed. It is theoretically guaranteed that the linear classifier obtained in this way has small generalization errors. Linear SVM can be further extended into non-linear SVMs by using kernel functions such as Gaussian and polynomial kernels (Boser, 1992; Schölkopf, 1999; Vapnik, 1999). When there a


Chunk 1139:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: 992; Schölkopf, 1999; Vapnik, 1999). When there are more than two classes, we can adopt the “one class versus all others” approach, i.e., take one class as positive and the other classes as negative. Boundary detection using classification model We are using a supervised machine learning approach to IE, so our system consists of two distinct phases: learning and extracting. In the learning phase our system uses a set of labeled documents to generate models which we can use for future predictions. The extra


Chunk 1140:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: which we can use for future predictions. The extraction phase takes the learned models and applies them to new unlabelled documents using the learned models to generate extractions. The method formalizes the IE problem as a classification problem. It is aimed at detecting the boundaries (start boundary and end boundary) of a special type of information. For IE from text, the basic unit that we are dealing with can be tokens or text-lines in the text. (Hereafter, we will use token as the basic unit in our ex


Chunk 1141:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: ter, we will use token as the basic unit in our explanation.) Then we try to learn two classifiers that are respectively used to identify the boundaries. The instances are all tokens in the document. All tokens that begin with a start-label are positive instances for the start classifier, while all the other tokens become negative instances for this classifier. Similarly, the positive instances for the end classifier are the last tokens of each end-label, and the other tokens are negative instances. 10
Star


Chunk 1142:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: d the other tokens are negative instances. 10
Start classifier Start Not start Dr. Trinkle's primary research interests lie in the areas of robotic manipulation End End classifier Not end Learning Extracting Start classifier Professor Steve Skiena will be at CMU Monday, January 13, and Tuesday, February 14. End classifier Figure 5. Example of Information Extraction as classification Figure 5 gives an example of IE as classification. There are two classifiers – one to identify starts of target text fragments


Chunk 1143:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 92, 'num_chars': 512}
Text:  – one to identify starts of target text fragments and the other to identify ends of text fragments. Here, the classifiers are based on token only (however other patterns, e.g. syntax, can also be incorporated into). Each token is classified as being a start or non-start and an end or non-end. When we classify a token as a start, and also classify one of the closely following token as an end, we view the tokens between these two tokens as a target instance. In the example, the tokens “Dr. Trinkle’s” is anno


Chunk 1144:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: In the example, the tokens “Dr. Trinkle’s” is annotated as a “speaker” and thus the token “Dr.” is a positive instance and the other tokens are as negative instances in the speaker-start classifier. Similarly, the token “Trinkle’s” is a positive instance and the other tokens are negative instances in the speaker-end classifier. The annotated data is used to train two classifiers in advance. In the extracting stage, the two classifiers are applied to identify the start token and the end token of the speaker.


Chunk 1145:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text:  the start token and the end token of the speaker. In the example, the tokens “Professor”, “Steve”, and “Skiena” are identified as two start tokens by the start classifier and one end token by the end classifier. Then, we combine the identified results and view tokens between the start token and the end token as a speaker. (i.e. “Professor Steve Skiena” is outputted as a speaker) In the extracting stage, we apply the two classifiers to each token to identify whether the token is a “start”, “end”, neither, o


Chunk 1146:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 98, 'num_chars': 512}
Text:  whether the token is a “start”, “end”, neither, or both. After the extracting stage, we need to combine the starts and the ends predicted by the two classifiers. We need to decide which of the starts (if there exist more than one starts) to match with which of the ends (if there exist more than one ends). For the combination, a simple method is to search for an end from a start and then view the tokens between the two tokens as the target. If there exist two starts and only one end (as the example in Figur


Chunk 1147:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 86, 'num_chars': 512}
Text: o starts and only one end (as the example in Figure 5), then we start the search progress from the first start and view the tokens between the first token and the end token (i.e. “Professor Steve Skiena”) as the target. However, in some applications, the simple combination method may not yield good results. Several works have been conducted to enhance the combination. For example, Finn et al propose a histogram model (Finn, 2004; Finn, 2006). In Figure 5, there are two possible extractions: “Professor Steve


Chunk 1148:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 95, 'num_chars': 512}
Text: ere are two possible extractions: “Professor Steve Skiena” and “Steve Skiena”. The histogram model estimates confidence as C * C * P(|e - s|). Here C is the confidence of the s e s start prediction and C is the confidence of the end prediction. (For example, with e Naïve Bayes, we can use the posterior probability as the confidence; with SVM, we can use the distance of the instance to the hyper-plane as the confidence.) P(|e - s|) is the probability of a text fragment of that length which we get from the t


Chunk 1149:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: xt fragment of that length which we get from the training data. 11
Finally, the method selects the text fragment with the highest confidence as output. To summarize, this IE classification approach simply learns to detect the start and the end of text fragments to be extracted. It treats IE as a standard classification task, augmented with a simple mechanism to combine the predicted start and end tags. Experiments indicate that this approach generally has high precision but low recall. This approach can be 


Chunk 1150:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: gh precision but low recall. This approach can be viewed as that of one-level boundary classification (Finn, 2004). Many approaches can be used to training the classification models, for example, Support Vector Machines (Vapnik, 1998), Maximum Entropy (Berger, 1996), Adaboost (Shapire, 1999), and Voted Perceptron (Collins, 2002). Enhancing IE by a two-level boundary classification model Experiments on many data sets and in several real-world applications show that the one-level boundary classification appro


Chunk 1151:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 83, 'num_chars': 512}
Text: w that the one-level boundary classification approach can competitive with the start-of-the-art rule learning based IE systems. However, as the classifiers are built on a very large number of negative instances and a small number of positive instances, the prior probability that an arbitrary instance is a boundary is very small. This gives a model that has very high precision. Because the prior probability of predicting a tag is so low, and because the data is highly imbalanced, when we actually do predicti


Chunk 1152:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: is highly imbalanced, when we actually do prediction for a tag, it is very likely that the prediction is correct. The one-level model is therefore much more likely to produce false negatives than false positives (high precision). To overcome the problem, a two-level boundary classification approach has been proposed by (Finn, 2004). The intuition behind the two-level approach is as follows. At the first level, the start and end classifiers have high precision. To make a prediction, both the start classifier


Chunk 1153:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text: n. To make a prediction, both the start classifier and the end classifier have to predict the start and end respectively. In many cases where we fail to extract a fragment, one of these classifiers made a prediction, but not the other. The second level assumes that these predictions by the first level are correct and is designed to identify the starts and ends that we failed to identify at the first level. The second-level models are learned from training data in which the prior probability that a given ins


Chunk 1154:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: ta in which the prior probability that a given instance is a boundary is much higher than for the one-level learner. This “focused” training data is constructed as follows. When building the second-level start model, we take only the instances that occur a fixed distance before an end tag. Similarly, for the second-level end model, we use only instances that occur a fixed distance after a start tag. For example, an second-level window of size 10 means that the second-level start model is built using only 10


Chunk 1155:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text: he second-level start model is built using only 10 instances that occur before an end-tag in the training data, while the second-level end model is built using only those instances that occur in the 10 instances after a start tag in the training data. Note that these second-level instances are encoded in the same way as for the first-level; the difference is simply that the second-level learner is only allowed to look at a small subset of the available training data. Figure 6 shows an example of the IE usin


Chunk 1156:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: ing data. Figure 6 shows an example of the IE using the two-level classification models. In the example of Figure 6, there are also two stages: learning and extracting. In learning, the tokens “Dr.” and “Trinkle’s” are the start and the end boundaries of a speaker respectively. For training the second-level start and end classifiers. We use 12
window size as three and thus three instances after the start are used to train the end classifier and three instances before the end are used to train the start clas


Chunk 1157:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: es before the end are used to train the start classifier. In the example, the three tokens “Trinkle’s”, “primary”, and “research” are instances of the second-level end classifier and the token “Dr.” is an instance of the second-level start classifier. Note, in this way, the instances used for training the second-level classifiers are only a subset of the instances for training the first-level classifiers. These second-level instances are encoded in the same way as for the first-level. When extracting, the s


Chunk 1158:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: way as for the first-level. When extracting, the second-level end classifier is only applied to the three tokens following the token which the first-level classifier predicted as a start and the token itself. Similarly the second-level start classifier is only applied to instances predicted as an end by the first-level classifier and the three preceding tokens. Instances of the end classifier The second-level start classifier Start Dr. Trinkle's primary research interests lie in the areas of robotic manipul


Chunk 1159:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: arch interests lie in the areas of robotic manipulation Not start The second-level End end classifier Instances of the Learning start classifier Not end Extracting Identified as start by the first-level start classifier Professor Steve Skiena will be at CMU Monday, January 13, and Tuesday, February 14. Predict using the second- level end classifier Figure 6. Example of Information Extraction by the two-level classification models In the exacting stage of the example, the token “Professor” is predicted as a 


Chunk 1160:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text:  example, the token “Professor” is predicted as a start by the first-level start classifier and no token is predicted as the end in the first-level model. Then we can use the second-level end classifier to make prediction for the three following tokens. This second-level classification models are likely to have much higher recall but lower precision. If we were to blindly apply the second-level models to the entire document, it would generate a lot of false positives. Therefore, the reason we can use the se


Chunk 1161:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: positives. Therefore, the reason we can use the second-level models to improve performance is that we only apply it to regions of documents where the first-level models have made a prediction. Specifically, during extraction, the second-level classifiers use the predictions of the first-level models to identify parts of the document that are predicted to contain targets. Figure 7 shows the extracting processing flow in the two-level classification approach. Given a set of documents that we want to extract f


Chunk 1162:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: Given a set of documents that we want to extract from, we convert these documents into a set of instances and then apply the first-level models for start and end to the instances and generate a set of predictions for starts and ends. The first-level predictions are then used to guide which instances we need apply the second-level classifiers to. We use the predictions of the first-level end model to decide which instances to apply the second-level start model to, and we use the predictions of the first-leve


Chunk 1163:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text: l to, and we use the predictions of the first-level start model to decide which instances to apply the second-level end model to. Applying the second-level models to the selected instances gives us a set of predictions which we pass to the combination to output our extracted 13
results. The intuition behind the two-level approach is that we use the unmatched first-level predictions (i.e. when we identify either the start or the end but not the other) as a guide to areas of text that we should look more clos


Chunk 1164:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text: ide to areas of text that we should look more closely at. We use more focused classifiers that are more likely to make a prediction on areas of text where it is highly likely that an unidentified fragment exists. These classifiers are more likely to make predictions due to a much lower data imbalance so they are only applied to instances where we have high probability of a fragment existing. As the level of imbalance falls, the recall of the model rises while precision falls. We use the second-level classif


Chunk 1165:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: e precision falls. We use the second-level classifiers to lookahead/lookback instances in a fixed windows size and obtain a subset of the instances in the first-level classifier. Extraction results Prediction results combination Start predictions of End predictions of second second level level Start Model of End Model of second level second level Start predictions of first End predictions of first level level Start Model of End Model of first first level level The first-level start The first-level end class


Chunk 1166:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: el The first-level start The first-level end classifier classifier Test data Figure 7. Extracting processing flow in the two-level classification approach This enables us to improve recall without hurting precision by identifying the missing complementary tags for orphan predictions. If we have 100% precision at first-level prediction then we can improve recall without any corresponding drop in precision. In practice, the drop in precision is proportional to the number of incorrect prediction at the first-l


Chunk 1167:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text:  the number of incorrect prediction at the first-level classification. Enhancing IE by unbalance classification model Besides the two-level boundary classification approach, we introduce another approach to deal with the problem so as to improve performance of the classification based method. As the classifiers are built on a very large number of negative instances and a small number of positive instances, the prior probability that an arbitrary instance is a boundary is very small. This gives a model that 


Chunk 1168:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 83, 'num_chars': 512}
Text: a boundary is very small. This gives a model that has very high precision, but low recall. In this section, we introduce an approach to the problem using an unbalanced classification model. The basic idea of the approach is to design a specific 14
classification method that is able to learn a better classifier on the unbalanced data. We have investigated the unbalanced classification model of SVMs (Support Vector Machines). Using the same notations in Section 2.2.1, we have the unbalanced classification mod


Chunk 1169:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 92, 'num_chars': 512}
Text: n 2.2.1, we have the unbalanced classification model: (4) 1 n+ n− Minimize: wTw+C ∑ξ+C ∑ξ 2 1 i 2 i i=1 i=1 s.t.:y (wTx +b)≥1−ξ,i=1,2,...,n i i i here, C and C are two cost parameters used to control the training errors of positive 1 2 examples and negative examples respectively. For example, with a larger C and a 1 small C , we can obtain a classification model that attempts penalize false positive 2 examples more than false negative examples. The model can actually increase the probability of examples to 


Chunk 1170:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text:  actually increase the probability of examples to be predicted as positive, so that we can improve the recall while likely hurting the precision, which is consistent with the method of two-level classification. Intuition shows that in this way we can control the trade-off between the problem of high precision and low recall and the training errors of the classification model. The model obtained by this formulation can perform better than the classical SVM model in the case of a large number of negative inst


Chunk 1171:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: del in the case of a large number of negative instances and a small number of positive instances. To distinguish this formulation from the classical SVM, we call the special formulation of SVM as Unbalanced-SVM. See also (Morik, 1999; Li, 2003) for details. Unbalanced-SVM enables us to improve the recall by adjusting the two parameters. We need to note that the special case of SVM might hurt the precision while improving the recall. The most advantage of the model is that it can achieve a better trade-off b


Chunk 1172:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text:  model is that it can achieve a better trade-off between precision and recall. Sequential Labeling based Extraction Methods Information extraction can be cast as a task of sequential labeling. In sequential labeling, a document is viewed as a sequence of tokens, and a sequence of labels are assigned to each token to indicate the property of the token. For example, consider the nature language processing task of labeling words of a sentence with their corresponding Part-Of-Speech (POS). In this task, each wo


Chunk 1173:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text: onding Part-Of-Speech (POS). In this task, each word is labeled with a tag indicating its appropriate POS. Thus the inputting sentence “Pierre Vinken will join the board as a nonexecutive director Nov. 29.” will result in an output as: [NNP Pierre] [NNP Vinken] [MD will] [VB join] [DT the] [NN board] [IN as] [DT a] [JJ nonexecutive] [NN director] [NNP Nov.] [CD 29] [. .] Formally, given an observation sequence x = (x , x ,..., x ), the information 1 2 n extraction task as sequential labeling is to find a la


Chunk 1174:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: action task as sequential labeling is to find a label sequence y* = (y , y ,..., y ) 1 2 n that maximizes the conditional probability p(y|x), i.e., y* = argmax p(y|x) (5) y Different from the rule learning and the classification based methods, sequential labeling enables describing the dependencies between target information. The dependencies can be utilized to improve the accuracy of the extraction. Hidden 15
Markov Model (Ghahramani, 1997), Maximum Entropy Markov Model (McCallum, 2000), and Conditional Ra


Chunk 1175:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 90, 'num_chars': 512}
Text:  Markov Model (McCallum, 2000), and Conditional Random Field (Lafferty, 2001) are widely used sequential labeling models. For example, a discrete Hidden Markov Model is defined by a set of output symbols X (e.g. a set of words in the above example), a set of states Y (e.g. a set of POS in the above example), a set of probabilities for transitions between the states p(y|y), and a probability distribution on output symbols for each state p(x|y). An i j i i observed sampling of the process (i.e. the sequence o


Chunk 1176:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 83, 'num_chars': 512}
Text: erved sampling of the process (i.e. the sequence of output symbols, e.g. “Pierre Vinken will join the board as a nonexecutive director Nov. 29.” in the above example) is produced by starting from some initial state, transitioning from it to another state, sampling from the output distribution at that state, and then repeating these latter two steps. The best label sequence can be found using Viterbi algorithm. Generative model Generative models define a joint probability distribution p(X, Y) where X and Y a


Chunk 1177:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: t probability distribution p(X, Y) where X and Y are random variables respectively ranging over observation sequences and their corresponding label sequences. In order to calculate the conditional probability p(y|x), Bayesian rule is employed: p(x,y) y*=argmax p(y|x)=argmax (6) y y p(x) Hidden Markov Models (HMMs) (Ghahramani, 1997) are one of the most common generative models currently used. In HMMs, each observation sequence is considered to have been generated by a sequence of state transitions, beginnin


Chunk 1178:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: rated by a sequence of state transitions, beginning in some start state and ending when some pre-designated final state is reached. At each state an element of the observation sequence is stochastically generated, before moving to the next state. In the case of POS tagging, each state of the HMM is associated with a POS tag. Although POS tags do not generate words, the tag associated with any given word can be considered to account for that word in some fashion. It is, therefore, possible to find the sequen


Chunk 1179:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 86, 'num_chars': 512}
Text: ion. It is, therefore, possible to find the sequence of POS tags that best accounts for any given sentence by identifying the sequence of states most likely to have been traversed when “generating” that sequence of words. The states in an HMM are considered to be hidden because of the doubly stochastic nature of the process described by the model. For any observation sequence, the sequence of states that best accounts for that observation sequence is essentially hidden from an observer and can only be viewe


Chunk 1180:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: ally hidden from an observer and can only be viewed through the set of stochastic processes that generate an observation sequence. The principle of identifying the most state sequence that best accounts for an observation sequence forms the foundation underlying the use of finite-state models for labeling sequential data. Formally, an HMM is fully defined by (cid:122) A finite set of states Y. (cid:122) A finite output alphabet X. (cid:122) A conditional distribution p(y’|y) representing the probability of 


Chunk 1181:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: tribution p(y’|y) representing the probability of moving from state y to state y’, where y, y’∈Y. 16
(cid:122) An observation probability distribution p(x|y) representing the probability of emitting observation x when in state y, where x∈X, y∈Y. (cid:122) An initial state distribution p(y), y∈Y. From the definition of HMMs, we can see that the probability of the state at time t depends only on the state at time t-1, and the observation generated at time t only depends on the state of the model at time t. Fi


Chunk 1182:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text: ly depends on the state of the model at time t. Figure 8 shows the structure of a HMM. Figure 8. Graphic structure of first-order HMMs These conditional independence relations, combined with the probability chain rule, can be used to factorize the joint distribution over a state sequence y and observation sequence x into the product of a set of conditional probabilities: n p(y,x)= p(y )p(x | y )∏ p(y | y )p(x | y ) (7) 1 1 1 t t−1 t t t=2 In supervised learning, the conditional probability distribution p(y|


Chunk 1183:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: ing, the conditional probability distribution p(y|y ) and t t-1 observation probability distribution p(x|y) can be gained with maximum likelihood. While in unsupervised learning, there is no analytic method to gain the distributions directly. Instead, Expectation Maximization (EM) algorithm is employed to estimate the distributions. Finding the optimal state sequence can be efficiently performed using a dynamic programming such as Viterbi algorithm. Limitations of generative models. Generative models define


Chunk 1184:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: ons of generative models. Generative models define a joint probability distribution p(X, Y) over observation and label sequences. This is useful if the trained model is to be used to generate data. However, to define a joint probability over observation and label sequences, a generative model needs to enumerate all possible observation sequences, typically requiring a representation in which observations are task-appropriate atomic entities, such as words or nucleotides. In particular, it is not practical t


Chunk 1185:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text:  nucleotides. In particular, it is not practical to represent multiple interacting features or long-range dependencies of the observations, since the inference problem for such models is intractable. Therefore, generative models must make strict independence assumptions in order to make inference tractable. In the case of an HMM, the observation at time t is assumed to depend only on the state at time t, ensuring that each observation element is treated as an isolated unit, independent from all other elemen


Chunk 1186:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: n isolated unit, independent from all other elements in the sequence (Wallach, 2002). In fact, most sequential data cannot be accurately represented as a set of isolated elements. Such data contain long-distance dependencies between observation elements and benefit from being represented in by a model that allows such dependencies and enables observation sequences to be represented by non-independent overlapping features. For example, in the POS task, when tagging a word, information such as the words surro


Chunk 1187:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: agging a word, information such as the words surrounding the current word, the previous tag, 17
whether the word begins with a capital character, can be used as complex features and help to improve the tagging performance. Discriminative models provide a convenient way to overcome the strong independence assumption of generative models. Discriminative models Instead of modeling joint probability distribution over observation and label sequences, discriminative models define a conditional distribution p(y|x)


Chunk 1188:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: ve models define a conditional distribution p(y|x) over observation and label sequences. This means that when identifying the most likely label sequence for a given observation sequence, discriminative models use the conditional distribution directly, without bothering to make any dependence assumption on observations or enumerate all the possible observation sequences to calculate the marginal probability p(x). Maximum Entropy Markov Models (MEMMs) MEMMs (McCallum, 2000) are a form of discriminative models


Chunk 1189:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text: cCallum, 2000) are a form of discriminative models for labeling sequential data. MEMMs consider observation sequences to be conditioned upon rather than generated by the label sequence. Therefore, instead of defining two types of distribution, a MEMM has only a single set of separately trained distributions of the form: p(y'| x)= p(y'| y,x) (8) which represent the probability of moving from state y to y’ on observation x. The fact the each of these functions is specific to a given state means that the choic


Chunk 1190:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 97, 'num_chars': 512}
Text:  is specific to a given state means that the choice of possible states at any given instant in time t+1 depends only on the state of the model at time t. Figure 9 show the graphic structure of MEMMs. Figure 9. Graphic structure of first-order MEMMs Given an observation sequence x, the conditional probability over label sequence y is given by n p(y| x)= p(y | x )∏ p(y | y ,x ) (9) 1 1 t t−1 t−1 t=2 Treating observations as events to be conditioned upon rather than generated means that the probability of each


Chunk 1191:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text:  than generated means that the probability of each transition may depend on non-independent, interacting features of the observation sequence. Making use of maximum entropy frame work and defining each state-observation transition function to be a log-linear model, equation (8) can be calculated as 18
1 p(y'|x)= exp(∑λ f (y',x)) (10) Z(y,x) k k k where Z(y, x) is a normalization factor; λ are parameters to be estimated and f are k k feature functions. The parameters can be estimated using Generalized Iterat


Chunk 1192:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: rameters can be estimated using Generalized Iterative Scaling (GIS) (McCallum, 2000). Each feature function can be represented as a binary feature. For example: ⎧1 if b(x) is true and y = y' f(y',x)=⎨ (11) ⎩0 otherwise Despite the differences between MEMMs and HMMs, there is still an efficient dynamic programming solution to the classic problem of identifying the most likely label sequence given an observation sequence. A variant Viterbi algorithm is given by (McCallum, 2000). Label bias problem. Maximum En


Chunk 1193:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: y (McCallum, 2000). Label bias problem. Maximum Entropy Markov Models define a set of separately trained per-state probability distributions. This leads to an undesirable behavior in some situations, named label bias problem (Lafferty, 2001). Here we use an example to describe the label bias problem. The MEMM in Figure 10 is designed to shallow parse the sentences: (1) The robot wheels Fred round. (2) The robot wheels are round. Figure 10. MEMM designed for shallow parsing Consider when shallow parsing the 


Chunk 1194:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: shallow parsing Consider when shallow parsing the sentence (1). Because there is only one outgoing transition from state 3 and 6, the per-state normalization requires that p(4|3, Fred) = p(7|6, are) = 1. Also it’s easy to obtain that p(8|7, round) = p(5|4, round) = p(2|1, robot) = p(1|0, The) = 1, etc. Now, given p(3|2, wheels) = p(6|2, wheels) = 0.5, by combining all these factors, we obtain p(0123459|The robot wheels Fred round.) = 0.5, p(0126789|The robot wheels Fred round.) = 0.5. Thus the MEMM ends up 


Chunk 1195:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text:  wheels Fred round.) = 0.5. Thus the MEMM ends up with two possible state sequences 0123459 and 0126789 with the same probability independently of the observation sequence. It’s impossible for the MEMM to tell which one is the most likely state sequence over the given sentence. Likewise, given p(3|2, wheels) < p(6|2, wheels), MEMM will always choose the bottom path despite what the preceding words and the following words are in the observation sequence. The label bias problem occurs because a MEMM uses per-


Chunk 1196:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 91, 'num_chars': 512}
Text: label bias problem occurs because a MEMM uses per-state exponential model for the conditional probability of the next states given the current state. 19
Conditional Random Fields (CRFs) CRFs are undirected graphical model trained to maximize a conditional probability. CRFs can be defined as follows: CRF Definition. Let G = (V, E) be a graph such that Y=(Y ) , so that Y is indexed v v∈V by the vertices of G. Then (X, Y) is a conditional random field in case, when conditioned on X, the random variable Y obey 


Chunk 1197:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 94, 'num_chars': 512}
Text: when conditioned on X, the random variable Y obey the Markov property with respect to v the graph: p(Y |X, Y , w≠v) = p(Y |X, Y , w∽v), where w∽v means that w and v are v w v w neighbors in G. A CRF is a random field globally conditioned on the observation X. Linear-chain CRFs were first introduced by Lafferty et al (2001). Figure 11 shows the graphic structure of the linear-chain CRFs. Figure 11. Graphic structure of linear-chain CRFs By the fundamental theorem of random fields (Harmmersley, 1971), the co


Chunk 1198:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 107, 'num_chars': 512}
Text: eorem of random fields (Harmmersley, 1971), the conditional distribution of the labels y given the observations data x has the form 1 T p (y|x)= exp(∑∑λ ⋅ f (y ,y ,x,t)) (12) λ Z (x) k k t−1 t λ t=1 k where Z (x) is the normalization factor, also known as partition function, which has λ the form T Z (x)=∑exp(∑∑λ ⋅ f (y ,y ,x,t)) (13) λ k k t−1 t y t=1 k where f (y , y, x, t) is a feature function which can be both real-valued and k t-1 t binary-valued. The feature functions can measure any aspect of a state


Chunk 1199:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 105, 'num_chars': 512}
Text: eature functions can measure any aspect of a state transition, y → y , and the observation sequence, x, centered at the current time step t. λ t−1 t k corresponds to the weight of the feature f . k The most probable labeling sequence for an input x y*=argmax p (y|x) (14) y λ can be efficiently calculated by dynamic programming using Viterbi algorithm. We can train the parameters λ=(λ , λ , ...) by maximizing the likelihood of a given 1 2 training set T ={(x ,y )}N : k k k=1 N T L = ∑(∑∑λ ⋅ f (y ,y ,x ,t)−lo


Chunk 1200:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 92, 'num_chars': 512}
Text: ,y )}N : k k k=1 N T L = ∑(∑∑λ ⋅ f (y ,y ,x ,t)−logZ (x )) (15) λ k k t−1 t i λ i i=1 t=1 k Many methods can be used to do the parameter estimation. The traditional 20
maximum entropy learning algorithms, such as GIS, IIS can be used to train CRFs (Darroch, 1972). In addition to the traditional methods, preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal, 1999) have been found to perform better than the traditional methods (Sha, 2004). The voted perceptro


Chunk 1201:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: aditional methods (Sha, 2004). The voted perceptron algorithm (Collins, 2002) can also be utilized to train the models efficiently and effectively. To avoid overfitting1, log-likelihood is often penalized by some prior distribution over the parameters. Empirical distributions such as Gaussian prior, exponential prior, and hyperbolic-L prior can be used, and empirical experiments suggest that Gaussian 1 prior is a safer prior to use in practice (Chen, 1999). CRF avoids the label bias problem because it has a


Chunk 1202:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: CRF avoids the label bias problem because it has a single exponential model for the conditional probability of the entire sequence of labels given the observation sequence. Therefore, the weights of different features at different states can be traded off against each other. Sequential labeling based extraction methods By casting information extraction as sequential labeling, a set of labels need to be defined first according to the extraction task. For example, in metadata extraction from research papers (


Chunk 1203:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: ple, in metadata extraction from research papers (Peng, 2004), labels such as TITLE, AUTHOR, EMAIL, and ABSTRACT are defined. A document is viewed as an observation sequence x. The observation unit can be a word, a text line, or any other unit. Then the task is to find a label sequence y that maximize the conditional probability p(y|x) using the models described above. In generative models, there is no other features can be utilized except the observation itself. Due to the conditional nature, discriminativ


Chunk 1204:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: self. Due to the conditional nature, discriminative models provide the flexibility of incorporating non-independent, arbitrary features as input to improve the performance. For example, in the task of metadata extraction from research papers, with CRFs we can use as features not only text content, but also layout and external lexicon. Empirical experiments show that the ability to incorporate non-independent, arbitrary features can significantly improve the performance. On the other hand, the ability to inc


Chunk 1205:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: performance. On the other hand, the ability to incorporate non-independent, arbitrary features of discriminative models may sometimes lead to too many features and some of the features are of little contributions to the model. A feature induction can be performed when training the model to obtain the features that are most useful for the model (McCallum, 2003). Non-linear Conditional Random Fields Conditional Random Fields (CRFs) are the state-of-the-art approaches in information extraction taking advantage


Chunk 1206:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 73, 'num_chars': 512}
Text: roaches in information extraction taking advantage of the dependencies to do better extraction, compared with HMMs (Ghahramani, 1997) and MEMMs (McCallum, 2000). However, the previous linear-chain CRFs only model the linear-dependencies in a sequence of information, and is not able to model the other kinds of dependencies (e.g. 21
non-linear dependencies) (Lafferty, 2001; Zhu, 2005). In this section, we will discuss several non-linear conditional random field models. Condition random fields for relational l


Chunk 1207:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: d models. Condition random fields for relational learning HMMs, MEMMs and linear-chain CRFs can only model dependencies between neighboring labels. But sometimes it is important to model certain kinds of long-range dependencies between entities. One important kind of dependency within information extraction occurs on repeated mentions of the same field. For example, when the same entity is mentioned more than once in a document, such as a person name Robert Booth, in many cases, all mentions have the same l


Chunk 1208:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: Booth, in many cases, all mentions have the same label, such as SEMINAR-SPEAKER. An IE system can take advantage of this fact by favoring labelings that treat repeated words identically, and by combining feature from all occurrences so that the extraction decision can be made based on global information. Furthermore, identifying all mentions of an entity can be useful in itself, because each mention might contain different useful information. The skip-chain CRF is proposed to address this (Sutton, 2005; Bun


Chunk 1209:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: CRF is proposed to address this (Sutton, 2005; Bunescu, 2005b). The skip-chain CRF is essentially a linear-chain CRF with additional long-distance edges between similar words. These additional edges are called skip edges. The features on skip edges can incorporate information from the context of both endpoints, so that strong evidence at one endpoint can influence the label at the other endpoint. Formally, the skip-chain CRF is defined as a general CRF with two clique templates: one for the linear-chain por


Chunk 1210:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 107, 'num_chars': 512}
Text: two clique templates: one for the linear-chain portion, and one for the skip edges. For an input x, let C ={(u,v)}be the set of all pairs of sequence positions for which there are skip edges. The probability of a label sequence y given an x is modeled as 1 T p (y|x)= exp(∑∑λ ⋅ f (y ,y ,x,t)+ ∑ ∑λ⋅ f (y ,y ,x,u,v)) (16) λ k k t−1 t l l u v Z(x) t=1 k (u,v)∈C l where Z(x) is the normalization factor, f is the feature function similar to that in k equation (12) and f is the feature function of the skip edges. 


Chunk 1211:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 86, 'num_chars': 512}
Text:  and f is the feature function of the skip edges. λ and λ are weights of l k l the two kinds of feature functions. Because the loops in a skip-chain CRF can be long and overlapping, exact inference is intractable for the data considered. The running time required by exact inference is exponential in the size of the largest clique in the graph’s junction tree. Instead, approximate inference using loopy belief propagation is performed, such as TRP (Wainwright, 2001). Richer kinds of long-distance factor than 


Chunk 1212:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text:  2001). Richer kinds of long-distance factor than just over pairs of words can be considered to augment the skip-chain model. These factors are useful for modeling exceptions to the assumption that similar words tend to have similar labels. For example, in named entity recognition, the word China is as a place name when it appears alone, but when it occurs within the phrase The China Daily, it should be labeled as an organization (Finkel, 2005). 22
2D CRFs for web information extraction Zhu et al (2005) pro


Chunk 1213:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: or web information extraction Zhu et al (2005) propose 2D Conditional Random Fields (2D CRFs). 2D CRFs are also a particular case of CRFs. They are aimed at extracting object information from two-dimensionally laid-out web pages. The graphic structure of a 2D CRF is a 2D grid, and it’s natural to model the 2D laid-out information. If viewing the state sequence on diagonal as a single state, a 2D CRF can be mapped to a linear-chain CRF, and thus the conditional distribution has the same form as a linear-chai


Chunk 1214:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: al distribution has the same form as a linear-chain CRF. Dynamic CRFs Sutton et al (2004) propose Dynamic Conditional Random Fields (DCRFs). As a particular case, a factorial CRF (FCRF) was used to jointly solve two NLP tasks (noun phrase chunking and Part-Of-Speech tagging) on the same observation sequence. Improved accuracy was obtained by modeling the dependencies between the two tasks. Tree-structure CRFs for information extraction We have investigated the problem of hierarchical information extraction 


Chunk 1215:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: he problem of hierarchical information extraction and propose Tree-structured Conditional Random Fields (TCRFs). TCRFs can incorporate dependencies across the hierarchically laid-out information. We here use an example to introduce the problem of hierarchical information extraction. Figure 12 (a) give an example document, in which the underlined text are what we want to extract including two telephone numbers and two addresses. The information can be organized as a tree structure (ref. Figure 12 (b)). In th


Chunk 1216:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: ed as a tree structure (ref. Figure 12 (b)). In this case, the existing linear-chain CRFs cannot model the hierarchical dependencies and thus cannot distinguish the office telephone number and the home telephone number from each other. Likewise for the office address and home address. Contact Information: John Booth Office: Tel: 8765-4321 Addr: F2, A building Home: Tel: 1234-5678 Addr: No. 123, B St. (a) Example document (b) Organized the document in tree-structure Figure 12. Example of tree-structured laid


Chunk 1217:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: ructure Figure 12. Example of tree-structured laid-out information To better incorporate dependencies across hierarchically laid-out information, we propose a Tree-structured Conditional Random Field (TCRF) model. We present the graphical structure of the TCRF model as a tree and reformulate the conditional distribution by defining three kinds of edge features respectively representing the parent-child dependency, child-parent dependency, and sibling dependency. As the tree structure can be cyclable, exact 


Chunk 1218:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: ncy. As the tree structure can be cyclable, exact inference in TCRFs is expensive. We propose to use the Tree-based Reparameterization (TRP) algorithm (Wainwright, 2001) to 23
compute the approximate marginal probabilities for edges and vertices. We conducted experiments on company annual reports collected from Shang Stock Exchange. On the annual reports we defined ten extraction tasks. Experimental results indicate that the TCRFs can significantly outperform the existing linear-chain CRF model (+7.67% in t


Chunk 1219:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: m the existing linear-chain CRF model (+7.67% in terms of F1-measure) for hierarchical information extraction. See (Tang, 2006b) for details. APPLICATIONS In this section, we introduce several extraction applications that we experienced. We will also introduce some well-known applications in this area. Information Extraction in Digital Libraries In digital libraries (DL), “metadata” is structured data for helping users find and process documents and images. With the metadata information, search engines can 


Chunk 1220:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: With the metadata information, search engines can retrieve required documents more accurately. Scientists and librarians need use greatly manual efforts and lots of time to create metadata for the documents. To alleviate the hard labor, many efforts have been made toward the automatic metadata generation based on information extraction. Here we take Citeseer, a popular scientific literature digital library, as an example in our explanation. Citeseer is a public specialty scientific and academic DL that was 


Chunk 1221:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: lic specialty scientific and academic DL that was created in NEC Labs, which is hosted on the World Wide Web at the College of Information Sciences and Technology, The Pennsylvania State University, and has over 700,000 documents, primarily in the fields of computer and information science and engineering (Lawrence, 1999; Han, 2003). Citeseer crawls and harvests documents on the web, extracts documents metadata automatically, and indexes the metadata to permit querying by metadata. By extending Dublin Core 


Chunk 1222:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: it querying by metadata. By extending Dublin Core metadata standard, Citeseer defines 15 different meta-tags for the document header, including Title, Author, Affiliation, and so on. They view the task of automatic document metadata generation as that of labeling the text with the corresponding meta-tags. Each meta-tag corresponds to a metadata class. The extraction task is cast as a classification problem and SVM is employed to perform the classification. They show that classifying each text line into one 


Chunk 1223:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: hey show that classifying each text line into one or more classes is more efficient for meta-tagging than classifying each word, and decompose the metadata extraction problem into two sub-problems: (1) line classification and (2) chunk identification of multi-class lines. In line classification, both word and line-specific features are used. Each line is represented by a set of word and line-specific features. A rule-based, context-dependent word clustering method is developed to overcome the problem of wor


Chunk 1224:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: method is developed to overcome the problem of word sparseness. For example, an author line “Chungki Lee James E. Burns” is represented as “CapNonDictWord: :MayName: :MayName: : SingleCap: :MayName:”, after word clustering. The weight of a word-specific feature 24
is the number of times this feature appears in the line. And line-specific features are features such as “Number of the words in the line”, “The position of the line”, “The percentage of dictionary words in the line”, and so on. The classification


Chunk 1225:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text:  words in the line”, and so on. The classification process is performed in two steps, an independent line classification followed by an iterative contextual line classification. Independent line classification use the features described above to assign one or more classes to each text line. After that, by making use of the sequential information among lines output by the first step, an iterative contextual line classification is performed. In each iteration, each line uses the previous N and next N lines’ c


Chunk 1226:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text:  each line uses the previous N and next N lines’ class information as features, concatenates them to the feature vector used in step one, and updates its class label. The procedure converges when the percentage of line with new class labels is lower than a threshold. The principle of the classification based method is the Two-level boundary classification approach as described in Section 2.2.3. After classifying each line into one or more classes, meta-tag can be assigned to lines that have only one class l


Chunk 1227:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: an be assigned to lines that have only one class label. For those that have more than one class label, a further identification is employed to extract metadata from each line. The task is cast as a chunk identification task. Punctuation marks and spaces between words are considered candidate chunk boundaries. A two-class chunk identification algorithm for this task was developed and it yields an accuracy of 75.5%. For lines that have more than two class labels, they are simplified to two-class chunk identif


Chunk 1228:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: ls, they are simplified to two-class chunk identification tasks by detecting natural chunk boundary. For instance, using the positions of email and URL in the line, the three-class chunk identification can be simplified as two-class chunk identification task. The position of the email address in the following three-class line “International Computer Science Institute, Berkeley, CA94704. Email: aberer@icsi.berkeley.edu.” is a natural chunk boundary between the other two classes. The method obtains an overall


Chunk 1229:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: e other two classes. The method obtains an overall accuracy of 92.9%. It’s adopted in the DL Citeseer and EbizSearch for automatic metadata extraction. It can be also generalized to other DL. See (Lawrence, 1999; Han, 2003) for details. Information Extraction from Emails We also make use of information extraction methods to email data (Tang, 2005a). Email is one of the commonest means for communication via text. It is estimated that an average computer user receives 40 to 50 emails per day (Ducheneaut, 2001


Chunk 1230:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: receives 40 to 50 emails per day (Ducheneaut, 2001). Many text mining applications need take emails as inputs, for example, email analysis, email routing, email filtering, information extraction from email, and newsgroup analysis. Unfortunately, information extraction from email has received little attention in the research community. Email data can contain different types of information. Specifically, it may contain headers, signatures, quotations, and text content. Furthermore, the text content may have p


Chunk 1231:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text:  content. Furthermore, the text content may have program codes, lists, and paragraphs; the header may have metadata information such as sender, receiver, subject, etc.; and the signature may have metadata information such as author name, author’s position, author’s address, etc. In this work, we formalize information extraction from email as that of text-block 25
detection and block-metadata detection. Specifically, the problem is defined as a process of detection of different types of informative blocks (i


Chunk 1232:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: ection of different types of informative blocks (it includes header, signature, quotation, program code, list, and paragraph detections) and detection of block-metadata (it includes metadata detection of header and metadata detection of signature). We propose to conduct email extraction in a ‘cascaded’ fashion. In the approach, we perform the extraction on an email by running several passes of processing on it: first at email body level (text-block detection), next at text-content level (paragraph detection


Chunk 1233:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: ), next at text-content level (paragraph detection), and then at block levels (header-metadata detection and signature-metadata detection). We view the tasks as classification and propose a unified statistical learning approach to the tasks, based on SVMs (Support Vector Machines). Features used in the models have also been defined. See (Tang, 2005a) for details. 1. From: SY <sandeep....@gmail.com> - Find messages by this author 2. Date: Mon, 4 Apr 2005 11:29:28 +0530 3. Subject: Re: ..How to do addition?? 


Chunk 1234:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 110, 'num_chars': 512}
Text: 9:28 +0530 3. Subject: Re: ..How to do addition?? 4. Hi Ranger, 5. Your design of Matrix 6. class is not good. From: SY <sandeep....@gmail.com> - Find messages by this author Source 7. what are you doing with two Date: Mon, 4 Apr 2005 11:29:28 +0530 SentTime Header 8. matrices in a single class?make class Matrix as follows Subject: Re: ..How to do addition?? Subject Hi Ranger, Paragraph 9 10. . i m clp ao ssrt Mja av ta r. ii xo . {* ; Y wo itu hr t wde os i mgn a to rif c M esa it nr i ax sc il na gss le i


Chunk 1235:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 111, 'num_chars': 512}
Text:  mgn a to rif c M esa it nr i ax sc il na gss le i s c ln ao sst ? m ag ko eo cd l. a w ssh Mat aa tr re ix y ao su fd oo lli on wg s Paragraph Text Content 11. public static int AnumberOfRows; 12. public static int AnumberOfColumns; i c pm l ua bp s lso i cr Mt sj a ta atv r tia i cx.i io { n. * t ; A numberOfRows; Email 13. public void inputArray() throws IOException public static int AnumberOfColumns; 1 14 5. . { InputStreamReader input = new InputStreamReader(System.in); public void inputArray() throws 


Chunk 1236:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text: eader(System.in); public void inputArray() throws IOException Program Code 16. BufferedReader keyboardInput = new BufferedReader(input) { 17. } I Bn up fu ft eS retr de Ram eaR de ea r d ke er y i bn op au rt d = In n pe uw t = I n np eu wtS Btr ue fa fm erR ede Rad ee ar d( eS ry (is nte pm ut. )in); } 18. -- Sandeep Yadav 19. Tel: 011-243600808 -- Sandeep Yadav AuthorName 20. Homepage: http://www.it.com/~Sandeep/ Tel: 011-243600808 Telephone Signature Homepage: http://www.it.com/~Sandeep/ Homepage 21. On 


Chunk 1237:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 95, 'num_chars': 512}
Text: page: http://www.it.com/~Sandeep/ Homepage 21. On Apr 3, 2005 5:33 PM, ranger <asiri....@gmail.com> wrote: On Apr 3, 2005 5:33 PM, ranger <asiri....@gmail.com> wrote: Forwarded Message 22. > Hi... I want to perform the addtion in my Matrix class. I got the program to > Hi... I want to perform the addtion in my Matrix class. I got the program to 23. > enter 2 Matricx and diaplay them. Hear is the code of the Matrix class and > enter 2 Matricx and diaplay them. Hear is the code of the Matrix class and 24. > T


Chunk 1238:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 102, 'num_chars': 512}
Text: . Hear is the code of the Matrix class and 24. > TestMatrix class. I'm glad If anyone can let me know how to do the addition Tnx > TestMatrix class. I'm glad If anyone can let me know how to do the addition Tnx Figure 13. Example of email Figure 14. Annotation results of the email message message Figure 13 shows an example of email that includes many typical information. Lines from 1 to 3 are a header; lines from 18 to 20 are a signature; and a forwarded message lies from line 21 to line 24. Lines from 4 to


Chunk 1239:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 93, 'num_chars': 512}
Text: sage lies from line 21 to line 24. Lines from 4 to 8 are the actual text content, which should be two paragraphs, but is mistakenly separated by extra line breaks. Moreover, the header has a sender (line 1), a sent time (line 2), and a subject (line 3); the signature has an author name (line 18), a telephone (line 19), and a homepage (line 20). Figure 14 shows an ideal result of information extraction on the email in Figure 13. Within it, the text-blocks (the header, signature and the forwarded message) hav


Chunk 1240:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: e header, signature and the forwarded message) have been identified. The actual text content has been detected. In the text content, extra line breaks have been detected and the text has been annotated as two paragraphs. Metadata information is recognized in the identified header and the identified signature. We propose a cascaded approach for information extraction from email and cast the extraction tasks as detection tasks of different types of information blocks. We employ a unified machine learning appr


Chunk 1241:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text:  blocks. We employ a unified machine learning approach in the detection tasks. The input is an email message. The implementation carries out extraction in the following steps. The identified text-blocks and other extraction results in each step will be saved for use in the later steps. 26
(1) Preprocessing. It uses patterns to recognize ‘special words’, including email address, IP address, URL, date, file directory, number (e.g. 5.42), money (e.g. $100), percentage (e.g. 92.86%), words containing special sy


Chunk 1242:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: centage (e.g. 92.86%), words containing special symbols (e.g. C#, .NET, .doc). (2) Forwarded message detection. It identifies forwarded messages using hard-coded rules. It views lines starting with special characters (e.g. >, |, >>) as forwarded messages. It then eliminates the identified forwarded messages for later processing. (3) Header and signature detection. It detects the header and signature (if there exist) in the email by using a classification model. It next eliminates the identified blocks (head


Chunk 1243:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: el. It next eliminates the identified blocks (headers and signatures). (4) Metadata detection in header and signature. It uses the identified headers and signatures as input and then detects the metadata information from the headers and signatures, respectively. (5) List and program code detection. It detects list and program code (if there exist) in the email with the same approach as that in header and signature detection and removes them from the text content. After that, only natural language text remai


Chunk 1244:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: tent. After that, only natural language text remains. (6) Paragraph annotation. It identifies whether or not each line break is a paragraph ending by using a classification model. If not, it removes the line break. As a result, the text is segmented into paragraphs. The step is based on paragraph ending detection. We make use of Support Vector Machines (SVM) as the classification model (Vapnik, 1998). We use SVM-light, which is available at http://svmlight.joachims.org/. We obtain high performances in all d


Chunk 1245:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: oachims.org/. We obtain high performances in all detection tasks. (The F1-measuer scores range from 89.83% to 97.17%.) The extracted information from email is applied to applications of email data cleaning (Tang, 2005a) and email classification. In email data cleaning, we try to remove ‘noisy’ (irrelevant) blocks for a specific application (e.g. term extraction, a task in which base noun phrases are extracted from documents) and transform relevant text into a canonical form as that in a newspaper article. F


Chunk 1246:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: a canonical form as that in a newspaper article. For term extraction, we identify and remove the header, signature, program code, and forwarded message. We view the remaining text as the relevant text. In the relevant text, we identify and remove extra line breaks, remove extra punctuations, and restore badly cased words. Experimental results show that the extraction based email cleaning can significantly improve the accuracy of term extraction. The improvements on precision range from +49.90% to +71.15%. S


Chunk 1247:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: ents on precision range from +49.90% to +71.15%. See (Tang, 2005a) for details. In email classification, we are aimed at taking advantage of the extracted information to improve the performance of email classification. We evaluated the classification results on Enron Email Dataset, which is available at http://www.cs.umass.edu/~ronb/enron_dataset.html. Experimental results show that the classification performance can be significantly improved (averagely +49.02% in terms of F1-measure) by making use of the e


Chunk 1248:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: 02% in terms of F1-measure) by making use of the extraction results from emails. The related issues are what we are currently researching, and will be reported elsewhere. 27
Person Profile Extraction Person information management is an important topic in both research community and industrial community. A person can have different types of information: person profile (including portrait, homepage, position, affiliation, publications, and documents), contact information (including address, email, telephone, 


Chunk 1249:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: information (including address, email, telephone, and fax number), and social network information (including person or professional relationships between persons, e.g. friend relationship). However, the information is usually hidden in heterogeneous and distributed web pages. We have investigated the problem of person information extraction. We have found that the person information is mainly hidden in person homepage, person introduction page (web page that introduces the person), person list (e.g. a facul


Chunk 1250:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text:  introduces the person), person list (e.g. a faculty list), and email message (e.g. in signature). We employed the classification based method to extract the person information from the different types of web pages. More specifically, in extraction we convert a web page into a token sequence (the token can be word, punctuation, and space). Then we view each token as a candidate and define features for each candidate. Due to space limitation, we omit the details of the feature definition. After that, we use 


Chunk 1251:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: ils of the feature definition. After that, we use two classification models to respectively identify whether a token is the start position and whether the token is the end position for each type of information. We next view the tokens between the start token and the end token as the target. We can also use the text-line as candidate in extraction. (a) (b) Figure 15. Personal Network Search system For learning the classification models, we have human annotators conduct annotation on the web pages. We also co


Chunk 1252:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: rs conduct annotation on the web pages. We also convert the web page into a token sequence and view each token as the candidate. Features are defined for each candidate. Finally, we learn two classification models respectively for the start position identification and the end position identification for each type of information. 28
As models, we use SVMs (Support Vector Machines) (Vapnik, 1998). Features are defined in the SVM models respectively for each type of the information. The average F1-measure obta


Chunk 1253:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: pe of the information. The average F1-measure obtained in extraction is 91.18%. We have developed a system based on the extracted person information, which is called ‘Personal Network Search’ (PNS shortly). In PNS, the user inputs a person name, and the system returns the information of the person. Given a person name, we first utilize Google API to get a list of relevant documents. Then a classification model is employed to identify whether or not a document in the list is really ‘related’ to the person. N


Chunk 1254:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: t in the list is really ‘related’ to the person. Next, we extract person information from the identified documents using the classification based method as described above. Figure 15 shows the snapshots of the PNS system. In Figure 15 (a), the user types a person name, and he gets a detailed description of the person. Figure 15 (b) shows the list of gathered persons in our current system. See (Tang, 2006a) for details. Table Extraction Using Conditional Random Fields Tables — textual tokens laid out in tabu


Chunk 1255:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 83, 'num_chars': 512}
Text: om Fields Tables — textual tokens laid out in tabular form — are often used to compactly communicate information in fields and records. They have been described as “databases designed for human eyes”. Tables appear in the earliest writing on clay tablets, and in the most modern Web pages. Some make use of line-art, while others rely on white space only. They sometimes consist merely of two simple columns, other times of extremely baroque collections of headings, embedded subheadings, and varying cell sizes.


Chunk 1256:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: ngs, embedded subheadings, and varying cell sizes. They are used in everything from government reports, to magazine articles, to academic publications. Pinto and McCallum (2003) propose a model of table extraction that richly integrates evidence from both content and layout by using Conditional Random Fields (CRFs). They describe a method that simultaneously locates tables in plain-text government statistical reports, and labels each of their constituent lines with tags such as header, sub-header, data, sep


Chunk 1257:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: es with tags such as header, sub-header, data, separator, etc. The features measure aspects of the input stream such as the percentage of alphabetic characters, the presence of regular expression matching months or years, and the degree to which white space in the current line aligns with white space in the previous line. In experiments on government reports, tables are located with 92% in terms of F1-measure, and lines are labeled with 94% accuracy — reducing error by 80% over a similarly configured hidden


Chunk 1258:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: ng error by 80% over a similarly configured hidden Markov model with the same features. See (Pinto, 2003) for details. See also (Wang, 2002). Shallow Parsing with Conditional Random Fields Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallow parsing problem is NP chunking, which finds the non-recursive cores of noun phrases called base NPs. The pioneering work of (Ramshaw,


Chunk 1259:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text:  called base NPs. The pioneering work of (Ramshaw, 1995) introduced NP chunking as a machine-learning problem, with standard datasets and 29
evaluation metrics. The task was extended to additional phrase types for the CoNLL-2000 shared task (Tjong Kim Sang, 2000), which is now the standard evaluation task for shallow parsing. Sha et al (2003) employ Conditional Random Fields (CRFs) into shallow parsing. They carried out an empirical study on different sequential labeling approaches in shallow parsing. Their


Chunk 1260:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: tial labeling approaches in shallow parsing. Their experimental results show that CRFs outperform all reported single-model NP chunking results on the standard evaluation dataset. They also compared different kinds of parameter estimation methods for training CRF models that confirm and strengthen previous results on shallow parsing and training methods for maximum entropy models. FUTURE RESEARCH DIRECTIONS There are a variety of promising directions for future research in applying supervised machine learni


Chunk 1261:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: ure research in applying supervised machine learning to information extraction. On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of (Collins, 2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. For example, (Taskar, 2003) proposes a maximal Markov model for sequential labeling task using the maximal margin theory. In information extraction, in addition to i


Chunk 1262:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: heory. In information extraction, in addition to identifying entities, an important problem is extracting specific types of relations between entities. For example, in newspaper text, one can identify that an organization is located in a particular city or that a person is affiliated with a specific organization (Zelenko, 2003); in biomedical text, one can identify that a protein interacts with another protein or that a protein is located in a particular part of the cell (Bunescu, 2005a; Craven, 1999). The 


Chunk 1263:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: t of the cell (Bunescu, 2005a; Craven, 1999). The entities may occur in different parts of a sentence or paragraph. New principled methods are needed to such problems to identify both the entities while identify their relations. Bunescu and Mooney (2005b) propose to use a Statistical Relational Learning (SRL) method for the complex problem. They are trying to integrate decision at different levels (e.g. different kinds of entity identification and different kinds of relations identification) into the SRL mo


Chunk 1264:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: kinds of relations identification) into the SRL model. Moreover, several recent projects have taken the first steps in this direction. For example, Sutton (2004) presents a dynamic version of CRF that integrates part-of-speech tagging and noun-phrase chunking into one coherent process. (Roth, 2004) presents an information extraction approach based on linear-programming that integrates recognition of entities with the identification of relations between these entities. As another future work, more applicatio


Chunk 1265:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text:  entities. As another future work, more applications, especially practical applications, need to be investigated. The new applications can provide rich data sources for conducting information extraction, at the same time bring big challenges to the field. This is because various applications have various characteristics, needing to use different methods to deal with. 30
CONCLUSIONS Aiming to apply methods and technologies from practical computer science such as compiler construction and artificial intellige


Chunk 1266:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text:  as compiler construction and artificial intelligence to the problem of processing unstructured textual data automatically, information extraction has become an important sub-discipline of language engineering, a branch of computer science. Nowadays, the significance of Information Extraction is determined by the growing amount of information available in unstructured (i.e. without metadata) form, for instance on the Internet. In this chapter, we have reviewed the information extraction methods. Specificall


Chunk 1267:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: ed the information extraction methods. Specifically, we focus on the three state-of-the-art methods: rule learning based method, classification based method, and sequential labeling base method. We have explained the principle of the three methods by using several developed systems as examples. We have also introduced our research work on the information methods and their applications. We also introduced several practical application of information extraction, ranging from natural language processing to inf


Chunk 1268:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: n, ranging from natural language processing to information extraction from web pages and plain texts. The rule learning based method try to exploit the regularity in language expressions of certain information to find common linguistic patterns that match these expressions. It is easy to understand by an average user. The method can obtain good performance when processing some semi-structured documents (e.g. template-based web page). Its disadvantage lies on that its rudimentary learning mechanisms cannot p


Chunk 1269:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text:  that its rudimentary learning mechanisms cannot provide enough generalization capabilities. This makes it difficult to obtain good performance in complicated situations (e.g. extraction from natural language text). The classification based method casts the IE task as a classification problem in terms of the statistical theory. It can incorporate different types of information (including words, syntax, a prior knowledge, etc.). Thus it has more generalization capabilities than the rule based method. In seve


Chunk 1270:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: n capabilities than the rule based method. In several real-world applications, it can outperform the rule based method. Its drawback is that its model is usually complex and it is difficult for the general user to understand (e.g. the feature definition). Thus the performances of extraction differ from application to application. The sequential labeling based method can make use of dependencies between information to improve the extraction performance. It is also based on the statistical theory and thus has


Chunk 1271:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text:  also based on the statistical theory and thus has strong generalization capabilities. In many applications, in particular natural language processing, it can outperform the rule based method and the classification based method. As for the disadvantage, similar to the classification based method, it is not easy to be understood by a general user. Information extraction suffers from uncertainty and implication of the natural language. Both of the two problems are difficult for machine to automatic extraction


Chunk 1272:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text:  are difficult for machine to automatic extraction, sometimes even for human. For example, “It is likely that ...”. In such sentence, it is difficult to determine the reliability degree of the information. Consider another example “After a furious fight, enemy raised the white flag”, here the white flag means a defeat. However, it would of course difficult for computer to conclude the implication. 31
Another interesting also important issue is how to make use of the prior knowledge in information extraction


Chunk 1273:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: e of the prior knowledge in information extraction. So far, a usual method for incorporating the prior knowledge is to use some domain-specific dictionaries, thesauri in the extraction. The question is whether the simple method still works well when dealing with more complex extraction tasks. A further question is if we can incorporate the different types of prior knowledge into a unified model for extraction. In future work, research community has to face the rising challenges and focuses on how to enhance


Chunk 1274:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 80, 'num_chars': 512}
Text: he rising challenges and focuses on how to enhance the practical usefulness of IE methods. ACKNOWLEDGE The work is funded by the Natural Science Foundation of China under Grant No. 90604025. Thanks to the anonymous reviewers for their constructive suggestions. REFERENCES Abney, S. (1991). Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny (Eds.), Principle-based parsing. Boston: Kluwer Academic Publishers. Berger, A. L., Della Pietra, S. A., & Della Pietra, V. J. (1996). A maximum entropy approach to 


Chunk 1275:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 81, 'num_chars': 512}
Text: etra, V. J. (1996). A maximum entropy approach to natural language processing. In Computational Linguistics (Vol.22, pp.39-71). MA: MIT Press. Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In D. Haussler (Eds.) 5th Annual ACM Workshop on COLT (pp.144-152). Pittsburgh, PA: ACM Press. Bunescu, R., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., et al. (2005). Comparative experiments on learning information extractors for proteins a


Chunk 1276:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text:  on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine (special issue on Summarization and Information Extraction from Medical Documents). 33(2), pp.139-155. Bunescu, R. & Mooney, R. J. (2005). Statistical relational learning for natural language information extraction. In Getoor, L., & Taskar, B. (Eds.), Statistical Relational Learning, forthcoming book Califf, M. E., & Mooney, R. J. (1998). Relational learning of pattern-match rules for information ext


Chunk 1277:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: earning of pattern-match rules for information extraction. In Working Notes of AAAI Spring Symposium on Applying Machine Learning to Discourse Processing. pp.6-11. Califf, M. E., & Mooney, R. J. (2003). Bottom-up relational learning of pattern matching rules for information extraction. Journal of Machine Learning Research. Vol.4, pp.177-210. Chen, S. F., & Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University. Ciravegna, F. (2


Chunk 1278:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: -108, Carnegie Mellon University. Ciravegna, F. (2001). (LP)2, an adaptive algorithm for information extraction from Web-related texts. In Proceedings of the IJCAI-2001 Workshop on Adaptive Text Extraction and Mining held in conjunction with 17th International Joint Conference on Artificial Intelligence (IJCAI), Seattle, USA. 32
Collins, M. (2002). Discriminative training methods for Hidden Markov models: theory and experiments with Perceptron algorithms. In Proceedings of the Conference on Empirical Method


Chunk 1279:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text:  Proceedings of the Conference on Empirical Methods in NLP (EMNLP’02). Craven, M., & Kumlien, J. (1999). Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology (ISMB-1999). pp.77-86. Heidelberg, Germany. Darroch, J. N., & Ratcliff, D. (1972). Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43 (5), pp.1470-1480. Ducheneaut, N., & Bellotti, V


Chunk 1280:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: 3 (5), pp.1470-1480. Ducheneaut, N., & Bellotti, V. (2001). E-mail as Habitat: An exploration of embedded personal information management. Interactions, Vol.8, pp.30-38. Finkel, J. R., Grenager, T., & Manning, C. D. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005). pp.363-370. Finn, A., & Kushmerick, N. (2004). Information extraction by convergent boundary c


Chunk 1281:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: ). Information extraction by convergent boundary classification. In AAAI-04 Workshop on Adaptive Text Extraction and Mining. San Jose, USA. Finn, A. (2006). A multi-level boundary classification approach to information extraction. Phd thesis, University College Dublin. Freitag, D. (1998). Information extraction from HTML: Application of a general machine learning approach. In Proceedings of the 15th Conference on Artificial Intelligence (AAAI’98). pp.517-523 Freitag, D., & Kushmerick, N. (2000). Boosted wra


Chunk 1282:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text:  Freitag, D., & Kushmerick, N. (2000). Boosted wrapper induction. In Proceedings of 17th National Conference on Artificial Intelligence. pp.577-583 Ghahramani, Z., & Jordan, M. I. (1997). Factorial Hidden Markov Models. Machine Learning, Vol.29, pp.245-273 Hammersley, J., & Clifford, P. (1971). Markov fields on finite graphs and lattices. Unpublished manuscript. Han, H., Giles, L., Manavoglu, E., Zha, H., Zhang, Z., & Fox, E.A. (2003). Automatic document metadata extraction using support vector machines. In


Chunk 1283:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: adata extraction using support vector machines. In Proceedings of 2003 Joint Conference on Digital Libraries (JCDL’03). pp.37-48 Kauchak, D., Smarr, J., & Elkan, C. (2004). Sources of success for boosted wrapper induction. The Journal of Machine Learning Research, Vol.5, pp.499-527. MA: MIT Press. Kristjansson, T. T., Culotta, A., Viola, P. A., & McCallum, A. (2004). Interactive information extraction with constrained conditional random fields. In Proceedings of AAAI’04, pp.412-418 Kushmerick, N., Weld, D. 


Chunk 1284:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 66, 'num_chars': 512}
Text: s of AAAI’04, pp.412-418 Kushmerick, N., Weld, D. S., & Doorenbos, R. (1997). Wrapper induction for information extraction. In Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI’97). pp.729-737. Kushmerick, N. (2000). Wrapper induction: Efficiency and expressiveness. Artificial Intelligence, Vol.118, pp.15-68. Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: 33
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18


Chunk 1285:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: d labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML’01). pp.282-289. Lawrence, S., Giles, C.L., & Bollacker K. (1999). Digital libraries and autonomous citation indexing. IEEE Computer, Vol.32(6), pp.67-71. Li, J., & Yu, Y. (2001). Learning to generate semantic annotation for domain specific sentences. In Proceedings of the Knowledge Markup and Semantic Annotation Workshop in K-CAP'2001, Victoria, BC. Li, X., & Liu, B. (2003). Learning to classify texts us


Chunk 1286:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: ., & Liu, B. (2003). Learning to classify texts using positive and unlabeled data. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI'2003). pp.587-592 McCallum, A., Freitag, D., & Pereira, F. (2000). Maximum Entropy Markov Models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning (ICML’00). pp.591-598. McCallum, A. (2003). Efficiently inducing features of Conditional Random Fields. In Proceedings of the 19th C


Chunk 1287:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: tional Random Fields. In Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence. pp.403-410. Morik, K., Brockhausen, P., & Joachims, T. (1999). Combining statistical learning with a knowledge-based approach - A case study in intensive care monitoring. In Proceedings of International Conference on Machine Learning (ICML’99). pp.268-277. Muslea, I., Minton, S., & Knoblock, C. (1998). STALKER: Learning extraction rules for semistructured, web-based information sources. In AAAI Workshop on


Chunk 1288:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: web-based information sources. In AAAI Workshop on AI and Information Integration. pp.74-81. Muslea, I., Minton, S., & Knoblock, C. (1999). Hierarchical wrapper induction for semistructured information sources. Autonomous Agents and Multi-Agent Systems. Vol.4, pp.93-114. Muslea, I. (1999). Extraction patterns for information extraction tasks: A survey. In Proceedings of AAAI-99: Workshop on Machine Learning for Information Extraction. Orlando. Muslea, I., Minton, S., & Knoblock, C. A. (2003). Active learnin


Chunk 1289:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: nton, S., & Knoblock, C. A. (2003). Active learning with strong and weak views: A case study on wrapper induction. In Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI). Acapulco, Mexico. Ng, H. T., Lim, C. Y., Koo, J. L. T. (1999). Learning to Recognize Tables in Free Text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics (ACL’99). pp. 443-450. Nocedal, J., & Wright, S. J. (1999). Numerical optimization.


Chunk 1290:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: ., & Wright, S. J. (1999). Numerical optimization. New York, USA: Springer press. Peng, F. (2001). Models for Information Extraction. Technique Report. Peng, F., & McCallum, A. (2004). Accurate information extraction from research papers using Conditional Random Fields. In Proceedings of HLT-NAACL. pp. 329-336. Pinto, D., McCallum, A., Wei, X., & Croft, W. B. (2003). Table Extraction Using Conditional Random Fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Develop


Chunk 1291:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: ional ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’03). 34
pp. 235-242. Ramshaw, L. A., & Marcus, M. P. (1995). Text chunking using transformation-based learning. In Proceedings of Third Workshop on Very Large Corpora, ACL. pp.67-73. Ratnaparkhi, A. (1998). Unsupervised Statistical Models for Prepositional Phrase Attachment. In Proceedings of COLING ACL’98. pp.1079-1085. Montreal, Canada. Riloff, E. (1993). Automatically Constructing a Dictionary for Information Extractio


Chunk 1292:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 63, 'num_chars': 512}
Text: onstructing a Dictionary for Information Extraction Tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence. pp.811-816. Riloff, E. (1996). Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence. pp.1044-1049. Riloff, E., & Jones, R. (1999). Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intell


Chunk 1293:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: Sixteenth National Conference on Artificial Intelligence. pp.474-479. Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) . pp.1-8. Boston, MA. Schölkopf B., Burges, C. J. C., & Smola A. J. (1999). Advances in kernel methods: Support vector learning. MA: MIT Press. Sha, F., & Pereira, F. (2003). Shallow parsing with Conditional Random Fields. In Proceeding


Chunk 1294:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: sing with Conditional Random Fields. In Proceedings of Human Language Technology, NAACL. pp.188-191. Shapire, R. E. (1999). A brief introduction to Boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-1999). pp.1401-1405. Shewchuk, J. R. (1994). An introduction to the conjugate gradient method without the agonizing pain. from http://www-2.cs.cmu.edu/.jrs/jrspapers.html#cg. Siefkes, C., & Siniakov, P. (2005). An overview and classification of adaptive approach


Chunk 1295:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: n overview and classification of adaptive approaches to information extraction. Journal on Data Semantics IV. Berlin, Germany: Springer. Soderland, S., Fisher, D., Aseltine, J., & Lehnert, W. (1995). CRYSTAL: Inducing a conceptual dictionary. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI’95). pp.1314-1319. Soderland, S. (1999). Learning information extraction rules for semi-structured and free text. Machine Learning. Boston: Kluwer Academic Publishers Sutt


Chunk 1296:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text:  Learning. Boston: Kluwer Academic Publishers Sutton, C., Rohanimanesh, K., & McCallum, A. (2004). Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of ICML’2004. pp.783-790. Sutton, C., & McCallum, A. (2005). An introduction to Conditional Random Fields for relational learning. In Getoor, L., & Taskar, B. (Eds.), Statistical Relational Learning, forthcoming book. Tang, J., Li, H., Cao, Y., & Tang, Z. (2005). Email Data Cleaning. In 


Chunk 1297:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 85, 'num_chars': 512}
Text: o, Y., & Tang, Z. (2005). Email Data Cleaning. In Proceedings of SIGKDD’2005. pp.489-499. Chicago, Illinois, USA. Tang, J., Li, J., Lu, H., Liang, B., & Wang, K. (2005). iASA: Learning to Annotate the Semantic Web. Journal on Data Semantic IV (pp. 110-145). New York, USA: Springer Press. 35
Tang. J., Hong, M., Zhang, J., Liang, B., and Li, J. (2006). A New Approach to Personal Network Search based on Information Extraction. In Proceedings of the first International Conference of Asian Semantic Web (ASWC). T


Chunk 1298:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: ational Conference of Asian Semantic Web (ASWC). To appear. Tang, J., Hong, M., Li, J., & Liang, B. (2006). Tree-structured conditional random fields for semantic annotation. In Proceedings of 5th International Conference of Semantic Web (ISWC’2006), pp.640-653. Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks. In Neural Information Processing Systems 2003. Tetko, I.V., Livingstone, D.J., & Luik, A.I. (1995). Neural network studies. 1. Comparison of overfitting and overtraining. Jou


Chunk 1299:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: 1. Comparison of overfitting and overtraining. Journal of Chemical Information and Computer Sciences, Vol.35, pp.826-833. Tjong Kim Sang, E. F., & Buchholz, S. (2000). Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000, pp.127-132. Vapnik, V. (1998). Statistical Learning Theroy. Springer Verlage, New York, 1998 Vapnik V. (1999). The Nature of Statistical Learning Theory. Springer Verlag, New York, 1999. Wallach, H. (2002). Efficient training of Conditional Random Fields. Mast


Chunk 1300:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 73, 'num_chars': 512}
Text: icient training of Conditional Random Fields. Master thesis. University of Edinburgh, USA. Wang, Y., & Hu, J. (2002). A Machine Learning based Approach for Table Detection on the Web. In Proceedings of the 11th International World Wide Web Conference (WWW’02). pp. 242-250. Honolulu, Hawaii, USA. Wainwright, M., Jaakkola, T., & Willsky, A. (2001). Tree-based reparameterization for approximate estimation on graphs with cycles. In Proceedings of Advances in Neural Information Processing Systems (NIPS'2001). pp


Chunk 1301:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: ral Information Processing Systems (NIPS'2001). pp.1001-1008. Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation extraction. Journal of Machine Learning Research, Vol.3, 1083-1106. Zhang, L., Pan, Y., & Zhang, T. (2004). Recognizing and Using Named Entities: Focused Named Entity Recognition Using Machine Learning. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’04). pp.281-288. Zhu, J., Nie, Z., Wen, J


Chunk 1302:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: l (SIGIR’04). pp.281-288. Zhu, J., Nie, Z., Wen, J., Zhang, B., & Ma, W. (2005). 2D Conditional Random Fields for Web information extraction. In Proceedings of 22nd International Conference on Machine Learning (ICML2005). pp.1044-1051. Bonn, Germany. ADDITIONAL READING Adwait, R. (1996). Maximum Entropy Model for POS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp.133-142. Somerset, New Jersey, 1996. Ahn, D. (2006). The Stages of Event Extraction. In Proceed


Chunk 1303:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: (2006). The Stages of Event Extraction. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events. pp. 1–8. Sydney, July 2006. Allen, J. (1994). Natural Language Understanding (2nd Edition). Addison Wesley. 1994 Altun, Y., Tsochantaridis, I., & Hofmann, T. (2003). Hidden Markov Support Vector 36
Machines. In Proceedings of the 20th International Conference on Machine Learning (ICML 2003). Appelt, D. & Israel, D. (1999). Introduction to Information Extraction Technology. In Proceedings


Chunk 1304:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text:  Information Extraction Technology. In Proceedings of IJCAI’99 Tutorial. Baum, L. E. & Petrie, T. (1966). Statistical Inference for Probabilistic Functions of Finite State Markov Chains. Annual of Mathematical statistics, 37:1554-1563, 1966. Borthwick, A., Sterling, J., Agichtein, E., & Grishman, R. (1998). Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. In Proceedings of the Sixth Workshop on Very Large Corpora New Brunswick, New Jersey. Bunescu, R.C. & Mooney, R.J. (2


Chunk 1305:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: swick, New Jersey. Bunescu, R.C. & Mooney, R.J. (2004). Collective Information Extraction with Relational Markov Networks. In Proceedings of ACL’2004. Cafarella, M.J., Downey, D., Soderland, S., & Etzioni, O. (2005). KnowItNow: Fast, Scalable Information Extraction from the Web. In Proceedings of HLT/EMNLP’2005. Chieu, H.L. (2002). A Maximum Entropy Approach to Information Extraction from Semi-Structured and Free Text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI’200


Chunk 1306:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 66, 'num_chars': 512}
Text: al Conference on Artificial Intelligence (AAAI’2002). pp.786-791. Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing(EMNLP’2002). pp.1-8, July 06, 2002. Dietterich, T. (2002). Machine Learning for Sequential Data: A Review. In Proceedings of the Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition. pp. 15


Chunk 1307:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: actic, and Statistical Pattern Recognition. pp. 15–30. 2002. Springer-Verlag. Downey, D., Etzioni, O., & Soderland, S. (2005). A Probabilistic Model of Redundancy in Information Extraction. In Proceedings of 22th International Joint Conference on Artificial Intelligence (IJCAI’2005). pp. 1034-1041. Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis: Probabilistic models of proteins and nucleic acids. Cambridge University Press, 1998. Eikvil, L. (1999). Information Extracti


Chunk 1308:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: ess, 1998. Eikvil, L. (1999). Information Extraction from World Wide Web - A Survey. Rapport Nr. 945, July, 1999. Embley, D.W. (2004). Toward Semantic Understanding - An Approach Based on Information Extraction. In Proceedings of the Fifteenth Australasian Database Conference, 2004. Freitag, D. (1998). Machine Learning for Information Extraction from Online Documents. PhD thesis, School of Computer Science. Carnegie Mellon University. Freitag, D. & McCallum, A. (2000). Information Extraction with HMM Struct


Chunk 1309:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text:  A. (2000). Information Extraction with HMM Structures Learned by Stochastic Optimization. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI’2000). Grishman, R. & Sundheim, B. (1996). Message Understanding Conference –6: A Brief History. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, June 1996. Hu, Y., Li, H., Cao, Y., Meyerzon, D., Teng, L., & Zheng, Q. (2006). Automatic Extraction of Titles from General Documents using Machi


Chunk 1310:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: ction of Titles from General Documents using Machine Learning. Information 37
Processing and Management. pp.1276-1293, 2006 Huffman, S.B. (1995). Learning Information Extraction Patterns from Examples. In Proceedings of Learning for Natural Language Processing’1995. pp. 246-260. Jackson, P. & Moulinier, I. (2002). Natural Language Processing for Online Applications. John Benjamins, 2002. Klein, D. & Manning, C. (2002). Conditional Structure Versus Conditional Estimation in NLP Models. In Proceedings of the 


Chunk 1311:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 74, 'num_chars': 512}
Text: l Estimation in NLP Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’2002), Philadelphia. Laender, A.H.F., Ribeiro-Neto, B.A., da Silva, A.S., & Teixeira, J.S. (2002). A Brief Survey of Web Data Extraction Tools . Journal of ACM SIGMOD Record, 2002. Leek, T.B. (1997). Information Extraction Using Hidden Markov Models. M.S. thesis. Moens, M. (2006). Information Extraction: Algorithms and Prospects in a Retrieval Context. Springer press Li, Y., Bontcheva, K.,


Chunk 1312:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: val Context. Springer press Li, Y., Bontcheva, K., & Cunningham, H. (2005). Using Uneven-Margins SVM and Perceptron for Information Extraction. In Proceedings of Ninth Conference on Computational Natural Language Learning (CoNLL-2005). pp.72-79 Manning, C., & Schutze, H. (1999). Markov Models. In Book: Foundations of Statistical Natural Language Processing. The MIT Press. 1999. Pazienza, M.T. (1999). Information Extraction : Towards Scalable, Adaptable Systems. Springer press. Punyakanok, V. & Roth, D. (200


Chunk 1313:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 73, 'num_chars': 512}
Text: ms. Springer press. Punyakanok, V. & Roth, D. (2001). The Use of Classifiers in Sequential Inference. In Proceedings of NIPS’01. pp.995-1001. Rabiner, L. A. (1989). Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Proceedings of the IEEE’1989. Shawe-Taylor, J. & Cristianini, N. (2000). Introduction to Support Vector Machines. Cambridge University Press, 2000 Skounakis, M., Craven, M., & Ray, S. (2003). Hierarchical Hidden Markov Models for Information Extraction. In Proce


Chunk 1314:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: Markov Models for Information Extraction. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico. Morgan Kaufmann,2003. Z. Zhang. (2004). Weakly-Supervised Relation Classification for Information Extraction. In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management (CIKM’2004).pp581-588. 1 In machine learning, usually a learning algorithm is trained using some set of training examples. The learner is assumed to reach


Chunk 1315:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 84, 'num_chars': 512}
Text: training examples. The learner is assumed to reach a state where it will also be able to predict the correct output for other examples. However, especially in cases where learning was performed too long or where training examples are rare, the learner may adjust to very specific random features of the training data, that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse (


Chunk 1316:
Document ID: 02f82b0b-7bde-4298-a16b-ec4a5a3f19e3
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Information Extraction - Methodologies and Applications.pdf', 'folder_name': 'Module 8', 'num_tokens': 11, 'num_chars': 66}
Text: ile the performance on unseen data becomes worse (Tetko, 1995). 38


Chunk 1317:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 66, 'num_chars': 512}
Text: Knowledge Graph Construction, Evaluation, and Applications Haihua Chen March 1, 2022 1/85
(cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph 2/85
(cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) 


Chunk 1318:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph 3/85
(cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph 4/85
(cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowl


Chunk 1319:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: ale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation 5/85
Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications 6/85
(cid:73) KG Representation: (cid:104)head,relationship,tail(cid:105). Introduction to KG What is a Knowledge Graph (KG) (cid:73) Knowledge graph (KG) is a multi-


Chunk 1320:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 64, 'num_chars': 512}
Text: aph (KG) (cid:73) Knowledge graph (KG) is a multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively 1. 1 S.Ji,etal.”ASurveyonKnowledgeGraphs:Representation,AcquisitionandApplications.”arXivpreprint arXiv:2002.00388(2020). 7/85
Introduction to KG What is a Knowledge Graph (KG) (cid:73) Knowledge graph (KG) is a multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectivel


Chunk 1321:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 44, 'num_chars': 512}
Text: as nodes and different types of edges, respectively 1. (cid:73) KG Representation: (cid:104)head,relationship,tail(cid:105). 1 S.Ji,etal.”ASurveyonKnowledgeGraphs:Representation,AcquisitionandApplications.”arXivpreprint arXiv:2002.00388(2020). 8/85
Introduction to KG A KG Example in the Film Domain 2 2 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 9/85
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, C


Chunk 1322:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: entations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 10/85
(cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. Introduction to K


Chunk 1323:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: e: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). (cid:73) Concept: A fundamental category of existence; Representations of categories. 11/85
(cid:73) Value: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potent


Chunk 1324:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: elf, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). (cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. 12/85
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). (cid:73) Concep


Chunk 1325:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: y, physically or not. (Wikipedia). (cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 13/85
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). (cid:73) Concept: A fundamental category of existen


Chunk 1326:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 14/85
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). (cid:73) Concept: A fundamental category of existence; Representations of categories. (


Chunk 1327:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 61, 'num_chars': 512}
Text: ory of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 15/85
(cid:73) Attribute/Property/Quality: A characteristic/quality to define an entity. Introduction to KG Edge (cid:73) Relationship: The relation between entities. 16/85
Introduction to KG Edge (cid:73) Relationship: The relation between entities. (cid:73) Attribute/Property/Quality: A characteristic/quality to define an entity.


Chunk 1328:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: ity: A characteristic/quality to define an entity. 17/85
Introduction to KG Model of KG 3 3 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 18/85
(cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 19/85
(c


Chunk 1329:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text: graph.”SemanticsConference 2019,Tutorial. 19/85
(cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 20/85
(cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs a


Chunk 1330:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 21/85
Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Know


Chunk 1331:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 54, 'num_chars': 512}
Text: cult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 22/85
Introduction to KG Common Characteristics of KGs 5 5 https://queue.acm.org/detail.cfm?id=3332266 23/85
(cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. (cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. 24/85


Chunk 1332:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: 3) High coverage over entities and concepts. 24/85
(cid:73) Structured Organization. (cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. 25/85
(cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. 2


Chunk 1333:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 57, 'num_chars': 512}
Text: relationships. (cid:73) Structured Organization. 26/85
Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. (cid:73) Provide users with explainable results. 27/85
Introduction to KG From KG Construction to KG Applications 28/85
Construction of large-scale KG The Pipeline of KG Construction 6 6 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 


Chunk 1334:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 53, 'num_chars': 512}
Text: ndfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 29/85
(cid:73) Identify which fields to be compared. (cid:73) Schema matching. Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. (cid:73) Data transformation and standardization. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 30/85
(cid:73) Schema matching. Construction of large-scale KG Data Preparation 7


Chunk 1335:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 61, 'num_chars': 512}
Text:  Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. (cid:73) Data transformation and standardization. (cid:73) Identify which fields to be compared. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 31/85
Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items


Chunk 1336:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 57, 'num_chars': 512}
Text: Parsing: locate, identify, and separate data items. (cid:73) Data transformation and standardization. (cid:73) Identify which fields to be compared. (cid:73) Schema matching. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 32/85
Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. (cid:73) Data transformation and standardization. (cid:73) Identify which field


Chunk 1337:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 44, 'num_chars': 512}
Text: and standardization. (cid:73) Identify which fields to be compared. (cid:73) Schema matching. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 33/85
Construction of large-scale KG Ingestion Flow 8 8 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 34/85
(cid:73) Synonyms: Entity Linking, Entity Resolution, Reference Reconciliation, Deduplication, Match/Merge, Merge/Purge. Construction of large-scale KG Entity Mapping 9 E


Chunk 1338:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text:  Construction of large-scale KG Entity Mapping 9 Entity Mapping: To identify and discover instances referring to the same real-world entity. (cid:73) Objective: (cid:73) Data Enrichment. (cid:73) Improve Data Quality by identifying and removing duplicates. (cid:73) Supporting fact correctness by merging duplicate facts from multiple sources. 9 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 35/85
Construction of large-scale KG Entity Mapping 9 Entity Mapping: To i


Chunk 1339:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 56, 'num_chars': 512}
Text: rge-scale KG Entity Mapping 9 Entity Mapping: To identify and discover instances referring to the same real-world entity. (cid:73) Objective: (cid:73) Data Enrichment. (cid:73) Improve Data Quality by identifying and removing duplicates. (cid:73) Supporting fact correctness by merging duplicate facts from multiple sources. (cid:73) Synonyms: Entity Linking, Entity Resolution, Reference Reconciliation, Deduplication, Match/Merge, Merge/Purge. 9 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.


Chunk 1340:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: ildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 36/85
Construction of large-scale KG Knowledge Fusion (Merging Entities) 10 After merging entity nodes in the graph, we end up with conflicting facts and connections. (cid:73) Resolving facts (and finding truth) (cid:73) Majority Voting (cid:73) Identify Authoritative Sources (cid:73) Fact Checker (cid:73) Gather evidence from different sources (cid:73) Evaluate evidences (cid:73) Model joint interactions (cid:73) Aggregate eviden


Chunk 1341:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 56, 'num_chars': 512}
Text: Model joint interactions (cid:73) Aggregate evidence and predict 10 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 37/85
Construction of large-scale KG Error Detection 11 (cid:73) Data Quality Rules (cid:73) Functional Dependency and its conditional variation, e.g.; Zip→− City (cid:73) Inconsistency Entity cannot be a movie and book Date-of-birth < date-of-death (cid:73) Outliers detection (cid:73) External signals for relationship validation (e.g.; co-clicks) (c


Chunk 1342:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 49, 'num_chars': 512}
Text: s for relationship validation (e.g.; co-clicks) (cid:73) NLP features 11 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 38/85
Construction of large-scale KG Fact Inference 12 (cid:73) Further data enrichment and data completion (cid:73) Internal: Dominant type and label (cid:73) External: Search engine method for enriching social links. 12 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 39/85
Towards a Legal Knowledge G


Chunk 1343:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 60, 'num_chars': 512}
Text: utorial39(2018). 39/85
Towards a Legal Knowledge Graph Our Vision of Legal KG 13 13 V.MirelesandM.Kaltenb ̈ock.”BuildingtheLegalKnowledgeGraph”.W3CWorkshop,2018 40/85
(cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal 


Chunk 1344:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 70, 'num_chars': 512}
Text: ation (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search 41/85
(cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions an


Chunk 1345:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: ort the comparative analyses of court decisions and different legal interpretations of legislation 42/85
(cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation


Chunk 1346:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: tion (cid:73) Enables the evolution of legislation and jurisdiction 43/85
(cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases 44/85
(cid:73


Chunk 1347:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: wledge with external knowledge bases 44/85
(cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization 45/85
Towards a Legal Knowledge Graph Why Do


Chunk 1348:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: ation 45/85
Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering 46/85
Towards a Legal Knowledge Graph Build an intelligent legal system, fo


Chunk 1349:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 76, 'num_chars': 512}
Text: wledge Graph Build an intelligent legal system, for automated the information retrieval, evidence detection , and argument generation in legal applications. 47/85
(cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. (cid:73) The lacking of automated to


Chunk 1350:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 41, 'num_chars': 512}
Text: rstanding 15. (cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-n


Chunk 1351:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: gyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 48/85
(cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. (cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often lon


Chunk 1352:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 36, 'num_chars': 512}
Text: are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex Int


Chunk 1353:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: usingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 49/85
(cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may u


Chunk 1354:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 31, 'num_chars': 512}
Text: (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 50/85
T


Chunk 1355:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: ”Complex IntelligentSystems4.1(2018):1-10. 50/85
Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15


Chunk 1356:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 32, 'num_chars': 512}
Text: h makes it difficult for semantic understanding 15. (cid:73) The lacking of automated tools in legal field. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 51/85
Towards a Legal Knowledge Graph How to 


Chunk 1357:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 60, 'num_chars': 512}
Text: -10. 51/85
Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution 16 16 M.Dalvi,N.Tandon,andP.Clark.”Domain-targeted,highprecisionknowledgeextraction.”Transactionsof theAssociationforComputationalLinguistics5(2017):233-246. 52/85
(cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tupl


Chunk 1358:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: ased machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 53/85
(cid:73) Tuple Generation: OpenIE 18 w


Chunk 1359:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: .html 53/85
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law do


Chunk 1360:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 60, 'num_chars': 512}
Text: on (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 54/85
(cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality eval


Chunk 1361:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 65, 'num_chars': 512}
Text: generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 55/85
(cid:73) Relation Canonicalization: Identify


Chunk 1362:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: 55/85
(cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Featu


Chunk 1363:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 64, 'num_chars': 512}
Text:  sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 56/85
(cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used


Chunk 1364:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 64, 'num_chars': 512}
Text: ing. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 57/85
Towards a Legal Knowledge Graph How to Build a Leg


Chunk 1365:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a


Chunk 1366:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 55, 'num_chars': 512}
Text: equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 58/85
(cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A 


Chunk 1367:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 57, 'num_chars': 512}
Text: ould be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 59/85
(cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality ev


Chunk 1368:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 55, 'num_chars': 512}
Text: ompleteness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 60/85
(cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, 


Chunk 1369:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: ntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQuality


Chunk 1370:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text: J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 61/85
(cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, co


Chunk 1371:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 54, 'num_chars': 512}
Text: trinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 62/85
(cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, sem


Chunk 1372:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 55, 'num_chars': 512}
Text: uality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 63/85
Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Synt


Chunk 1373:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 55, 'num_chars': 512}
Text: y and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 64/85
Knowledge Graph Quality Evaluation Basic Quality Requirements for a KG 65/85
Knowledge Graph Quality Evaluatio


Chunk 1374:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: s for a KG 65/85
Knowledge Graph Quality Evaluation Comprehensive KG Quality Requirements 20 1. accessibility 2. accuracy 3. appropriate 4. believability (veracity) amount 5. complete- 6. concise rep- 7. consistent 8. cost- ness resentation representation effectiveness 9. easy of ma- 10. easy of op- 11. easy of un- 12. flexibility nipulating eration derstanding 13. free-of- 14. interoper- 15. objectivity 16. relevancy error ability 17. reputation 18. security 19. timeliness 20. traceability (velocity) 21. u


Chunk 1375:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 48, 'num_chars': 512}
Text: y 19. timeliness 20. traceability (velocity) 21. under- 22. value- 23. variety **fitness for standability added use** 20 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 66/85
Knowledge Graph Quality Evaluation Knowledge graph quality evaluation framework 21 21 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 67/85
Knowledge Graph Quality Evaluation KG quality requirements mapped to applications 22 22 H.


Chunk 1376:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 48, 'num_chars': 512}
Text: ality requirements mapped to applications 22 22 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 68/85
Knowledge Graph Quality Evaluation KG quality requirements mapped to quality dimensions 23 23 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 69/85
KG Applications Significant Publications on Applications of KGs 70/85
KG Applications Use Cases of Knowledge Graphs 24 (cid:73) Information Retrieval, Search (cid


Chunk 1377:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 52, 'num_chars': 512}
Text: phs 24 (cid:73) Information Retrieval, Search (cid:73) Recommendation (cid:73) Questions Answering (cid:73) Medical Text Analysis (cid:73) Conversation (cid:73) ... 24 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 71/85
KG Applications Use Case: Search 25 25 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 72/85
KG Applications Use Case: Search (cid:73) Offer related knowledge that is related to searched results. 73/85
KG Applications Use Case: Searc


Chunk 1378:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: hed results. 73/85
KG Applications Use Case: Search (cid:73) Recommend semantically related content. (cid:73) Example: What a search engine should recommend if a user search for “flu shot”? (cid:73) Recommend conceptually consistent content. (cid:73) Example: What Netflix should recommend if a user search for “Star War”? 74/85
KG Applications Use Case: Question Answering Fact Answering 75/85
KG Applications Use Case: Question Answering Knowledge-based Q&A 76/85
KG Applications Use Case: Medical Text Analyti


Chunk 1379:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: /85
KG Applications Use Case: Medical Text Analytics 26 26 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 77/85
Summary KG for AI: ML + NLP + Conflation + Inference 27 27 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 78/85
Summary Combine KG Construction, Evaluation, with Application 79/85
(cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with know


Chunk 1380:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 59, 'num_chars': 512}
Text: language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-Zentru


Chunk 1381:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: stuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 80/85
(cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on do


Chunk 1382:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 42, 'num_chars': 512}
Text: how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 81/85
(cid:73) Use language models like BERT in conjunction with 


Chunk 1383:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 72, 'num_chars': 512}
Text: Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. 28 P.Bonatti,etal.”Knowledgegraphs:newdirec


Chunk 1384:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 44, 'num_chars': 512}
Text: ed 28. 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 82/85
(cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework in


Chunk 1385:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 48, 'num_chars': 512}
Text: ved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”


Chunk 1386:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 64, 'num_chars': 512}
Text: ao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 83/85
Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic r


Chunk 1387:
Document ID: 278c0874-92e8-400e-92fd-d93482e80e1a
Metadata: {'file_name': '', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/Knowledge_Graph_Haihua Chen.pdf', 'folder_name': 'Module 8', 'num_tokens': 31, 'num_chars': 428}
Text: mbeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 84/85
Questions, Comments and Suggestions? Thank you! 85/85


Chunk 1388:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: 


import re
import string
import nltk
import spacy
import pandas as pd
import numpy as np
import math
from tqdm import tqdm

from spacy.matcher import Matcher
from spacy.tokens import Span
from spacy import displacy

pd.set_option('display.max_colwidth', 200)
# load spaCy model
nlp = spacy.load("en_core_web_sm")


# ### **1.1 Pattern: X such as Y**

# SpaCy’s dependency parsing can identify and match specific syntactic trees (e.g., noun phrases followed by prepositions).

# In[ ]:


doc = nlp("John bought 


Chunk 1389:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 88, 'num_chars': 512}
Text: prepositions).

# In[ ]:


doc = nlp("John bought fruits such as apples.")
for token in doc:
    if token.dep_ == 'prep' and token.head.text == 'fruits':
        print(token.head, token.text, token.nbor().text)


# Pattern: We define a pattern where:
# 
# - The first token is a noun.
# - The next two tokens are "such" and "as".
# - The last token is a proper noun. A proper noun is a specific name for an individual person, place, organization, or entity.
# - Matcher: The Matcher object is created, and the pa


Chunk 1390:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 91, 'num_chars': 512}
Text: Matcher: The Matcher object is created, and the pattern is added. The matcher is applied to the doc object, and it checks for matches.
# - Matches Handling: If a match is found, it extracts the span (the sequence of tokens that match the pattern) and prints it.

# In[ ]:


# sample text
text = "GDP in developing countries such as Vietnam will continue growing at a high rate."

# create a spaCy object
doc = nlp(text)
# print token, dependency, POS tag
for tok in doc:
  print(tok.text, "-->",tok.dep_,"-->", t


Chunk 1391:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 61, 'num_chars': 512}
Text:  in doc:
  print(tok.text, "-->",tok.dep_,"-->", tok.pos_)
  #define the pattern
pattern = [{'POS':'NOUN'},
           {'LOWER': 'such'},
           {'LOWER': 'as'},
           {'POS': 'PROPN'} #proper noun
           ]

# A token whose lowercase form matches such, e.g. “Such” or “SUCH”.
# A token whose is_punct flag is set to True, i.e. any punctuation.


# Matcher class object
matcher = Matcher(nlp.vocab)
matcher.add("matching_1", None, pattern)
matches = matcher(doc)
span = doc[matches[0][1]:matches[0][2


Chunk 1392:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 89, 'num_chars': 512}
Text: matcher(doc)
span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# - The adjectival modifier (amod) indicates that an adjective is modifying a noun.
# - The object of a preposition (pobj) is the noun or pronoun that follows and completes the meaning of a preposition.
#   - countries is the object of the preposition in:
# 
#   - countries --> pobj --> in
# 
# - The root is the main verb of the sentence or the central word that ties everything together.

# In[ ]:


# Matcher class object
matcher = Matc


Chunk 1393:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 60, 'num_chars': 512}
Text: 

# In[ ]:


# Matcher class object
matcher = Matcher(nlp.vocab)

#define the pattern
pattern = [{'POS':'VERB', 'OP':"?"}, # adjectival modifier
           {'POS':'NOUN'},
           {'LOWER': 'such'},
           {'LOWER': 'as'},
           {'POS': 'PROPN'}]

matcher.add("matching_1", None, pattern)
matches = matcher(doc)

span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# The matcher also lets you use quantifiers, specified as the 'OP' key. Quantifiers let you define sequences of tokens to be mat


Chunk 1394:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 103, 'num_chars': 512}
Text: fiers let you define sequences of tokens to be matched, e.g. one or more punctuation marks, or specify optional tokens. Note that there are no nested or scoped quantifiers – instead, you can build those behaviors with on_match callbacks.
# 
# |   OP		|   DESCRIPTION	|
# |---	|---	|
# |   !	|   Negate the pattern, by requiring it to match exactly 0 times.	|
# |   ?		|   Make the pattern optional, by allowing it to match 0 or 1 times.	|
# |   +	|   Require the pattern to match 1 or more times.	|
# |   *	|   A


Chunk 1395:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text:  pattern to match 1 or more times.	|
# |   *	|   Allow the pattern to match zero or more times.	|

# ### **1.2 Pattern: X and/or Y**

# In[ ]:


doc = nlp("Here is how you can keep your car and other vehicles clean.")

# print dependency tags and POS tags
for tok in doc:
  print(tok.text, "-->",tok.dep_, "-->",tok.pos_)

# Matcher class object
matcher = Matcher(nlp.vocab)

#define the pattern
pattern = [{'DEP':'amod', 'OP':"?"},
           {'POS':'NOUN'},
           {'LOWER': 'and', 'OP':"?"},
           {'


Chunk 1396:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 60, 'num_chars': 512}
Text:          {'LOWER': 'and', 'OP':"?"},
           {'LOWER': 'or', 'OP':"?"},
           {'LOWER': 'other'},
           {'POS': 'NOUN'}]

matcher.add("matching_1", None, pattern)

matches = matcher(doc)
span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# In[ ]:


doc = nlp("Here is how you can keep your car or other vehicles clean.")

# print dependency tags and POS tags
for tok in doc:
  print(tok.text, "-->",tok.dep_, "-->",tok.pos_)

# Matcher class object
matcher = Matcher(nlp.vocab)

#define the 


Chunk 1397:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 54, 'num_chars': 512}
Text:  object
matcher = Matcher(nlp.vocab)

#define the pattern
pattern = [{'DEP':'amod', 'OP':"?"},
           {'POS':'NOUN'},
           {'LOWER': 'and', 'OP':"?"},
           {'LOWER': 'or', 'OP':"?"},
           {'LOWER': 'other'},
           {'POS': 'NOUN'}]

matcher.add("matching_1", None, pattern)

matches = matcher(doc)
span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# ### **1.3 Pattern: X, including Y**

# In[ ]:


doc = nlp("Eight people, including two children, were injured in the explosion"


Chunk 1398:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 44, 'num_chars': 512}
Text: uding two children, were injured in the explosion")

for tok in doc:
  print(tok.text, "-->",tok.dep_, "-->",tok.pos_)

# Matcher class object
matcher = Matcher(nlp.vocab)

#define the pattern
pattern = [{'DEP':'nummod','OP':"?"}, # numeric modifier
           {'DEP':'amod','OP':"?"}, # adjectival modifier
           {'POS':'NOUN'},
           {'IS_PUNCT': True},
           {'LOWER': 'including'},
           {'DEP':'nummod','OP':"?"},
           {'DEP':'amod','OP':"?"},
           {'POS':'NOUN'}]

matcher.a


Chunk 1399:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 55, 'num_chars': 512}
Text: ','OP':"?"},
           {'POS':'NOUN'}]

matcher.add("matching_1", None, pattern)

matches = matcher(doc)
span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# ### **1.4 Pattern: X, especially Y**

# In[ ]:


doc = nlp("A healthy eating pattern includes fruits, especially whole fruits.")

for tok in doc:
  print(tok.text, tok.dep_, tok.pos_)

# Matcher class object
matcher = Matcher(nlp.vocab)

#define the pattern
pattern = [{'DEP':'nummod','OP':"?"},
           {'DEP':'amod','OP':"?"},
           {'


Chunk 1400:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 46, 'num_chars': 512}
Text: 
           {'DEP':'amod','OP':"?"},
           {'POS':'NOUN'},
           {'IS_PUNCT':True},
           {'LOWER': 'especially'},
           {'DEP':'nummod','OP':"?"},
           {'DEP':'amod','OP':"?"},
           {'POS':'NOUN'}]

matcher.add("matching_1", None, pattern)

matches = matcher(doc)
span = doc[matches[0][1]:matches[0][2]]
print(span.text)


# ### **1.5 Subtree Matching for Relation Extraction**
# We have to be extremely creative to come up with new rules to capture different patterns. It is dif


Chunk 1401:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: new rules to capture different patterns. It is difficult to build patterns that generalize well across different sentences.

# To enhance the rule-based methods for relation/information extraction, we should try to understand the dependency structure of the sentences at hand.

# In[ ]:


text = "Tableau was recently acquired by Salesforce."

# Plot the dependency graph
doc = nlp(text)
displacy.render(doc, style='dep',jupyter=True)
for tok in doc:
  print(tok.text,"-->",tok.dep_,"-->",tok.pos_)


# **Explana


Chunk 1402:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 78, 'num_chars': 512}
Text: .text,"-->",tok.dep_,"-->",tok.pos_)


# **Explanation of Dependencies:**
# - "Tableau" is the nsubjpass (nominal subject in passive voice) of the verb acquired.
# - "was" is an auxpass (auxiliary verb for passive voice), helping form the passive construction.
# - "recently" is an advmod (adverbial modifier) of acquired, providing timing information.
# - "acquired" is the ROOT of the sentence, meaning it's the central action.
# by is the agent introducing Salesforce, the doer of the action.
# - "Salesforce"


Chunk 1403:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 87, 'num_chars': 512}
Text: lesforce, the doer of the action.
# - "Salesforce" is the pobj (object of preposition by).

# **Key Functionality of the following:**
# - subjpass detection: The function checks for the passive subject (dependency tag nsubjpass), which indicates that the sentence is in the passive voice.
# - subj and obj extraction: Depending on whether the sentence is passive or active, the function retrieves the subject and object of the sentence.
# - Returns: It returns the subject (x) and object (y).

# In[ ]:


def sub


Chunk 1404:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 77, 'num_chars': 512}
Text: he subject (x) and object (y).

# In[ ]:


def subtree_matcher(doc):
  subjpass = 0

  for i,tok in enumerate(doc):
    # find dependency tag that contains the text "subjpass"
    if tok.dep_.find("subjpass") == True:
      subjpass = 1

  x = ''
  y = ''

  # if subjpass == 1 then sentence is passive
  if subjpass == 1:
    for i,tok in enumerate(doc):
      if tok.dep_.find("subjpass") == True:
        y = tok.text

      if tok.dep_.endswith("obj") == True:
        x = tok.text

  # if subjpass == 0 then


Chunk 1405:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 75, 'num_chars': 512}
Text: e:
        x = tok.text

  # if subjpass == 0 then sentence is not passive
  else:
    for i,tok in enumerate(doc):
      if tok.dep_.endswith("subj") == True:
        x = tok.text

      if tok.dep_.endswith("obj") == True:
        y = tok.text

  return x,y


subtree_matcher(nlp("Tableau was recently acquired by Salesforce."))


# If the sentence is passive:
# - The subject (the entity receiving the action, i.e., the passive subject) is assigned to y.
# - The object (the agent doing the action) is assigne


Chunk 1406:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 82, 'num_chars': 512}
Text: The object (the agent doing the action) is assigned to x.
# 
# If the sentence is active:
# - The subject (the doer of the action) is assigned to x.
# - The object (the receiver of the action) is assigned to y.

# In[ ]:


subtree_matcher(nlp("Careem, a ride hailing major in middle east, was acquired by Uber."))


# # 2. **Machine Learning based approaches**
# 
# Wonderful Open Source Information Extraction Projects:
# 
# https://awesomeopensource.com/projects/information-extraction

# ## **2.1 Open Informa


Chunk 1407:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 69, 'num_chars': 512}
Text: ts/information-extraction

# ## **2.1 Open Information Extraction tools**
# A curated list of Open Information Extraction (OIE) resources: research papers, code, data, applications, etc. The list is not limited to Open Information Extraction systems exclusively. It also includes work highly related to the field of OIE, such as taxonomizing open relations and using OIE in downstream applications.
# 
# https://github.com/gkiril/oie-resources

# ### **(1) Snips NLU**
# https://github.com/snipsco/snips-nlu
# ht


Chunk 1408:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text:  NLU**
# https://github.com/snipsco/snips-nlu
# https://snips-nlu.readthedocs.io/en/latest/installation.html

# In Snips NLU (and other natural language understanding systems), an intent represents the user's goal or purpose behind a specific input or query. It is the action or task the user wants to perform. Identifying the correct intent is crucial in building conversational agents, as it determines how the system should respond

# In[ ]:


parsing = nlu_engine.parse("Hey, lights on in the lounge !")
prin


Chunk 1409:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 42, 'num_chars': 512}
Text: ngine.parse("Hey, lights on in the lounge !")
print(json.dumps(parsing, indent=2))


# Supported builtin entities: https://snips-nlu.readthedocs.io/en/latest/builtin_entities.html

# ### **(2) ReVerb**
# 
# http://www.fiber-space.de/reverb2/reverb-doc/index.html#

# ### **(3) Stanford OpenIE**
# 
# https://pypi.org/project/stanford-openie/

# ## **2.2 Domain-specific Information Extraction**

# ### **(1) LegalNLP**
# 
# https://lexpredict-lexnlp.readthedocs.io/en/latest/modules/extract/extract.html#nlp-base


Chunk 1410:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: io/en/latest/modules/extract/extract.html#nlp-based-extraction-methods

# - Monetary Values: Extracts monetary amounts mentioned in contracts or legal documents.
# - Dates: Extracts dates (e.g., contract dates, deadlines, etc.) from legal documents.
# - Legal Entities: Extracts references to legal entities such as parties (e.g., companies, individuals).
# - References to Laws or Statutes: Detects legal citations or references to laws, articles, or statutes.
# - Clause and Section Extraction: Extracts legal 


Chunk 1411:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 73, 'num_chars': 512}
Text: # - Clause and Section Extraction: Extracts legal clauses and their contents, such as confidentiality clauses or indemnity clauses.

# ### **(2) medaCy**
# 
# Medical Text Mining and Information Extraction with spaCy
# 
# https://github.com/NLPatVCU/medaCy

# ### **(3) scispacy**
# 
# https://allenai.github.io/scispacy/

# ## 2.3 Keywords Extraction

# ### (1) Yet Another Keyword Extractor (YAKE)
# 
# YAKE! is a light-weight unsupervised automatic keyword extraction method which rests on text statistical fe


Chunk 1412:
Document ID: 5c33547f-4f95-44e4-830b-56a17ae2e34b
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 26, 'num_chars': 184}
Text: traction method which rests on text statistical features extracted from single documents to select the most important keywords of a text.
# 
# Github: https://github.com/LIAAD/yake

# 


Chunk 1413:
Document ID: 38659ba3-69f2-42e9-a06b-4f2b715e5e88
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 4, 'num_chars': 24}
Text: 


pip install yake


# 


Chunk 1414:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 79, 'num_chars': 512}
Text: 


import yake

# Sample text
text = """
Natural language processing (NLP) is a sub-field of artificial intelligence (AI)
that focuses on the interaction between computers and humans through natural language.
The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is valuable.
"""

# Define YAKE parameters
language = "en"          # Language of the text
max_ngram_size = 3       # Max size of extracted n-grams (up to trigrams)
deduplication_threshol


Chunk 1415:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 44, 'num_chars': 512}
Text: ed n-grams (up to trigrams)
deduplication_threshold = 0.9  # Similarity threshold for removing duplicates
num_of_keywords = 5      # Number of keywords to extract

# Create a YAKE keyword extractor
kw_extractor = yake.KeywordExtractor(lan=language,
                                     n=max_ngram_size,
                                     dedupLim=deduplication_threshold,
                                     top=num_of_keywords)

# Extract keywords
keywords = kw_extractor.extract_keywords(text)

# Print the


Chunk 1416:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 71, 'num_chars': 512}
Text: = kw_extractor.extract_keywords(text)

# Print the keywords
for kw, score in keywords:
    print(f"Keyword: {kw}, Score: {score}")


# ### (2) rake-nltk
# 
# RAKE short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text.
# 
# Github: https://github.com/csurfer/rake-nltk
# 
# Tutorial: https://www.airpair.com


Chunk 1417:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 67, 'num_chars': 512}
Text: r/rake-nltk
# 
# Tutorial: https://www.airpair.com/nlp/keyword-extraction-tutorial

# ### (3) TextRank
# 
# TextRank uses unsupervised methods for keyword and sentence extraction.
# 
# https://github.com/JRC1995/TextRank-Keyword-Extraction

# ### (4) pke - python keyphrase extraction
# 
# pke is an open source python-based keyphrase extraction toolkit. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models. pke also allows for


Chunk 1418:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 58, 'num_chars': 512}
Text: xtended to develop new models. pke also allows for easy benchmarking of state-of-the-art keyphrase extraction models
# 
# https://github.com/boudinfl/pke

# ### (5) spaCy, YAKE, rake-nltk and Gensim for Keyword Extraction, comparision
# 
# https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c

# ### (6) Keyword-Extraction using BERT
# 
# Deep Keyphrase Extraction using BERT. using BERT Token Classification Model to extract keywords from a sente


Chunk 1419:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 45, 'num_chars': 512}
Text: ssification Model to extract keywords from a sentence.
# 
# https://github.com/ibatra/BERT-Keyword-Extractor
# 
# https://github.com/ibatra/BERT-Keyword-Extractor/blob/master/BERT-Keyword%20Extractor.ipynb
# 
# KeyBERT: https://maartengr.github.io/KeyBERT/

# ## 2.4 Named Entity recognition

# 
# ### (1) Spacy
# 
# Overview in the general domain: https://pahulpreet86.github.io/name-entity-recognition-pre-trained-models-review/
# 
# https://spacy.io/usage/linguistic-features#named-entities
# 
# 
# Sample tex


Chunk 1420:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 68, 'num_chars': 512}
Text: guistic-features#named-entities
# 
# 
# Sample text
# 
# text = "Apple is planning to open a new store in New York on December 25, 2023."
# 
# Entity: Apple, Label: ORG
# Entity: New York, Label: GPE
# Entity: December 25, 2023, Label: DATE
# 
# 
# 
# ### (2) Spark NLP
# 
# GitHub: https://github.com/JohnSnowLabs/spark-nlp
# 
# Google Colab: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb
# 
# ### (3)  bert-base-NER:
# https://h


Chunk 1421:
Document ID: 5a546062-8439-4711-a820-154d4f4c5cc3
Metadata: {'file_name': 'W9_IE_Demo_Yuhan.ipynb', 'file_path': '/Users/liteshperumalla/Desktop/Files/masters/Smart AI Tutor/Modules/Module 8/W9_IE_Demo_Yuhan.ipynb', 'folder_name': 'Module 8', 'num_tokens': 33, 'num_chars': 297}
Text: _EN.ipynb
# 
# ### (3)  bert-base-NER:
# https://huggingface.co/dslim/bert-base-NER
# 
# ### (4) bio-bert-ner:
# 
# https://github.com/dmis-lab/biobert
# 
# https://github.com/ncbi-nlp/bluebert

# ## Recommended: TextRazor (Extract meaning from text)
# 
# https://www.textrazor.com/

# In[ ]:






✅ ChromaDB initialized successfully.
✅ 3188 document nodes created and stored in ChromaDB.
✅ Vector store index created successfully.
✅ Index persisted to ./persisted_index
def factorial(n):
    if n < 0:
        return 0
    elif n == 0 or n == 1:
        return 1
    else:
        result = n * factorial(n - 1)
        return result

num = 5;
print("Factorial of", num,"is", factorial(num))
